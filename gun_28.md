# GÃ¼n 28: Checkpoint vÉ™ Modelin SaxlanmasÄ± ğŸ’¾

## 28.1. Checkpoint NÉ™dir?

**Checkpoint (NÉ™zarÉ™t NÃ¶qtÉ™si)** tÉ™lim prosesinin mÃ¼É™yyÉ™n bir anÄ±nda modelin vÉ™ziyyÉ™tinin (Ã§É™kilÉ™rinin, optimallaÅŸdÄ±rÄ±cÄ±nÄ±n vÉ™ziyyÉ™tinin, cari epoxanÄ±n) yadda saxlanmasÄ±dÄ±r.

**NiyÉ™ Checkpoint Vacibdir?**

1.  **FasilÉ™siz TÉ™lim:** TÉ™lim prosesi elektrik kÉ™silmÉ™si, proqram xÉ™tasÄ± vÉ™ ya serverin baÄŸlanmasÄ± sÉ™bÉ™bindÉ™n dayandÄ±rÄ±larsa, son Checkpoint-dÉ™n davam etmÉ™k mÃ¼mkÃ¼ndÃ¼r. Bu, vaxta vÉ™ resurslara qÉ™naÉ™t edir.
2.  **Modelin TÉ™hlili:** TÉ™limin mÃ¼xtÉ™lif mÉ™rhÉ™lÉ™lÉ™rindÉ™ki modellÉ™ri (mÉ™sÉ™lÉ™n, 1-ci epoxa, 5-ci epoxa) saxlamaq vÉ™ sonradan mÃ¼qayisÉ™ etmÉ™k.

## 28.2. PyTorch-da Checkpoint SaxlanmasÄ±

PyTorch-da Checkpoint saxlamaq Ã¼Ã§Ã¼n adÉ™tÉ™n bir lÃ¼ÄŸÉ™t (dictionary) istifadÉ™ olunur. Bu lÃ¼ÄŸÉ™tÉ™ modelin Ã§É™kilÉ™ri ilÉ™ yanaÅŸÄ±, tÉ™limin davam etdirilmÉ™si Ã¼Ã§Ã¼n lazÄ±m olan bÃ¼tÃ¼n mÉ™lumatlar daxil edilir.

**Checkpoint-É™ Daxil EdilÉ™nlÉ™r:**

| MÉ™lumat | MÉ™qsÉ™d |
| :--- | :--- |
| **`model.state_dict()`** | Modelin bÃ¼tÃ¼n Ã¶yrÉ™nilmiÅŸ Ã§É™kilÉ™ri. |
| **`optimizer.state_dict()`** | OptimallaÅŸdÄ±rÄ±cÄ±nÄ±n cari vÉ™ziyyÉ™ti (mÉ™sÉ™lÉ™n, AdamW-nin daxili dÉ™yiÅŸÉ™nlÉ™ri). |
| **`epoch` / `step`** | TÉ™limin hansÄ± mÉ™rhÉ™lÉ™dÉ™ dayandÄ±ÄŸÄ±nÄ± gÃ¶stÉ™rir. |
| **`loss`** | Cari vÉ™ ya É™n yaxÅŸÄ± Validasiya Loss-u. |

**Checkpoint Saxlama FunksiyasÄ±:**

```python
def save_checkpoint(model, optimizer, epoch, loss, path):
    """Modelin vÉ™ziyyÉ™tini yadda saxlayÄ±r."""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(checkpoint, path)
    print(f"Checkpoint '{path}' faylÄ±na uÄŸurla yazÄ±ldÄ±.")
```

## 28.3. Checkpoint-dÉ™n BÉ™rpa

TÉ™limi Checkpoint-dÉ™n bÉ™rpa etmÉ™k Ã¼Ã§Ã¼n:

```python
def load_checkpoint(model, optimizer, path):
    """Modelin vÉ™ziyyÉ™tini Checkpoint-dÉ™n bÉ™rpa edir."""
    if not os.path.exists(path):
        print(f"XÉ™ta: Checkpoint faylÄ± '{path}' tapÄ±lmadÄ±.")
        return 0 # 0-cÄ± epoxadan baÅŸla
        
    checkpoint = torch.load(path)
    
    # Model vÉ™ OptimallaÅŸdÄ±rÄ±cÄ±nÄ±n Ã§É™kilÉ™rini bÉ™rpa et
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    
    print(f"Checkpoint-dÉ™n bÉ™rpa olundu. Epoch: {epoch}, Loss: {loss:.4f}")
    return epoch + 1 # NÃ¶vbÉ™ti epoxadan davam et
```

## 28.4. `accelerate` ilÉ™ Checkpoint

ÆgÉ™r siz GÃ¼n 25-dÉ™ Ã¶yrÉ™ndiyimiz kimi `accelerate` istifadÉ™ edirsinizsÉ™, proses daha da sadÉ™lÉ™ÅŸir:

```python
# Saxlamaq
accelerator.save_state("checkpoint_dir")

# BÉ™rpa etmÉ™k
accelerator.load_state("checkpoint_dir")
```

`accelerate` avtomatik olaraq modelin, optimallaÅŸdÄ±rÄ±cÄ±nÄ±n vÉ™ziyyÉ™tini vÉ™ digÉ™r lazÄ±m olan bÃ¼tÃ¼n mÉ™lumatlarÄ± yadda saxlayÄ±r vÉ™ bÉ™rpa edir. **RTX 2050** kimi mÉ™hdud resurslu cihazlarda tÉ™lim edÉ™rkÉ™n, **`accelerate` ilÉ™ Checkpoint** istifadÉ™ etmÉ™k É™n tÃ¶vsiyÉ™ olunan yoldur.

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `train_accelerate.py` skriptinizÉ™ `accelerator.save_state()` É™mrini É™lavÉ™ edin. MÉ™sÉ™lÉ™n, hÉ™r epoxanÄ±n sonunda vÉ™ ya Validasiya Loss-u É™n yaxÅŸÄ± olduÄŸu zaman.
