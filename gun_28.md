# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 28

## Checkpoint vÉ™ Modelin SaxlanmasÄ±: TÉ™limi Qorumaq ğŸ’¾

Salam! DÃ¼nÉ™n modelimizin nÉ™ qÉ™dÉ™r yaxÅŸÄ± Ã¶yrÉ™ndiyini Ã¶lÃ§É™n **Perplexity** metrikini Ã¶yrÉ™ndik. TÉ™lim prosesi uzun Ã§É™kdiyi Ã¼Ã§Ã¼n (bizim 5000 addÄ±mlÄ±q tÉ™limimiz T4 GPU-da bir neÃ§É™ saat Ã§É™kÉ™ bilÉ™r), elektrik kÉ™silmÉ™si vÉ™ ya proqram xÉ™tasÄ± kimi hallara qarÅŸÄ± modelin vÉ™ziyyÉ™tini mÃ¼ntÉ™zÉ™m olaraq yadda saxlamaq lazÄ±mdÄ±r. Bu proses **Checkpoint** adlanÄ±r.

### 1. Checkpoint NÉ™dir?

> **Checkpoint** â€” tÉ™limin mÃ¼É™yyÉ™n bir nÃ¶qtÉ™sindÉ™ modelin bÃ¼tÃ¼n vÉ™ziyyÉ™tinin (Ã§É™kilÉ™r, optimallaÅŸdÄ±rÄ±cÄ±nÄ±n vÉ™ziyyÉ™ti, scheduler-in vÉ™ziyyÉ™ti, cari tÉ™lim addÄ±mÄ±) yadda saxlanÄ±lmasÄ±dÄ±r.

Checkpoint-lÉ™r sayÉ™sindÉ™ tÉ™lim yarÄ±mÃ§Ä±q dayansa belÉ™, biz son yadda saxlanmÄ±ÅŸ nÃ¶qtÉ™dÉ™n tÉ™limÉ™ davam edÉ™ bilÉ™rik.

### 2. Modelin SaxlanmasÄ± ÃœÃ§Ã¼n Ä°ki Æsas Fayl

PyTorch-da modelin saxlanÄ±lmasÄ± Ã¼Ã§Ã¼n iki É™sas Ã¼sul var:

| Ãœsul | NÉ™ SaxlanÄ±lÄ±r? | NÉ™ Ã¼Ã§Ã¼n Ä°stifadÉ™ Olunur? |
| :--- | :--- | :--- |
| **Modelin Ã‡É™kilÉ™ri (State Dict)** | YalnÄ±z modelin Ã¶yrÉ™nilmiÅŸ parametrlÉ™ri (Ã§É™kilÉ™r vÉ™ meyilliklÉ™r). | Modelin **istifadÉ™si** (inference) vÉ™ ya baÅŸqa bir layihÉ™yÉ™ kÃ¶Ã§Ã¼rÃ¼lmÉ™si Ã¼Ã§Ã¼n. |
| **Tam Checkpoint** | Modelin Ã§É™kilÉ™ri, optimallaÅŸdÄ±rÄ±cÄ±nÄ±n vÉ™ziyyÉ™ti, scheduler vÉ™ tÉ™lim addÄ±mÄ±. | TÉ™limin **davam etdirilmÉ™si** Ã¼Ã§Ã¼n. |

Bizim `accelerate` kitabxanamÄ±z hÉ™r iki prosesi asanlaÅŸdÄ±rÄ±r.

### 3. `train.py` SkriptindÉ™ Checkpoint-in TÉ™tbiqi

Biz `train.py` skriptindÉ™ hÉ™r `EVAL_INTERVAL` (500 addÄ±m) keÃ§dikdÉ™ modelin vÉ™ziyyÉ™tini yadda saxlayacaÄŸÄ±q.

```python
# train.py (Æsas TÉ™lim DÃ¶vrÃ¼)

# ... (É™vvÉ™lki kodlar) ...

# 5. Æsas TÉ™lim DÃ¶vrÃ¼
best_val_loss = float('inf') # Æn yaxÅŸÄ± validasiya itkisini izlÉ™mÉ™k Ã¼Ã§Ã¼n
for iter_num in tqdm(range(MAX_ITERS), desc="TÉ™lim Prosesi"):
    
    # A. Validasiya vÉ™ Checkpoint
    if iter_num % EVAL_INTERVAL == 0:
        val_loss, val_ppl = estimate_loss()
        print(f"AddÄ±m {iter_num}: Validasiya Ä°tkisi (Loss) = {val_loss:.4f}, PPL = {val_ppl:.2f}")
        
        # 1. Æn YaxÅŸÄ± Modeli Yadda Saxlamaq
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            # Modelin Ã§É™kilÉ™rini yadda saxlayÄ±rÄ±q
            unwrapped_model = accelerator.unwrap_model(model)
            torch.save(unwrapped_model.state_dict(), 'best_model.pt')
            print(">>> Æn yaxÅŸÄ± model Ã§É™kilÉ™ri 'best_model.pt' faylÄ±na yazÄ±ldÄ±.")
            
        # 2. TÉ™limi Davam EtdirmÉ™k Ã¼Ã§Ã¼n Checkpoint
        accelerator.save_state(f"checkpoint_{iter_num}")
        print(f">>> Checkpoint 'checkpoint_{iter_num}' qovluÄŸuna yazÄ±ldÄ±.")

    # ... (qalan tÉ™lim addÄ±mlarÄ±) ...
```

### 4. Kodun Ä°zahÄ± (Æsas MÉ™qamlar)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 11 | `best_val_loss = float('inf')` | Æn yaxÅŸÄ± modeli mÃ¼É™yyÉ™n etmÉ™k Ã¼Ã§Ã¼n sonsuzluqdan baÅŸlayÄ±rÄ±q. |
| 18 | `if val_loss < best_val_loss:` | ÆgÉ™r cari validasiya itkisi É™vvÉ™lkindÉ™n daha yaxÅŸÄ±dÄ±rsa (kiÃ§ikdirsÉ™). |
| 20 | `unwrapped_model = accelerator.unwrap_model(model)` | `accelerator` modelÉ™ É™lavÉ™ qatlar É™lavÉ™ edir. Ã‡É™kilÉ™ri saxlamaq Ã¼Ã§Ã¼n É™vvÉ™lcÉ™ modeli "aÃ§malÄ±yÄ±q". |
| 21 | `torch.save(unwrapped_model.state_dict(), 'best_model.pt')` | Modelin Ã¶yrÉ™nilmiÅŸ Ã§É™kilÉ™rini (`state_dict`) **`.pt`** faylÄ±na yazÄ±rÄ±q. |
| 24 | `accelerator.save_state(f"checkpoint_{iter_num}")` | TÉ™limi davam etdirmÉ™k Ã¼Ã§Ã¼n lazÄ±m olan bÃ¼tÃ¼n mÉ™lumatlarÄ± (model, optimizer, scheduler) bir qovluÄŸa yazÄ±r. |

### 5. Checkpoint-dÉ™n TÉ™limÉ™ Davam EtmÉ™k

ÆgÉ™r tÉ™lim yarÄ±mÃ§Ä±q dayansa, onu davam etdirmÉ™k Ã¼Ã§Ã¼n `train.py` skriptinin É™vvÉ™linÉ™ bu kodu É™lavÉ™ edirik:

```python
# train.py (ÆvvÉ™ldÉ™)

# ... (bÃ¼tÃ¼n importlar vÉ™ konfiqurasiyalar) ...

# 6. TÉ™limÉ™ Davam EtmÉ™k (Resume)
RESUME_FROM_CHECKPOINT = "checkpoint_2500" # Davam etmÉ™k istÉ™diyiniz qovluÄŸun adÄ±

if RESUME_FROM_CHECKPOINT:
    print(f"TÉ™lim '{RESUME_FROM_CHECKPOINT}' checkpoint-dÉ™n davam etdirilir...")
    accelerator.load_state(RESUME_FROM_CHECKPOINT)
    # Cari addÄ±mÄ± checkpoint-dÉ™n alÄ±rÄ±q
    starting_iteration = int(RESUME_FROM_CHECKPOINT.split('_')[-1])
else:
    starting_iteration = 0

# 7. Æsas TÉ™lim DÃ¶vrÃ¼ (YenilÉ™nmiÅŸ)
# for iter_num in tqdm(range(starting_iteration, MAX_ITERS), desc="TÉ™lim Prosesi"):
# ...
```

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `train.py` skriptinÉ™ Checkpoint saxlama vÉ™ É™n yaxÅŸÄ± modeli yadda saxlama funksiyalarÄ±nÄ± É™lavÉ™ edin.
2.  TÉ™limi bir neÃ§É™ addÄ±m iÅŸlÉ™din, dayandÄ±rÄ±n vÉ™ sonra `RESUME_FROM_CHECKPOINT` dÉ™yiÅŸÉ™nini tÉ™yin edÉ™rÉ™k tÉ™limÉ™ davam edin.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **TÉ™limin SonlandÄ±rÄ±lmasÄ± vÉ™ Modelin HazÄ±rlanmasÄ±** mÃ¶vzusunu Ã¶yrÉ™nÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 800 sÃ¶z.
