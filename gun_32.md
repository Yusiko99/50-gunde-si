# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 32

## PyTorch-dan Hugging Face-É™ Ã‡evirmÉ™ (II HissÉ™) ğŸ§©

Salam! DÃ¼nÉ™n PyTorch Ã§É™kilÉ™rini Hugging Face (HF) modelinÉ™ kÃ¶Ã§Ã¼rmÉ™ prosesinÉ™ baÅŸladÄ±q. Bu gÃ¼n isÉ™ É™n Ã§É™tin hissÉ™ni â€“ **Ã‡oxbaÅŸlÄ± DiqqÉ™t (MHA)** qatÄ±nÄ±n Ã§É™kilÉ™rinin uyÄŸunlaÅŸdÄ±rÄ±lmasÄ±nÄ± vÉ™ **Tokenizatorun** saxlanmasÄ±nÄ± tamamlayÄ±rÄ±q.

### 1. MHA Ã‡É™kilÉ™rinin KÃ¶Ã§Ã¼rÃ¼lmÉ™si

Bizim NanoGPT modelimizdÉ™ Q, K, V (Query, Key, Value) Ã§É™kilÉ™ri **`c_attn`** adlÄ± tÉ™k bir xÉ™tti qatda birlÉ™ÅŸdirilmiÅŸdi. Hugging Face GPT-2 modelindÉ™ isÉ™ bu Ã§É™kilÉ™r ayrÄ±-ayrÄ± qatlarda saxlanÄ±lÄ±r.

Bizim `c_attn` Ã§É™kisini 3 bÉ™rabÉ™r hissÉ™yÉ™ bÃ¶lÃ¼b, HF modelinin Q, K, V Ã§É™kilÉ™rinÉ™ kopyalamalÄ±yÄ±q.

#### `export_hf.py` Skriptinin YenilÉ™nmÉ™si

`convert_nano_to_hf` funksiyasÄ±na bu hissÉ™ni É™lavÉ™ edirik:

```python
# export_hf.py (convert_nano_to_hf funksiyasÄ±nÄ±n iÃ§indÉ™)

# ... (É™vvÉ™lki kodlar) ...

    # Transformer BloklarÄ±nÄ±n (12 É™dÉ™d) Ã§É™kilÉ™rini kÃ¶Ã§Ã¼rÃ¼rÃ¼k
    for i in range(nano_model.config.n_layer):
        # ... (Layer Norms vÉ™ FFN-in kÃ¶Ã§Ã¼rÃ¼lmÉ™si) ...
        
        # Multi-Head Attention (MHA) Ã‡É™kilÉ™rinin KÃ¶Ã§Ã¼rÃ¼lmÉ™si
        
        # 1. NanoGPT-dÉ™n birlÉ™ÅŸdirilmiÅŸ QKV Ã§É™kilÉ™rini alÄ±rÄ±q
        qkv_weight = nano_state_dict[f'transformer.h.{i}.attn.c_attn.weight']
        qkv_bias = nano_state_dict[f'transformer.h.{i}.attn.c_attn.bias']
        
        # 2. Ã‡É™kilÉ™ri 3 bÉ™rabÉ™r hissÉ™yÉ™ bÃ¶lÃ¼rÃ¼k (Q, K, V)
        # HÉ™r hissÉ™nin Ã¶lÃ§Ã¼sÃ¼ n_embd (768)
        q_w, k_w, v_w = torch.chunk(qkv_weight, 3, dim=0)
        q_b, k_b, v_b = torch.chunk(qkv_bias, 3, dim=0)
        
        # 3. Hugging Face modelinÉ™ kopyalayÄ±rÄ±q
        # HF-dÉ™ Q, K, V birlÉ™ÅŸdirilmiÅŸ ÅŸÉ™kildÉ™ saxlanÄ±lÄ±r
        hf_qkv_weight = torch.cat([q_w, k_w, v_w], dim=0)
        hf_qkv_bias = torch.cat([q_b, k_b, v_b], dim=0)
        
        # HF modelinin state dict-inÉ™ kopyalayÄ±rÄ±q
        hf_state_dict[f'transformer.h.{i}.attn.c_attn.weight'].copy_(hf_qkv_weight)
        hf_state_dict[f'transformer.h.{i}.attn.c_attn.bias'].copy_(hf_qkv_bias)
        
        # MHA-nÄ±n son proyeksiya qatÄ±nÄ± kopyalayÄ±rÄ±q
        hf_state_dict[f'transformer.h.{i}.attn.c_proj.weight'].copy_(
            nano_state_dict[f'transformer.h.{i}.attn.c_proj.weight']
        )
        hf_state_dict[f'transformer.h.{i}.attn.c_proj.bias'].copy_(
            nano_state_dict[f'transformer.h.{i}.attn.c_proj.bias']
        )
        
        # ... (qalan kodlar) ...
```

**Kodun Ä°zahÄ±:**
*   `torch.chunk(qkv_weight, 3, dim=0)`: BirlÉ™ÅŸdirilmiÅŸ Ã§É™ki matrisini (Ã¶lÃ§Ã¼sÃ¼ 3 * 768) 3 bÉ™rabÉ™r hissÉ™yÉ™ (hÉ™r biri 768 Ã¶lÃ§Ã¼lÃ¼) bÃ¶lÃ¼r.
*   `torch.cat([q_w, k_w, v_w], dim=0)`: BÉ™zi HF modellÉ™ri Q, K, V-ni birlÉ™ÅŸdirilmiÅŸ ÅŸÉ™kildÉ™ saxlayÄ±r. Biz dÉ™ bÃ¶lÃ¼b yenidÉ™n birlÉ™ÅŸdiririk.

### 2. Tokenizatorun SaxlanmasÄ±

Bizim tokenizatorumuz Hugging Face-in `tokenizers` kitabxanasÄ± ilÉ™ yaradÄ±lÄ±b. Onu HF-in `transformers` kitabxanasÄ±nÄ±n istifadÉ™ edÉ™ bilÉ™cÉ™yi formata Ã§evirmÉ™liyik.

#### `export_hf.py` Skriptinin YenilÉ™nmÉ™si (Æsas Ä°cra Bloku)

```python
# export_hf.py (Æsas Ä°cra Bloku)

# ... (É™vvÉ™lki kodlar) ...

    # 5. Tokenizatoru saxla
    tokenizer = Tokenizer.from_file("az_bpe_tokenizer.json")
    
    # Hugging Face Tokenizatorunu yaratmaq Ã¼Ã§Ã¼n
    from transformers import PreTrainedTokenizerFast
    
    # Tokenizatoru HF formatÄ±nda saxlamaq Ã¼Ã§Ã¼n
    hf_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        bos_token="<|endoftext|>", # BaÅŸlanÄŸÄ±c tokeni
        eos_token="<|endoftext|>", # Son tokeni
        unk_token="<|endoftext|>", # BilinmÉ™yÉ™n token
        pad_token="<|endoftext|>", # Padding tokeni
    )
    
    # Tokenizatoru qovluÄŸa yazÄ±rÄ±q
    hf_tokenizer.save_pretrained("az_llm_hf")
    print("Tokenizator 'az_llm_hf' qovluÄŸuna yazÄ±ldÄ±.")
    
    # 6. Modeli saxla
    hf_model.save_pretrained("az_llm_hf")
    print("Model 'az_llm_hf' qovluÄŸuna yazÄ±ldÄ±.")
    
    print("\nPyTorch-dan Hugging Face-É™ Ã§evirmÉ™ uÄŸurla tamamlandÄ±!")
```

### 3. Yekun Ä°cra

Ä°ndi `export_hf.py` skriptini icra etdikdÉ™, **`az_llm_hf`** adlÄ± bir qovluq yaranacaq. Bu qovluÄŸun iÃ§indÉ™ modelin Ã§É™kilÉ™ri (`pytorch_model.bin`) vÉ™ tokenizator fayllarÄ± (`tokenizer.json`, `tokenizer_config.json` vÉ™ s.) olacaq.

Bu qovluq artÄ±q Hugging Face ekosistemindÉ™ istifadÉ™ oluna bilÉ™r.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `export_hf.py` faylÄ±nÄ± yuxarÄ±dakÄ± kodla tamamlayÄ±n.
2.  `pip install transformers` É™mrini icra edin (É™gÉ™r É™vvÉ™lki gÃ¼n etmÉ™misinizsÉ™).
3.  Skripti icra edin: `python export_hf.py`.
4.  Yaranan `az_llm_hf` qovluÄŸunun iÃ§indÉ™ki fayllarÄ± yoxlayÄ±n.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **GGUF FormatÄ±na Ã‡evirmÉ™** prosesinÉ™ baÅŸlayÄ±rÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
