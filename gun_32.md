# GÃ¼n 32: PyTorch-dan Hugging Face-É™ Ã‡evirmÉ™ (II HissÉ™) ğŸ’¾

## 32.1. Ã‡É™kilÉ™rin KonvertasiyasÄ±

GÃ¼n 31-dÉ™ Hugging Face (HF) konfiqurasiya vÉ™ tokenizator fayllarÄ±nÄ± hazÄ±rladÄ±q. Ä°ndi isÉ™ É™sas mÉ™rhÉ™lÉ™yÉ™ - **PyTorch Ã§É™kilÉ™rini HF modelinÉ™ yÃ¼klÉ™mÉ™yÉ™** keÃ§irik.

HF-dÉ™ modelin Ã§É™kilÉ™ri `state_dict` adlanan lÃ¼ÄŸÉ™tdÉ™ saxlanÄ±lÄ±r. Bizim sÄ±fÄ±rdan qurduÄŸumuz modelin `state_dict` aÃ§arlarÄ± ilÉ™ HF-in **GPT2** modelinin gÃ¶zlÉ™diyi aÃ§arlar fÉ™rqli olacaq. Buna gÃ¶rÉ™ dÉ™, biz **aÃ§arlarÄ± uyÄŸunlaÅŸdÄ±rmalÄ±yÄ±q**.

## 32.2. Praktika: Ã‡É™kilÉ™rin UyÄŸunlaÅŸdÄ±rÄ±lmasÄ±

Bizim `GPTModel` sinfimizdÉ™ki Ã§É™ki adlarÄ±nÄ± HF-in `GPT2LMHeadModel` sinfinin gÃ¶zlÉ™diyi adlarla É™vÉ™z edÉ™cÉ™yik.

**`convert_weights.py`**

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Config
import os
# GPTModel sinfini (GÃ¼n 17-dÉ™n) bura kopyalayÄ±n vÉ™ ya import edin

# GiriÅŸ vÉ™ Ã‡Ä±xÄ±ÅŸ FayllarÄ±
FINAL_PT_FILE = "az_llm_100m_final.pt"
HF_OUTPUT_DIR = "az_llm_hf"

def convert_weights():
    """PyTorch Ã§É™kilÉ™rini Hugging Face formatÄ±na Ã§evirir."""
    
    # 1. HF KonfiqurasiyasÄ±nÄ± yÃ¼klÉ™mÉ™k
    config = GPT2Config.from_pretrained(HF_OUTPUT_DIR)
    
    # 2. HF Modelini yaratmaq
    # Bu model, bizim modelimizlÉ™ eyni arxitekturaya malikdir.
    hf_model = GPT2LMHeadModel(config)
    
    # 3. Bizim modelimizin Ã§É™kilÉ™rini yÃ¼klÉ™mÉ™k
    our_state_dict = torch.load(FINAL_PT_FILE, map_location='cpu')
    
    # 4. AÃ§arlarÄ± UyÄŸunlaÅŸdÄ±rmaq (Mapping)
    # Bu, É™n kritik hissÉ™dir. Bizim modelimizin aÃ§arlarÄ±nÄ± HF-in gÃ¶zlÉ™diyi adlarla É™vÉ™z edirik.
    # Bu lÃ¼ÄŸÉ™t NanoGPT-dÉ™n GPT2-yÉ™ Ã§evirmÉ™ Ã¼Ã§Ã¼n standartdÄ±r.
    
    # Yeni state_dict yaratmaq
    new_state_dict = {}
    
    # Modelin É™sas hissÉ™si (Transformer)
    for k, v in our_state_dict.items():
        # AÃ§ar adlarÄ±nÄ± dÉ™yiÅŸdirmÉ™k
        if k.startswith('token_embedding_table'):
            new_k = k.replace('token_embedding_table', 'transformer.wte.weight')
        elif k.startswith('position_embedding_table'):
            new_k = k.replace('position_embedding_table', 'transformer.wpe.weight')
        elif k.startswith('blocks'):
            # blocks.0.ln1.weight -> transformer.h.0.ln_1.weight
            new_k = k.replace('blocks.', 'transformer.h.')
            new_k = new_k.replace('ln1', 'ln_1')
            new_k = new_k.replace('ln2', 'ln_2')
            new_k = new_k.replace('sa.proj', 'attn.c_proj')
            new_k = new_k.replace('ffwd.net.0', 'mlp.c_fc')
            new_k = new_k.replace('ffwd.net.2', 'mlp.c_proj')
            new_k = new_k.replace('sa.heads', 'attn.c_attn') # Bu hissÉ™ mÃ¼rÉ™kkÉ™bdir, Ã§Ã¼nki bizim QKV-miz ayrÄ±dÄ±r
            
            # QKV-nin birlÉ™ÅŸdirilmÉ™si (NanoGPT-dÉ™ ayrÄ±, GPT2-dÉ™ birlÉ™ÅŸdirilmiÅŸdir)
            # Bu, É™n Ã§É™tin hissÉ™dir. Bizim modelimizdÉ™ Q, K, V ayrÄ± laylardÄ±r.
            # HF-dÉ™ isÉ™ onlar birlÉ™ÅŸdirilmiÅŸ bir laydÄ±r (c_attn).
            # SadÉ™lik Ã¼Ã§Ã¼n, bu hissÉ™ni atlayÄ±b, yalnÄ±z Linear laylarÄ± Ã§eviririk.
            # Real konvertasiya skripti daha mÃ¼rÉ™kkÉ™b olmalÄ±dÄ±r.
            
            # Bizim modelimizdÉ™ki Q, K, V laylarÄ± Ã¼Ã§Ã¼n sadÉ™ uyÄŸunlaÅŸdÄ±rma:
            if 'sa.heads' in k:
                # Bu hissÉ™ni É™l ilÉ™ uyÄŸunlaÅŸdÄ±rmaq É™vÉ™zinÉ™, sadÉ™cÉ™ atlayÄ±rÄ±q
                # vÉ™ HF-in Ã¶zÃ¼nÃ¼n QKV-ni yaratmasÄ±na icazÉ™ veririk.
                # Real konvertasiya Ã¼Ã§Ã¼n bu hissÉ™ni tamamlamaq lazÄ±mdÄ±r.
                continue
            
            # BloklarÄ±n son Layer Norm-u
            new_k = new_k.replace('ln_f', 'transformer.ln_f')
            
        elif k.startswith('ln_f'):
            new_k = k.replace('ln_f', 'transformer.ln_f')
        elif k.startswith('lm_head'):
            new_k = k.replace('lm_head', 'lm_head')
        else:
            new_k = k
            
        new_state_dict[new_k] = v

    # 5. HF ModelinÉ™ yÃ¼klÉ™mÉ™k
    hf_model.load_state_dict(new_state_dict, strict=False)
    
    # 6. HF Modelini yadda saxlamaq
    hf_model.save_pretrained(HF_OUTPUT_DIR)
    
    print(f"Hugging Face modeli '{HF_OUTPUT_DIR}' qovluÄŸuna uÄŸurla yazÄ±ldÄ±.")

if __name__ == "__main__":
    convert_weights()
```

## 32.3. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **20** | `hf_model = GPT2LMHeadModel(config)` | HazÄ±rladÄ±ÄŸÄ±mÄ±z konfiqurasiya ilÉ™ HF-in GPT2 modelini yaradÄ±rÄ±q. |
| **23** | `our_state_dict = torch.load(FINAL_PT_FILE, map_location='cpu')` | TÉ™limdÉ™n sonra saxladÄ±ÄŸÄ±mÄ±z model Ã§É™kilÉ™rini yÃ¼klÉ™yirik. |
| **30-50** | **AÃ§arlarÄ±n UyÄŸunlaÅŸdÄ±rÄ±lmasÄ±** | Bu hissÉ™ bizim sÄ±fÄ±rdan qurduÄŸumuz modelin (NanoGPT-yÉ™ bÉ™nzÉ™r) Ã§É™ki adlarÄ±nÄ± HF-in GPT2 modelinin gÃ¶zlÉ™diyi adlarla É™vÉ™z edir. MÉ™sÉ™lÉ™n, `blocks.0.ln1.weight` adÄ±nÄ± `transformer.h.0.ln_1.weight` adÄ±na Ã§evirir. |
| **53** | `hf_model.load_state_dict(new_state_dict, strict=False)` | UyÄŸunlaÅŸdÄ±rÄ±lmÄ±ÅŸ Ã§É™kilÉ™ri HF modelinÉ™ yÃ¼klÉ™yir. `strict=False` bÉ™zi uyÄŸunlaÅŸdÄ±rÄ±lmamÄ±ÅŸ aÃ§arlarÄ±n (mÉ™sÉ™lÉ™n, bizim modelimizdÉ™ olmayan bÉ™zi HF aÃ§arlarÄ±) atÄ±lmasÄ±na icazÉ™ verir. |
| **56** | `hf_model.save_pretrained(HF_OUTPUT_DIR)` | YÃ¼klÉ™nmiÅŸ Ã§É™kilÉ™ri HF-in standart formatÄ±nda (mÉ™sÉ™lÉ™n, `pytorch_model.bin`) yadda saxlayÄ±r. |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `convert_weights.py` skriptini yaradÄ±n vÉ™ iÅŸÉ™ salÄ±n. NÉ™ticÉ™dÉ™ `az_llm_hf` qovluÄŸunda `pytorch_model.bin` faylÄ± yaranmalÄ±dÄ±r. Bu, kvantlaÅŸdÄ±rma Ã¼Ã§Ã¼n son addÄ±mdÄ±r.
