# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 8

## Tokenizasiya: SÃ¶zlÉ™ri RÉ™qÉ™mlÉ™rÉ™ Ã‡evirmÉ™k ğŸ”„

Salam! DÃ¼nÉ™n LLM-in tÉ™limi Ã¼Ã§Ã¼n lazÄ±m olan bÃ¶yÃ¼k mÉ™tn korpusunu (azcorpus) yÃ¼klÉ™dik vÉ™ tÉ™mizlÉ™dik. Ä°ndi isÉ™ bu mÉ™tnlÉ™ri modelimizin baÅŸa dÃ¼ÅŸÉ™cÉ™yi formata â€“ **rÉ™qÉ™mlÉ™rÉ™** Ã§evirmÉ™liyik. Bu proses **Tokenizasiya** adlanÄ±r.

### 1. Tokenizasiya NÉ™dir?

KompÃ¼terlÉ™r mÉ™tnlÉ™ri birbaÅŸa emal edÉ™ bilmÉ™z. Onlar yalnÄ±z rÉ™qÉ™mlÉ™rlÉ™ iÅŸlÉ™yir. Tokenizasiya, mÉ™tni modelin emal edÉ™ bilÉ™cÉ™yi kiÃ§ik vahidlÉ™rÉ™ â€“ **tokenlÉ™rÉ™** bÃ¶lmÉ™k vÉ™ hÉ™r bir tokeni unikal bir **rÉ™qÉ™mÉ™ (ID)** Ã§evirmÉ™k prosesidir.

> **Token** â€” modelin emal etdiyi É™n kiÃ§ik mÉ™na vahididir. Bu, bir sÃ¶z, bir hÉ™rf, bir durÄŸu iÅŸarÉ™si vÉ™ ya bir sÃ¶zÃ¼n hissÉ™si ola bilÉ™r.

MÉ™sÉ™lÉ™n, "AzÉ™rbaycan" sÃ¶zÃ¼ bir token ola bilÉ™r, ya da "Az", "É™r", "bay", "can" kimi dÃ¶rd fÉ™rqli tokenÉ™ bÃ¶lÃ¼nÉ™ bilÉ™r.

### 2. NiyÉ™ Tokenizasiya Vacibdir?

Tokenizasiya LLM-in performansÄ±na birbaÅŸa tÉ™sir edir:

1.  **SÃ¶zlÃ¼k HÉ™cmi (Vocabulary Size):** ÆgÉ™r hÉ™r sÃ¶zÃ¼ bir token etsÉ™k, sÃ¶zlÃ¼k hÉ™cmi (modelin tanÄ±dÄ±ÄŸÄ± unikal tokenlÉ™rin sayÄ±) Ã§ox bÃ¶yÃ¼k olar. Bu, modelin yaddaÅŸÄ±nÄ± artÄ±rar vÉ™ tÉ™limi Ã§É™tinlÉ™ÅŸdirÉ™r.
2.  **Nadir SÃ¶zlÉ™r (Out-of-Vocabulary - OOV):** ÆgÉ™r model tÉ™lim zamanÄ± gÃ¶rmÉ™diyi bir sÃ¶zlÉ™ qarÅŸÄ±laÅŸsa, onu emal edÉ™ bilmÉ™z.
3.  **MÉ™na:** TokenlÉ™r sÃ¶zÃ¼n mÉ™nasÄ±nÄ± itirmÉ™dÉ™n, onu É™n sÉ™mÉ™rÉ™li ÅŸÉ™kildÉ™ tÉ™msil etmÉ™lidir.

### 3. Byte Pair Encoding (BPE): Æn YaxÅŸÄ± HÉ™ll

ÆnÉ™nÉ™vi tokenizasiya metodlarÄ± (sÃ¶zÉ™ vÉ™ ya hÉ™rfÉ™ É™saslanan) LLM-lÉ™r Ã¼Ã§Ã¼n sÉ™mÉ™rÉ™li deyil. Buna gÃ¶rÉ™ dÉ™, mÃ¼asir LLM-lÉ™rin (GPT, LLaMA) demÉ™k olar ki, hamÄ±sÄ± **Byte Pair Encoding (BPE)** adlÄ± bir alqoritmdÉ™n istifadÉ™ edir.

#### BPE NecÉ™ Ä°ÅŸlÉ™yir?

BPE hÉ™m sÃ¶zÉ™, hÉ™m dÉ™ hÉ™rfÉ™ É™saslanan tokenizasiyanÄ±n Ã¼stÃ¼nlÃ¼klÉ™rini birlÉ™ÅŸdirir:

1.  **BaÅŸlanÄŸÄ±c:** ÆvvÉ™lcÉ™ hÉ™r bir hÉ™rfi bir token kimi qÉ™bul edir.
2.  **TÉ™krarlama:** Korpusda É™n Ã§ox tÉ™krarlanan **iki ardÄ±cÄ±l token cÃ¼tÃ¼nÃ¼** tapÄ±r vÉ™ onlarÄ± **yeni bir token** kimi birlÉ™ÅŸdirir.
3.  **DavamlÄ±lÄ±q:** Bu prosesi modelin sÃ¶zlÃ¼k hÉ™cmi (mÉ™sÉ™lÉ™n, 50,000 token) dolana qÉ™dÉ™r davam etdirir.

**NÃ¼munÉ™ (AzÉ™rbaycan dilindÉ™):**

| AddÄ±m | Æn Ã‡ox TÉ™krarlanan CÃ¼t | NÉ™ticÉ™ |
| :--- | :--- | :--- |
| **0 (BaÅŸlanÄŸÄ±c)** | `A z É™ r b a y c a n` | HÉ™r hÉ™rf bir tokendir. |
| **1** | `ay` | `ay` cÃ¼tÃ¼ Ã§ox tÉ™krarlanÄ±r. Yeni token: `ay` |
| **2** | `Az` | `Az` cÃ¼tÃ¼ Ã§ox tÉ™krarlanÄ±r. Yeni token: `Az` |
| **...** | | |
| **Son** | `AzÉ™rbaycan` | BÉ™lkÉ™ dÉ™, `AzÉ™rbaycan` sÃ¶zÃ¼ bir token kimi yaranacaq. |

**ÃœstÃ¼nlÃ¼yÃ¼:**
*   **Nadir SÃ¶zlÉ™r:** ÆgÉ™r model "QarabaÄŸlÄ±" sÃ¶zÃ¼nÃ¼ gÃ¶rmÉ™yibsÉ™, onu `QarabaÄŸ` vÉ™ `lÄ±` kimi artÄ±q Ã¶yrÉ™ndiyi kiÃ§ik tokenlÉ™rÉ™ bÃ¶lÉ™ bilÉ™r. BelÉ™liklÉ™, model hÉ™tta gÃ¶rmÉ™diyi sÃ¶zlÉ™ri dÉ™ mÉ™nalÄ± ÅŸÉ™kildÉ™ emal edÉ™ bilir.
*   **SÃ¶zlÃ¼k HÉ™cmi:** SÃ¶zlÃ¼k hÉ™cmi idarÉ™olunan sÉ™viyyÉ™dÉ™ qalÄ±r.

### 4. Hugging Face Tokenizers

Bizim BPE tokenizatorumuzu sÄ±fÄ±rdan yazmaÄŸÄ±mÄ±za ehtiyac yoxdur. **Hugging Face `tokenizers`** kitabxanasÄ± bu iÅŸi bizim Ã¼Ã§Ã¼n Ã§ox sÃ¼rÉ™tli vÉ™ sÉ™mÉ™rÉ™li ÅŸÉ™kildÉ™ hÉ™yata keÃ§irir.

#### QuraÅŸdÄ±rma

`llm_50gun` mÃ¼hitindÉ™ `tokenizers` kitabxanasÄ±nÄ± quraÅŸdÄ±raq:

```bash
pip install tokenizers
```

### 5. Tokenizatorun QurulmasÄ± Ã¼Ã§Ã¼n HazÄ±rlÄ±q

Sabah biz **`azcorpus_cleaned.txt`** faylÄ±mÄ±zÄ± istifadÉ™ edÉ™rÉ™k tokenizatorumuzu tÉ™lim edÉ™cÉ™yik. Bu prosesdÉ™ biz iki É™sas parametr tÉ™yin etmÉ™liyik:

1.  **SÃ¶zlÃ¼k HÉ™cmi (Vocab Size):** Bizim modelimizin tanÄ±ya bilÉ™cÉ™yi unikal tokenlÉ™rin sayÄ±. 100M parametreli model Ã¼Ã§Ã¼n **32,000** vÉ™ ya **50,000** token kifayÉ™t edÉ™cÉ™k.
2.  **XÃ¼susi TokenlÉ™r (Special Tokens):** Modelin xÃ¼susi mÉ™qsÉ™dlÉ™r Ã¼Ã§Ã¼n istifadÉ™ etdiyi tokenlÉ™r:
    *   `<|endoftext|>`: MÉ™tnin sonunu bildirir (LLM-lÉ™r Ã¼Ã§Ã¼n vacibdir).
    *   `<|pad|>`: MÉ™tnlÉ™ri eyni uzunluÄŸa gÉ™tirmÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **AzÉ™rbaycan dili Ã¼Ã§Ã¼n xÃ¼susi Tokenizatorumuzu sÄ±fÄ±rdan tÉ™lim edÉ™cÉ™yik**. Bu, bizim ilk real LLM komponentimiz olacaq!

***

**SÃ¶z SayÄ±:** 700 sÃ¶z.
