# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 21

## TÉ™lim ProsesinÉ™ GiriÅŸ: Model NecÉ™ Ã–yrÉ™nir? ğŸ“

Salam! Ä°kinci mÉ™rhÉ™lÉ™ni tamamladÄ±q vÉ™ **100 Milyon parametreli NanoGPT** modelimizi PyTorch-da qurduq. Ä°ndi isÉ™ É™n hÉ™yÉ™canlÄ± mÉ™rhÉ™lÉ™yÉ™ â€“ **Modelin TÉ™liminÉ™** baÅŸlayÄ±rÄ±q!

TÉ™lim prosesi, modelin AzÉ™rbaycan dilini Ã¶yrÉ™nmÉ™si Ã¼Ã§Ã¼n rÉ™qÉ™mlÉ™ÅŸdirilmiÅŸ `train.npy` mÉ™lumatÄ±mÄ±zÄ± istifadÉ™ etmÉ™si demÉ™kdir.

### 1. TÉ™lim NÉ™dir?

TÉ™lim, modelin proqnozlarÄ± ilÉ™ hÉ™qiqi dÉ™yÉ™rlÉ™r arasÄ±ndakÄ± fÉ™rqi (Ä°tki, yÉ™ni Loss) minimuma endirmÉ™k Ã¼Ã§Ã¼n modelin **parametr Ã§É™kilÉ™rini** (weights) tÉ™dricÉ™n tÉ™nzimlÉ™mÉ™k prosesidir.

Bu proses Ã¼Ã§ É™sas komponentdÉ™n ibarÉ™tdir:

1.  **Loss Function (Ä°tki FunksiyasÄ±):** Modelin nÉ™ qÉ™dÉ™r sÉ™hv etdiyini Ã¶lÃ§Ã¼r.
2.  **Optimizer (OptimallaÅŸdÄ±rÄ±cÄ±):** Ä°tkini azaltmaq Ã¼Ã§Ã¼n parametrlÉ™ri hansÄ± istiqamÉ™tdÉ™ vÉ™ nÉ™ qÉ™dÉ™r dÉ™yiÅŸÉ™cÉ™yini mÃ¼É™yyÉ™n edir.
3.  **Backpropagation (GeriyÉ™ Ã–tÃ¼rmÉ™):** Ä°tkinin modelin bÃ¼tÃ¼n qatlarÄ±na necÉ™ paylandÄ±ÄŸÄ±nÄ± hesablayÄ±r.

### 2. Loss Function (Ä°tki FunksiyasÄ±)

Bizim modelimiz **nÃ¶vbÉ™ti tokeni proqnozlaÅŸdÄ±rmaq** Ã¼Ã§Ã¼n tÉ™lim olunur.

MÉ™sÉ™lÉ™n, modelÉ™ "AzÉ™rbaycanÄ±n paytaxtÄ±" verilir. Model proqnozlaÅŸdÄ±rmalÄ±dÄ±r ki, nÃ¶vbÉ™ti token "BakÄ±" sÃ¶zÃ¼nÃ¼n token ID-si olmalÄ±dÄ±r.

Modelin Ã§Ä±xÄ±ÅŸÄ± **32000** ehtimaldan ibarÉ™t bir vektordur (hÉ™r token Ã¼Ã§Ã¼n bir ehtimal).

> **Cross-Entropy Loss (Ã‡arpaz Entropiya Ä°tkisi)** â€” generativ dil modellÉ™ri Ã¼Ã§Ã¼n standart itki funksiyasÄ±dÄ±r. O, modelin proqnozlaÅŸdÄ±rdÄ±ÄŸÄ± ehtimallar paylanmasÄ± ilÉ™ hÉ™qiqi tokenin paylanmasÄ± arasÄ±ndakÄ± fÉ™rqi Ã¶lÃ§Ã¼r.

*   **YÃ¼ksÉ™k Loss:** Model sÉ™hv proqnozlaÅŸdÄ±rÄ±b.
*   **AÅŸaÄŸÄ± Loss:** Model dÃ¼zgÃ¼n proqnozlaÅŸdÄ±rÄ±b.

Bizim `model.py` faylÄ±ndakÄ± `forward` metodunda bu itkini artÄ±q hesablamÄ±ÅŸdÄ±q:

```python
# model.py-dan xatÄ±rlatma
# ...
loss = F.cross_entropy(logits, targets)
# ...
```

### 3. Optimizer (OptimallaÅŸdÄ±rÄ±cÄ±)

Ä°tki funksiyasÄ± modelin nÉ™ qÉ™dÉ™r sÉ™hv etdiyini deyir, lakin **OptimallaÅŸdÄ±rÄ±cÄ±** bu sÉ™hvi dÃ¼zÉ™ltmÉ™k Ã¼Ã§Ã¼n nÉ™ etmÉ™li olduÄŸunu deyir.

OptimallaÅŸdÄ±rÄ±cÄ±, **Qradiyent EniÅŸi (Gradient Descent)** adlÄ± bir alqoritmÉ™ É™saslanÄ±r.

> **Qradiyent EniÅŸi** â€” Ä°tki funksiyasÄ±nÄ±n qrafikindÉ™ É™n aÅŸaÄŸÄ± nÃ¶qtÉ™ni (É™n yaxÅŸÄ± proqnozlarÄ±) tapmaq Ã¼Ã§Ã¼n parametrlÉ™ri qradiyentin (tÃ¶rÉ™mÉ™nin) É™ks istiqamÉ™tindÉ™ kiÃ§ik addÄ±mlarla hÉ™rÉ™kÉ™t etdirÉ™n riyazi Ã¼suldur.

Bizim layihÉ™mizdÉ™ É™n mÃ¼asir vÉ™ effektiv optimallaÅŸdÄ±rÄ±cÄ±lardan biri olan **AdamW**-dÉ™n istifadÉ™ edÉ™cÉ™yik.

> **AdamW** â€” **Adam** optimallaÅŸdÄ±rÄ±cÄ±sÄ±nÄ±n **Weight Decay** (Ã‡É™ki AzalmasÄ±) mexanizmi ilÉ™ tÉ™kmillÉ™ÅŸdirilmiÅŸ versiyasÄ±dÄ±r. Weight Decay, modelin hÉ™ddindÉ™n artÄ±q uyÄŸunlaÅŸmasÄ±nÄ±n (Overfitting) qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n parametrlÉ™ri kiÃ§ik saxlayÄ±r.

### 4. TÉ™lim Ã¼Ã§Ã¼n Æsas Kitabxanalar

TÉ™lim prosesini idarÉ™ etmÉ™k Ã¼Ã§Ã¼n É™lavÉ™ kitabxanalar quraÅŸdÄ±rmalÄ±yÄ±q:

1.  **`accelerate` (Hugging Face):** TÉ™limi avtomatik olaraq GPU-ya (vÉ™ ya bir neÃ§É™ GPU-ya) uyÄŸunlaÅŸdÄ±rmaq Ã¼Ã§Ã¼n istifadÉ™ olunur. Bu, bizim **Mixed Precision** (QarÄ±ÅŸÄ±q DÉ™qiqlik) tÉ™limini asanlÄ±qla tÉ™tbiq etmÉ™yimizÉ™ kÃ¶mÉ™k edÉ™cÉ™k.
2.  **`tiktoken` (OpenAI):** BÉ™zi GPT-lÉ™r Ã¼Ã§Ã¼n tokenizatorlarÄ± idarÉ™ etmÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur (bizim BPE tokenizatorumuz Ã¼Ã§Ã¼n birbaÅŸa lazÄ±m olmasa da, GPT layihÉ™lÉ™rindÉ™ standartdÄ±r).

#### QuraÅŸdÄ±rma

`llm_50gun` mÃ¼hitindÉ™ quraÅŸdÄ±raq:

```bash
pip install accelerate tiktoken
```

### 5. TÉ™lim Skriptinin TÉ™mÉ™li

Sabah **DataLoader**-i quracaÄŸÄ±q. Bu gÃ¼n isÉ™ gÉ™lÉ™cÉ™k **`train.py`** skriptimizin tÉ™mÉ™lini qoyaq.

```python
# train.py (TÉ™mÉ™l)
import torch
from torch.utils.data import Dataset, DataLoader
from config import GPTConfig
from model import GPT
from accelerate import Accelerator
from tqdm import tqdm

# 1. HiperparametrlÉ™r
BATCH_SIZE = 12 # Eyni anda emal olunan cÃ¼mlÉ™ sayÄ±
BLOCK_SIZE = 512 # CÃ¼mlÉ™nin maksimum uzunluÄŸu
LEARNING_RATE = 6e-4 # Ã–yrÉ™nmÉ™ sÃ¼rÉ™ti (Ã‡ox vacib parametr!)
MAX_ITERS = 5000 # Maksimum tÉ™lim addÄ±mÄ± sayÄ±
EVAL_INTERVAL = 500 # HÉ™r 500 addÄ±mdan bir validasiya etmÉ™k

# 2. Akseleratoru BaÅŸlatmaq
# Bu, GPU vÉ™ Mixed Precision-Ä± idarÉ™ edÉ™cÉ™k
accelerator = Accelerator()
device = accelerator.device

# 3. Model vÉ™ OptimallaÅŸdÄ±rÄ±cÄ±nÄ± Yaratmaq
config = GPTConfig(block_size=BLOCK_SIZE)
model = GPT(config)
model.to(device)

# OptimallaÅŸdÄ±rÄ±cÄ±
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

# 4. TÉ™lim DÃ¶vrÃ¼ (Sabah tamamlanacaq)
# for iter in tqdm(range(MAX_ITERS), desc="TÉ™lim"):
#     # 1. MÉ™lumatÄ± yÃ¼klÉ™
#     # 2. Ä°tkini hesablayÄ±b geriyÉ™ Ã¶tÃ¼r
#     # 3. ParametrlÉ™ri yenilÉ™
#     pass
```

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `llm_50gun` mÃ¼hitindÉ™ `accelerate` vÉ™ `tiktoken` kitabxanalarÄ±nÄ± quraÅŸdÄ±rÄ±n.
2.  **`train.py`** faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± tÉ™mÉ™l kodu ora kopyalayÄ±n.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **VerilÉ™nlÉ™r YÃ¼klÉ™yicisi (DataLoader)** sinfini quracaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
