# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 16

## Transformer Blokunun QurulmasÄ± ğŸ—ï¸

Salam! DÃ¼nÉ™n NanoGPT modelimizin É™n mÃ¼rÉ™kkÉ™b hissÉ™si olan **Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention)** mexanizmini PyTorch-da qurduq. Bu gÃ¼n isÉ™ bu mexanizmi digÉ™r É™sas komponentlÉ™rlÉ™ birlÉ™ÅŸdirÉ™rÉ™k **Transformer Blokunu** (vÉ™ ya NanoGPT-dÉ™ki adÄ±yla **Block** sinfini) yaradacaÄŸÄ±q.

### 1. Transformer Blokunun KomponentlÉ™ri

Bir **Transformer Bloku** iki É™sas alt-blokdan ibarÉ™tdir:

1.  **Multi-Head Attention (MHA):** MÉ™tnin fÉ™rqli hissÉ™lÉ™ri arasÄ±ndakÄ± É™laqÉ™lÉ™ri Ã¶yrÉ™nir.
2.  **Feed-Forward Network (FFN):** HÉ™r bir tokeni fÉ™rdi ÅŸÉ™kildÉ™ emal edÉ™n, sadÉ™, lakin gÃ¼clÃ¼ bir neyron ÅŸÉ™bÉ™kÉ™sidir.

Bu iki alt-blokun hÉ™r biri **Qat NormallaÅŸdÄ±rmasÄ± (Layer Normalization)** vÉ™ **QalÄ±q ÆlaqÉ™ (Residual Connection)** ilÉ™ É™hatÉ™ olunur.

| Komponent | Funksiya |
| :--- | :--- |
| **LayerNorm** | HÉ™r bir qatÄ±n giriÅŸini normallaÅŸdÄ±rÄ±r. Bu, tÉ™limi daha stabil vÉ™ sÃ¼rÉ™tli edir. |
| **Residual Connection** | QatÄ±n giriÅŸini birbaÅŸa Ã§Ä±xÄ±ÅŸa É™lavÉ™ edir. Bu, modelin dÉ™rinlÉ™ÅŸdikcÉ™ Ã¶yrÉ™nmÉ™ qabiliyyÉ™tini itirmÉ™sinin qarÅŸÄ±sÄ±nÄ± alÄ±r. |
| **GELU** | **Gaussian Error Linear Unit** â€“ FFN-dÉ™ istifadÉ™ olunan aktivasiya funksiyasÄ±dÄ±r. ReLU-dan daha yaxÅŸÄ± nÉ™ticÉ™lÉ™r verir. |

### 2. PyTorch-da Transformer Blokunun QurulmasÄ±

AÅŸaÄŸÄ±dakÄ± kodu **`block.py`** adlÄ± bir faylda yazaq. Bu kod, dÃ¼nÉ™n yazdÄ±ÄŸÄ±mÄ±z `MultiHeadAttention` sinfini istifadÉ™ edÉ™cÉ™k.

```python
# block.py
import torch
import torch.nn as nn
from torch.nn import functional as F
from attention import MultiHeadAttention # DÃ¼nÉ™nki sinif
from config import GPTConfig

class Block(nn.Module):
    """ NanoGPT-dÉ™ bir Transformer Blokunu tÉ™msil edir """

    def __init__(self, config):
        super().__init__()
        self.config = config

        # 1. Qat NormallaÅŸdÄ±rmasÄ± (LayerNorm) - DiqqÉ™tdÉ™n É™vvÉ™l
        self.ln_1 = nn.LayerNorm(config.n_embd)
        # 2. Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention)
        self.attn = MultiHeadAttention(config)

        # 3. Qat NormallaÅŸdÄ±rmasÄ± (LayerNorm) - FFN-dÉ™n É™vvÉ™l
        self.ln_2 = nn.LayerNorm(config.n_embd)
        # 4. Ä°rÉ™li Ã–tÃ¼rmÉ™ ÅÉ™bÉ™kÉ™si (Feed-Forward Network)
        # Standart olaraq, FFN-in gizli qatÄ± giriÅŸ Ã¶lÃ§Ã¼sÃ¼nÃ¼n 4 qatÄ±dÄ±r (768 * 4 = 3072)
        self.mlp = nn.ModuleDict(dict(
            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),
            gelu    = nn.GELU(),
            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),
            dropout = nn.Dropout(config.dropout),
        ))
        self.mlp_forward = nn.Sequential(self.mlp.c_fc, self.mlp.gelu, self.mlp.c_proj, self.mlp.dropout)

    def forward(self, x):
        # 1. DiqqÉ™t Alt-Bloku
        # QalÄ±q ÆlaqÉ™ (Residual Connection) + LayerNorm + Attention
        # LayerNorm-u É™vvÉ™lcÉ™ tÉ™tbiq etmÉ™k (Pre-LN) daha stabil tÉ™limÉ™ sÉ™bÉ™b olur
        x = x + self.attn(self.ln_1(x))

        # 2. FFN Alt-Bloku
        # QalÄ±q ÆlaqÉ™ (Residual Connection) + LayerNorm + FFN
        x = x + self.mlp_forward(self.ln_2(x))

        return x
```

### 3. Kodun Ä°zahÄ± (HÉ™r SÉ™trin DetallÄ± Ä°zahÄ±)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 12 | `class Block(nn.Module):` | Transformer Blokumuzun sinfini tÉ™yin edirik. |
| 17 | `self.ln_1 = nn.LayerNorm(config.n_embd)` | Birinci Layer Norm qatÄ±nÄ± yaradÄ±rÄ±q. |
| 19 | `self.attn = MultiHeadAttention(config)` | DÃ¼nÉ™n yazdÄ±ÄŸÄ±mÄ±z Ã‡oxbaÅŸlÄ± DiqqÉ™t mexanizmini daxil edirik. |
| 23 | `self.ln_2 = nn.LayerNorm(config.n_embd)` | Ä°kinci Layer Norm qatÄ±nÄ± yaradÄ±rÄ±q. |
| 26-31 | `self.mlp = nn.ModuleDict(...)` | **Feed-Forward Network (FFN)**-i tÉ™yin edirik. O, 4 É™sas hissÉ™dÉ™n ibarÉ™tdir: giriÅŸ xÉ™tti qatÄ± (`c_fc`), aktivasiya funksiyasÄ± (`gelu`), Ã§Ä±xÄ±ÅŸ xÉ™tti qatÄ± (`c_proj`) vÉ™ `dropout`. |
| 32 | `self.mlp_forward = nn.Sequential(...)` | FFN-in komponentlÉ™rini ardÄ±cÄ±l icra olunacaq ÅŸÉ™kildÉ™ birlÉ™ÅŸdiririk. |
| 35 | `def forward(self, x):` | MÉ™lumatÄ±n blokdan keÃ§mÉ™ ardÄ±cÄ±llÄ±ÄŸÄ±nÄ± tÉ™yin edirik. |
| 39 | `x = x + self.attn(self.ln_1(x))` | **DiqqÉ™t Alt-Bloku:** GiriÅŸ (`x`) LayerNorm-dan keÃ§irilir, sonra DiqqÉ™t mexanizminÉ™ verilir vÉ™ nÉ™ticÉ™ yenidÉ™n giriÅŸÉ™ É™lavÉ™ edilir (`x + ...`). Bu, **QalÄ±q ÆlaqÉ™dir**. |
| 43 | `x = x + self.mlp_forward(self.ln_2(x))` | **FFN Alt-Bloku:** Eyni ÅŸÉ™kildÉ™, LayerNorm-dan keÃ§irilir, FFN-dÉ™n keÃ§irilir vÉ™ nÉ™ticÉ™ yenidÉ™n giriÅŸÉ™ É™lavÉ™ edilir. |

### 4. QalÄ±q ÆlaqÉ™ (Residual Connection)

QalÄ±q ÆlaqÉ™nin É™hÉ™miyyÉ™tini bir daha vurÄŸulayaq:

```python
output = input + Sublayer(LayerNorm(input))
```

Bu, modelin Ã¶yrÉ™nmÉ™ prosesini asanlaÅŸdÄ±rÄ±r. ÆgÉ™r model yeni qatda heÃ§ nÉ™ Ã¶yrÉ™nmÉ™sÉ™ belÉ™, **É™vvÉ™lki mÉ™lumatÄ± (input)** birbaÅŸa nÃ¶vbÉ™ti qata Ã¶tÃ¼rÉ™ bilir. Bu, modelin **dÉ™rinliyini** (bizim halÄ±mÄ±zda 12 qat) artÄ±rmaÄŸa imkan verir.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  **`attention.py`** vÉ™ **`config.py`** fayllarÄ±nÄ±n mÃ¶vcud olduÄŸundan É™min olun.
2.  **`block.py`** faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± kodu ora kopyalayÄ±n.
3.  KiÃ§ik bir sÄ±naq skripti yazÄ±n:
    ```python
    # SÄ±naq skripti
    from config import GPTConfig
    from block import Block
    
    config = GPTConfig()
    block = Block(config)
    
    # SÄ±naq giriÅŸi: 4 cÃ¼mlÉ™ (batch), hÉ™r biri 10 token uzunluÄŸunda, 768 Ã¶lÃ§Ã¼lÃ¼ vektor
    dummy_input = torch.randn(4, 10, config.n_embd)
    
    output = block(dummy_input)
    print(f"Ã‡Ä±xÄ±ÅŸ Tensorunun Ã–lÃ§Ã¼sÃ¼: {output.shape}")
    # NÉ™ticÉ™ (4, 10, 768) olmalÄ±dÄ±r.
    ```

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah bÃ¼tÃ¼n bu komponentlÉ™ri birlÉ™ÅŸdirÉ™rÉ™k **GPT (NanoGPT)** modelinin tam sinfini yaradacaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
