# GÃ¼n 16: Transformer Blokunun QurulmasÄ± ğŸ§±

## 16.1. Transformer Bloku NÉ™dir?

**Transformer Bloku** (vÉ™ ya GPT Bloku) modelin É™sas tÉ™krar olunan vahididir. Bizim 100M parametrli modelimizdÉ™ bu blokdan **12 É™dÉ™d** ardÄ±cÄ±l istifadÉ™ olunacaq.

Bir Transformer Bloku iki É™sas alt-blokdan ibarÉ™tdir:

1.  **Multi-Head Attention (MHA):** MÉ™tnin kontekstini Ã¶yrÉ™nir (GÃ¼n 15).
2.  **Feed-Forward Network (FFN):** MHA-dan gÉ™lÉ™n mÉ™lumatÄ± emal edir vÉ™ modelin Ã¶yrÉ™nmÉ™ qabiliyyÉ™tini artÄ±rÄ±r.

Bu iki alt-blok arasÄ±nda vÉ™ onlardan sonra **Layer Normalization (Lay NormallaÅŸdÄ±rmasÄ±)** vÉ™ **Residual Connection (QalÄ±q ÆlaqÉ™)** istifadÉ™ olunur.

## 16.2. Layer Normalization vÉ™ Residual Connection

*   **Residual Connection (QalÄ±q ÆlaqÉ™):** GiriÅŸ mÉ™lumatÄ±nÄ± (x) alt-blokun Ã§Ä±xÄ±ÅŸÄ±na É™lavÉ™ edir. YÉ™ni, `Ã§Ä±xÄ±ÅŸ = x + AltBlok(x)`. Bu, qradiyentlÉ™rin dÉ™rin ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™ belÉ™ asanlÄ±qla axmasÄ±na vÉ™ modelin daha sÃ¼rÉ™tli Ã¶yrÉ™nmÉ™sinÉ™ kÃ¶mÉ™k edir.
*   **Layer Normalization (Lay NormallaÅŸdÄ±rmasÄ±):** HÉ™r bir alt-blokun Ã§Ä±xÄ±ÅŸÄ±nÄ± normallaÅŸdÄ±rÄ±r. Bu, tÉ™lim prosesini sabitlÉ™ÅŸdirir vÉ™ sÃ¼rÉ™tlÉ™ndirir.

## 16.3. Praktika: Transformer Blokunun QurulmasÄ±

Ä°ndi isÉ™ `MultiHeadAttention` sinfini vÉ™ `FeedForward` sinfini birlÉ™ÅŸdirÉ™rÉ™k `Block` sinfini quraq.

**`block.py`**

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
# MultiHeadAttention sinfini (GÃ¼n 15-dÉ™n) bura kopyalayÄ±n vÉ™ ya import edin

# Modelin É™sas hiperparametrlÉ™ri (GÃ¼n 13-dÉ™n)
n_embd = 768  # Embedding Ã¶lÃ§Ã¼sÃ¼
n_head = 12   # BaÅŸlarÄ±n sayÄ±
block_size = 256 # Kontekst uzunluÄŸu

# ... (Head sinfinin kodu) ...
# ... (MultiHeadAttention sinfinin kodu) ...

class FeedForward(nn.Module):
    """SadÉ™ Ä°rÉ™li-Ã–tÃ¼rmÉ™ ÅÉ™bÉ™kÉ™si (MLP)"""
    
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            # 1. GeniÅŸlÉ™ndirmÉ™: Ã–lÃ§Ã¼nÃ¼ 4 dÉ™fÉ™ artÄ±rÄ±rÄ±q (768 * 4 = 3072)
            nn.Linear(n_embd, 4 * n_embd),
            nn.GELU(), # Aktivasiya funksiyasÄ± (ReLU-dan daha yaxÅŸÄ±dÄ±r)
            # 2. Daraltma: Ã–lÃ§Ã¼nÃ¼ yenidÉ™n 768-É™ qaytarÄ±rÄ±q
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(0.1), # RTX 2050 Ã¼Ã§Ã¼n Overfitting-in qarÅŸÄ±sÄ±nÄ± almaq
        )

    def forward(self, x):
        return self.net(x)


class Block(nn.Module):
    """Transformer Blokunun TÉ™krar Olunan Vahidi"""
    
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        
        # 1. Multi-Head Attention (MHA)
        self.sa = MultiHeadAttention(n_head, head_size)
        
        # 2. Feed-Forward Network (FFN)
        self.ffwd = FeedForward(n_embd)
        
        # 3. Layer Normalization (NormallaÅŸdÄ±rma)
        # HÉ™r bir alt-blokdan É™vvÉ™l tÉ™tbiq olunur (Pre-Layer Norm)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        # 1. Birinci Alt-Blok: MHA + Residual Connection + Layer Norm
        # Layer Norm-dan keÃ§irib MHA-ya Ã¶tÃ¼rÃ¼rÃ¼k, sonra giriÅŸi (x) É™lavÉ™ edirik.
        x = x + self.sa(self.ln1(x))
        
        # 2. Ä°kinci Alt-Blok: FFN + Residual Connection + Layer Norm
        # Layer Norm-dan keÃ§irib FFN-É™ Ã¶tÃ¼rÃ¼rÃ¼k, sonra giriÅŸi (x) É™lavÉ™ edirik.
        x = x + self.ffwd(self.ln2(x))
        
        return x

# NÃ¼munÉ™: TÉ™k bir Transformer Bloku yaratmaq
block = Block(n_embd=n_embd, n_head=n_head)
print(block)
```

## 16.4. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **32** | `nn.Linear(n_embd, 4 * n_embd)` | FFN-in ilk xÉ™tti layÄ±. GiriÅŸ Ã¶lÃ§Ã¼sÃ¼nÃ¼ 4 dÉ™fÉ™ artÄ±rÄ±r. Bu geniÅŸlÉ™ndirmÉ™ modelÉ™ daha mÃ¼rÉ™kkÉ™b É™laqÉ™lÉ™ri Ã¶yrÉ™nmÉ™yÉ™ imkan verir. |
| **33** | `nn.GELU()` | **Gaussian Error Linear Unit** (GELU) aktivasiya funksiyasÄ±. ReLU-dan daha hamar vÉ™ LLM-lÉ™rdÉ™ daha Ã§ox istifadÉ™ olunur. |
| **35** | `nn.Linear(4 * n_embd, n_embd)` | FFN-in ikinci xÉ™tti layÄ±. Ã–lÃ§Ã¼nÃ¼ yenidÉ™n modelin É™sas Ã¶lÃ§Ã¼sÃ¼nÉ™ qaytarÄ±r. |
| **57** | `self.ln1 = nn.LayerNorm(n_embd)` | Birinci Layer Norm layÄ±. |
| **61** | `x = x + self.sa(self.ln1(x))` | **Residual Connection** (`x + ...`) vÉ™ **Pre-Layer Normalization** (`self.ln1(x)`) tÉ™tbiq olunur. Bu, Transformer arxitekturasÄ±nÄ±n standart tÉ™tbiqidir. |
| **64** | `x = x + self.ffwd(self.ln2(x))` | Ä°kinci alt-blokun (FFN) tÉ™tbiqi. |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `block.py` skriptini yaradÄ±n. `Block` sinfinin `forward` funksiyasÄ±ndakÄ± **Residual Connection** vÉ™ **Layer Normalization** ardÄ±cÄ±llÄ±ÄŸÄ±nÄ± dÉ™rindÉ™n analiz edin. Bu, GPT modelinin É™sasÄ±nÄ± tÉ™ÅŸkil edir.
