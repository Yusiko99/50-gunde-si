# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 31

## PyTorch-dan Hugging Face-É™ Ã‡evirmÉ™ (I HissÉ™) ğŸ”„

Salam! DÃ¼nÉ™n modelimizi Ollama Ã¼Ã§Ã¼n hazÄ±rlamaq mÉ™qsÉ™dilÉ™ **GGUF** formatÄ±na keÃ§mÉ™yÉ™ qÉ™rar verdik. Bu keÃ§idin ilk addÄ±mÄ± isÉ™ bizim tÉ™miz PyTorch modelimizi (NanoGPT) **Hugging Face (HF)** formatÄ±na Ã§evirmÉ™kdir.

### 1. NiyÉ™ Hugging Face?

**Hugging Face** ekosistemi LLM-lÉ™r Ã¼Ã§Ã¼n sÉ™naye standartÄ±dÄ±r. GGUF kimi alÉ™tlÉ™r birbaÅŸa PyTorch Ã§É™kilÉ™rini deyil, Hugging Face formatÄ±nda saxlanmÄ±ÅŸ modellÉ™ri qÉ™bul edir.

Bizim NanoGPT modelimiz GPT-2 arxitekturasÄ±na É™saslanÄ±r. Buna gÃ¶rÉ™ dÉ™, Ã§É™kilÉ™rimizi HF-in standart **`gpt2`** modelinin Ã§É™kilÉ™rinÉ™ uyÄŸunlaÅŸdÄ±rmalÄ±yÄ±q.

### 2. Ã‡É™kilÉ™rin KÃ¶Ã§Ã¼rÃ¼lmÉ™si (State Dict Mapping)

Bizim `best_model.pt` faylÄ±ndakÄ± Ã§É™kilÉ™rin adlarÄ± ilÉ™ HF-in `gpt2` modelindÉ™ki Ã§É™kilÉ™rin adlarÄ± fÉ™rqlidir. Biz bu adlarÄ± bir-birinÉ™ uyÄŸunlaÅŸdÄ±ran bir funksiya yazmalÄ±yÄ±q.

AÅŸaÄŸÄ±dakÄ± kodu **`export_hf.py`** faylÄ±nda yazaq.

```python
# export_hf.py
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoConfig
from config import GPTConfig
from model import GPT
from tokenizers import Tokenizer
import os

# 1. Ã‡É™kilÉ™rin KÃ¶Ã§Ã¼rÃ¼lmÉ™si FunksiyasÄ±
def convert_nano_to_hf(nano_model, hf_model):
    """ NanoGPT modelinin Ã§É™kilÉ™rini Hugging Face GPT-2 modelinÉ™ kÃ¶Ã§Ã¼rÃ¼r """
    
    # NanoGPT-nin Ã§É™kilÉ™rini alÄ±rÄ±q
    nano_state_dict = nano_model.state_dict()
    # Hugging Face modelinin Ã§É™kilÉ™rini alÄ±rÄ±q
    hf_state_dict = hf_model.state_dict()
    
    # Ã‡É™kilÉ™rin kÃ¶Ã§Ã¼rÃ¼lmÉ™si Ã¼Ã§Ã¼n xÉ™ritÉ™ (mapping)
    mapping = {
        # GÃ¶mÃ¼lmÉ™ QatlarÄ±
        'transformer.wte.weight': 'transformer.wte.weight',
        'transformer.wpe.weight': 'transformer.wpe.weight',
        # Son NormallaÅŸdÄ±rma
        'transformer.ln_f.weight': 'transformer.ln_f.weight',
        'transformer.ln_f.bias': 'transformer.ln_f.bias',
        # Dil Modeli BaÅŸÄ± (LM Head)
        'lm_head.weight': 'lm_head.weight',
    }
    
    # Transformer BloklarÄ±nÄ±n (12 É™dÉ™d) Ã§É™kilÉ™rini kÃ¶Ã§Ã¼rÃ¼rÃ¼k
    for i in range(nano_model.config.n_layer):
        # Layer Norms
        mapping[f'transformer.h.{i}.ln_1.weight'] = f'transformer.h.{i}.ln_1.weight'
        mapping[f'transformer.h.{i}.ln_1.bias'] = f'transformer.h.{i}.ln_1.bias'
        mapping[f'transformer.h.{i}.ln_2.weight'] = f'transformer.h.{i}.ln_2.weight'
        mapping[f'transformer.h.{i}.ln_2.bias'] = f'transformer.h.{i}.ln_2.bias'
        
        # Multi-Head Attention (MHA)
        # NanoGPT-dÉ™ c_attn var, HF-dÉ™ isÉ™ ayrÄ±-ayrÄ± c_attn, c_proj
        # Bizim c_attn-imiz Q, K, V-ni birlÉ™ÅŸdirir.
        # Bu hissÉ™ni xÃ¼susi olaraq tÉ™nzimlÉ™mÉ™liyik (NÃ¶vbÉ™ti gÃ¼n)
        
        # FFN (MLP)
        mapping[f'transformer.h.{i}.mlp.c_fc.weight'] = f'transformer.h.{i}.mlp.c_fc.weight'
        mapping[f'transformer.h.{i}.mlp.c_fc.bias'] = f'transformer.h.{i}.mlp.c_fc.bias'
        mapping[f'transformer.h.{i}.mlp.c_proj.weight'] = f'transformer.h.{i}.mlp.c_proj.weight'
        mapping[f'transformer.h.{i}.mlp.c_proj.bias'] = f'transformer.h.{i}.mlp.c_proj.bias'
        
    # KÃ¶Ã§Ã¼rÃ¼lmÉ™miÅŸ Ã§É™kilÉ™ri (MHA) nÃ¶vbÉ™ti gÃ¼n É™lavÉ™ edÉ™cÉ™yik.
    
    # Ã‡É™kilÉ™ri kÃ¶Ã§Ã¼rÃ¼rÃ¼k
    for nano_key, hf_key in mapping.items():
        if nano_key in nano_state_dict and hf_key in hf_state_dict:
            hf_state_dict[hf_key].copy_(nano_state_dict[nano_key])
            
    # Modelin Ã§É™kilÉ™rini yenilÉ™yirik
    hf_model.load_state_dict(hf_state_dict)
    return hf_model

# 2. Æsas Ä°cra Bloku
if __name__ == '__main__':
    # 1. NanoGPT-ni yÃ¼klÉ™
    config = GPTConfig()
    nano_model = GPT(config)
    nano_model.load_state_dict(torch.load('best_model.pt'))
    nano_model.eval()
    
    # 2. Hugging Face KonfiqurasiyasÄ±nÄ± yarat
    hf_config = AutoConfig.from_pretrained("gpt2")
    # Bizim konfiqurasiyamÄ±zÄ± tÉ™tbiq et
    hf_config.vocab_size = config.vocab_size
    hf_config.n_layer = config.n_layer
    hf_config.n_head = config.n_head
    hf_config.n_embd = config.n_embd
    hf_config.max_position_embeddings = config.block_size
    
    # 3. Hugging Face Modelini yarat
    hf_model = AutoModelForCausalLM.from_config(hf_config)
    
    # 4. Ã‡É™kilÉ™ri kÃ¶Ã§Ã¼r
    hf_model = convert_nano_to_hf(nano_model, hf_model)
    
    # 5. Tokenizatoru saxla
    tokenizer = Tokenizer.from_file("az_bpe_tokenizer.json")
    # Tokenizatoru HF formatÄ±nda saxlamaq Ã¼Ã§Ã¼n xÃ¼susi bir addÄ±m lazÄ±mdÄ±r (NÃ¶vbÉ™ti gÃ¼n)
    
    # 6. Modeli saxla
    # hf_model.save_pretrained("az_llm_hf")
    
    print("PyTorch-dan Hugging Face-É™ Ã§evirmÉ™nin ilk hissÉ™si tamamlandÄ±.")
```

### 3. Kodun Ä°zahÄ± (Æsas MÉ™qamlar)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 14 | `def convert_nano_to_hf(nano_model, hf_model):` | Ã‡É™kilÉ™ri kÃ¶Ã§Ã¼rÉ™n É™sas funksiyadÄ±r. |
| 23 | `mapping = { ... }` | Ã‡É™kilÉ™rin adlarÄ±nÄ±n uyÄŸunlaÅŸdÄ±rÄ±lmasÄ± lÃ¼ÄŸÉ™tidir. |
| 38 | `mapping[f'transformer.h.{i}.ln_1.weight'] = ...` | `i` dÉ™yiÅŸÉ™ni ilÉ™ 12 Transformer Blokunun hÉ™r birinin Ã§É™kilÉ™rini tÉ™k-tÉ™k uyÄŸunlaÅŸdÄ±rÄ±rÄ±q. |
| 52 | `hf_state_dict[hf_key].copy_(nano_state_dict[nano_key])` | NanoGPT-dÉ™n alÄ±nan Ã§É™ki dÉ™yÉ™rini HF modelinin Ã§É™ki dÉ™yÉ™rinÉ™ kopyalayÄ±r. |
| 65 | `hf_config = AutoConfig.from_pretrained("gpt2")` | HF-in standart GPT-2 konfiqurasiyasÄ±nÄ± yÃ¼klÉ™yirik. |
| 67-71 | `hf_config.vocab_size = config.vocab_size` | Bizim NanoGPT konfiqurasiyamÄ±zÄ± HF konfiqurasiyasÄ±na tÉ™tbiq edirik. |
| 74 | `hf_model = AutoModelForCausalLM.from_config(hf_config)` | Yeni konfiqurasiya ilÉ™ boÅŸ HF modelini yaradÄ±rÄ±q. |

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `export_hf.py` faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± kodu ora kopyalayÄ±n.
2.  `best_model.pt` faylÄ±nÄ±n mÃ¶vcud olduÄŸundan É™min olun.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **Ã‡oxbaÅŸlÄ± DiqqÉ™t (MHA)** Ã§É™kilÉ™rinin kÃ¶Ã§Ã¼rÃ¼lmÉ™si vÉ™ **Tokenizatorun HF formatÄ±nda saxlanmasÄ±** mÃ¶vzusunu tamamlayacaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
