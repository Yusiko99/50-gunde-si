# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 33

## GGUF FormatÄ±na Ã‡evirmÉ™: Ollama Ã¼Ã§Ã¼n HazÄ±rlÄ±q ğŸ“¦

Salam! DÃ¼nÉ™n modelimizi uÄŸurla Hugging Face (HF) formatÄ±na Ã§evirdik. Bu gÃ¼n isÉ™ Ollama-da istifadÉ™ edÉ™ bilÉ™cÉ™yimiz yÃ¼ngÃ¼l model formatÄ± olan **GGUF**-a keÃ§irik.

### 1. GGUF NÉ™dir?

**GGUF (GPT-GEneration Unified Format)** â€” É™sasÉ™n **`llama.cpp`** layihÉ™si tÉ™rÉ™findÉ™n inkiÅŸaf etdirilmiÅŸ, LLM-lÉ™ri CPU-da vÉ™ ya yÃ¼ngÃ¼l GPU-larda (mÉ™sÉ™lÉ™n, bizim T4) sÃ¼rÉ™tli vÉ™ yaddaÅŸa qÉ™naÉ™t edÉ™n ÅŸÉ™kildÉ™ iÅŸlÉ™tmÉ™k Ã¼Ã§Ã¼n nÉ™zÉ™rdÉ™ tutulmuÅŸ bir fayl formatÄ±dÄ±r.

GGUF-un É™sas Ã¼stÃ¼nlÃ¼yÃ¼, modelin Ã§É™kilÉ™rini **Quantization** (KvantlaÅŸdÄ±rma) edÉ™rÉ™k Ã¶lÃ§Ã¼nÃ¼ kÉ™skin ÅŸÉ™kildÉ™ azaltmasÄ±dÄ±r.

### 2. Ã‡evirmÉ™ Prosesi

GGUF-a Ã§evirmÉ™ prosesi iki É™sas addÄ±mdan ibarÉ™tdir:

1.  **Modelin HazÄ±rlanmasÄ±:** HF modelini `llama.cpp` tÉ™rÉ™findÉ™n istifadÉ™ oluna bilÉ™cÉ™k tÉ™mÉ™l formata Ã§evirmÉ™k.
2.  **GGUF-a KvantlaÅŸdÄ±rma:** HazÄ±rlanmÄ±ÅŸ modeli istÉ™diyimiz kvantlaÅŸdÄ±rma sÉ™viyyÉ™sindÉ™ GGUF faylÄ±na Ã§evirmÉ™k.

Biz bu proses Ã¼Ã§Ã¼n **`llama.cpp`** layihÉ™sinin alÉ™tlÉ™rindÉ™n istifadÉ™ edÉ™cÉ™yik.

### 3. `llama.cpp` AlÉ™tlÉ™rinin QuraÅŸdÄ±rÄ±lmasÄ±

`llama.cpp` C++ ilÉ™ yazÄ±lmÄ±ÅŸdÄ±r, lakin bizÉ™ lazÄ±m olan alÉ™tlÉ™r Python vasitÉ™silÉ™ istifadÉ™ oluna bilÉ™r.

#### A. `llama-cpp-python` QuraÅŸdÄ±rÄ±lmasÄ±

```bash
# Windows-da Anaconda Prompt-da icra edin
# Qeyd: Bu quraÅŸdÄ±rma bir az vaxt ala bilÉ™r.
pip install llama-cpp-python
```

#### B. `llama.cpp` RepozitoriyasÄ±nÄ±n KlonlanmasÄ±

BizÉ™ Ã§evirmÉ™ skriptlÉ™ri Ã¼Ã§Ã¼n `llama.cpp` repozitoriyasÄ± lazÄ±mdÄ±r.

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

### 4. GGUF-a Ã‡evirmÉ™ Skripti

Bizim modelimiz GPT-2 arxitekturasÄ±na É™saslandÄ±ÄŸÄ± Ã¼Ã§Ã¼n, `llama.cpp` repozitoriyasÄ±ndakÄ± **`convert-hf-to-gguf.py`** skriptindÉ™n istifadÉ™ edÉ™cÉ™yik.

AÅŸaÄŸÄ±dakÄ± É™mrlÉ™ri `llama.cpp` qovluÄŸunun iÃ§indÉ™ icra edirik.

#### A. TÉ™mÉ™l GGUF FaylÄ±nÄ±n YaradÄ±lmasÄ± (FP32)

ÆvvÉ™lcÉ™ modelin tam dÉ™qiqlikdÉ™ (FP32) GGUF faylÄ±nÄ± yaradÄ±rÄ±q.

```bash
# llama.cpp qovluÄŸunun iÃ§indÉ™
python convert-hf-to-gguf.py \
    ../az_llm_hf \
    --outfile ../az_llm_fp32.gguf \
    --model-name az-nano-llm \
    --vocab-only
```

**Kodun Ä°zahÄ±:**
*   `../az_llm_hf`: Hugging Face formatÄ±nda saxladÄ±ÄŸÄ±mÄ±z modelin qovluÄŸudur.
*   `--outfile ../az_llm_fp32.gguf`: Yaranacaq GGUF faylÄ±nÄ±n adÄ±dÄ±r.
*   `--model-name az-nano-llm`: ModelÉ™ verdiyimiz addÄ±r.
*   `--vocab-only`: Bu, yalnÄ±z tokenizatoru GGUF formatÄ±na Ã§evirir.

#### B. KvantlaÅŸdÄ±rma (Quantization)

Ä°ndi isÉ™ bu tÉ™mÉ™l GGUF faylÄ±nÄ± kvantlaÅŸdÄ±rÄ±rÄ±q. Biz **Q4_K_M** kvantlaÅŸdÄ±rma sÉ™viyyÉ™sini seÃ§irik. Bu, 4-bit kvantlaÅŸdÄ±rmadÄ±r vÉ™ Ã¶lÃ§Ã¼nÃ¼ tÉ™xminÉ™n **8 dÉ™fÉ™** azaldÄ±r.

```bash
# llama.cpp qovluÄŸunun iÃ§indÉ™
./quantize ../az_llm_fp32.gguf ../az_llm_q4km.gguf Q4_K_M
```

**Kodun Ä°zahÄ±:**
*   `./quantize`: `llama.cpp` tÉ™rÉ™findÉ™n tÉ™min edilÉ™n kvantlaÅŸdÄ±rma alÉ™tidir.
*   `../az_llm_fp32.gguf`: GiriÅŸ faylÄ± (tÉ™mÉ™l GGUF).
*   `../az_llm_q4km.gguf`: Ã‡Ä±xÄ±ÅŸ faylÄ± (kvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ GGUF).
*   `Q4_K_M`: KvantlaÅŸdÄ±rma nÃ¶vÃ¼.

**NÉ™ticÉ™:** Bu prosesin sonunda, tÉ™xminÉ™n **62 MB** Ã¶lÃ§Ã¼sÃ¼ndÉ™ **`az_llm_q4km.gguf`** adlÄ± bir fayl É™ldÉ™ edÉ™cÉ™yik. Bu, bizim Ollama-da istifadÉ™ edÉ™cÉ™yimiz son model faylÄ±dÄ±r.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `llama-cpp-python` kitabxanasÄ±nÄ± quraÅŸdÄ±rÄ±n.
2.  `llama.cpp` repozitoriyasÄ±nÄ± klonlayÄ±n.
3.  YuxarÄ±dakÄ± iki É™mri icra edin.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **Ollama-ya GiriÅŸ** vÉ™ **Modelin Ollama-da YÃ¼klÉ™nmÉ™si** mÃ¶vzusunu Ã¶yrÉ™nÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
