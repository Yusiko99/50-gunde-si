# GÃ¼n 33: GGUF FormatÄ±na Ã‡evirmÉ™ (KvantlaÅŸdÄ±rma) ğŸ“¦

## 33.1. KvantlaÅŸdÄ±rma Prosesinin MÉ™ntiqi

GÃ¼n 32-dÉ™ modelimizi Hugging Face (HF) formatÄ±na Ã§evirdik. Ä°ndi bu HF modelini **GGUF (GPT-GEneration.cpp Unified Format)** formatÄ±na Ã§evirmÉ™liyik.

**MÉ™ntiq:** GGUF formatÄ±, modelin Ã§É™kilÉ™rini **Int4** kimi aÅŸaÄŸÄ± dÉ™qiqliyÉ™ Ã§evirÉ™rÉ™k modelin Ã¶lÃ§Ã¼sÃ¼nÃ¼ 8 dÉ™fÉ™ azaldÄ±r. Bu, modelin mÉ™hdud VRAM-lÄ± GPU-larda (mÉ™sÉ™lÉ™n, 4GB RTX 2050) vÉ™ ya CPU-da sÃ¼rÉ™tli iÅŸlÉ™mÉ™sini tÉ™min edir.

KvantlaÅŸdÄ±rma prosesi adÉ™tÉ™n **Llama.cpp** layihÉ™sinin alÉ™tlÉ™ri ilÉ™ hÉ™yata keÃ§irilir. Bu proses iki É™sas addÄ±mdan ibarÉ™tdir:

1.  **Xam Ã‡evrilmÉ™:** HF modelini xam FP32 GGUF formatÄ±na Ã§evirmÉ™k.
2.  **KvantlaÅŸdÄ±rma:** Xam GGUF-u Int4 (Q4_0) formatÄ±na Ã§evirmÉ™k.

## 33.2. Praktika: KvantlaÅŸdÄ±rma Skripti (Simulyasiya)

KvantlaÅŸdÄ±rma prosesi bir neÃ§É™ terminal É™mri tÉ™lÉ™b etdiyi Ã¼Ã§Ã¼n, biz bu prosesi simulyasiya edÉ™n vÉ™ É™sas mÉ™ntiqi izah edÉ™n bir skript tÉ™qdim edirik.

**`quantize_to_gguf.py`**

```python
import os
import subprocess

HF_MODEL_PATH = "az_llm_hf"
OUTPUT_DIR = "az_llm_gguf"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 1. Xam GGUF FaylÄ±nÄ±n AdÄ± (FP32)
RAW_GGUF_FILE = os.path.join(OUTPUT_DIR, "az_llm_f32.gguf")

# 2. KvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ GGUF FaylÄ±nÄ±n AdÄ± (Q4_0)
FINAL_GGUF_FILE = os.path.join(OUTPUT_DIR, "az_llm_100m_q4_0.gguf")
QUANTIZATION_TYPE = "Q4_0" # 4-bit kvantlaÅŸdÄ±rma

def simulate_quantization():
    """GGUF Ã§evrilmÉ™ vÉ™ kvantlaÅŸdÄ±rma prosesini simulyasiya edir."""
    
    print(f"1. Hugging Face modelinin xam GGUF-a Ã§evrilmÉ™si...")
    # Realda: llama.cpp/convert.py az_llm_hf --outtype f32 --outfile az_llm_gguf/az_llm_f32.gguf
    
    # Simulyasiya: Xam GGUF faylÄ±nÄ± yaratmaq
    with open(RAW_GGUF_FILE, 'w') as f:
        f.write("Bu fayl modelin FP32 Ã§É™kilÉ™rini ehtiva edir.")
        
    print(f"Xam GGUF faylÄ± yaradÄ±ldÄ±: {RAW_GGUF_FILE}")
    
    print(f"\n2. KvantlaÅŸdÄ±rma ({QUANTIZATION_TYPE}) prosesi...")
    # Realda: llama.cpp/quantize az_llm_gguf/az_llm_f32.gguf az_llm_gguf/az_llm_q4_0.gguf Q4_0
    
    # Simulyasiya: KvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ GGUF faylÄ±nÄ± yaratmaq
    with open(FINAL_GGUF_FILE, 'w') as f:
        f.write(f"Bu fayl {QUANTIZATION_TYPE} kvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ GGUF modelini ehtiva edir.")
        
    print(f"KvantlaÅŸdÄ±rma tamamlandÄ±. Yekun GGUF faylÄ±: {FINAL_GGUF_FILE}")
    print(f"Modelin Ã¶lÃ§Ã¼sÃ¼ tÉ™xminÉ™n 50-70MB olacaq.")

if __name__ == "__main__":
    if not os.path.exists(HF_MODEL_PATH):
        print("XÉ™ta: Hugging Face model qovluÄŸu tapÄ±lmadÄ±. ZÉ™hmÉ™t olmasa GÃ¼n 32-ni tamamlayÄ±n.")
    else:
        simulate_quantization()
```

## 33.3. KvantlaÅŸdÄ±rmanÄ±n MÉ™ntiqi Ä°zahÄ±

| AddÄ±m | MÉ™ntiqi Æsas |
| :--- | :--- |
| **Xam Ã‡evrilmÉ™** | HF modelinin Ã§É™kilÉ™ri (mÉ™sÉ™lÉ™n, `pytorch_model.bin`) Llama.cpp-nin daxili formatÄ±na (GGUF) kÃ¶Ã§Ã¼rÃ¼lÃ¼r. Bu mÉ™rhÉ™lÉ™dÉ™ dÉ™qiqlik (FP32) saxlanÄ±lÄ±r. |
| **KvantlaÅŸdÄ±rma** | **Kritik AddÄ±m.** Bu alÉ™t, FP32 Ã§É™kilÉ™rini oxuyur vÉ™ onlarÄ± **4-bitlik (Int4)** tam É™dÉ™dlÉ™rÉ™ Ã§evirir. Bu Ã§evrilmÉ™ zamanÄ± modelin dÉ™qiqliyindÉ™ minimal itki ilÉ™ yaddaÅŸ tÉ™lÉ™bi kÉ™skin ÅŸÉ™kildÉ™ azalÄ±r. |
| **Q4_0** | **Q**uantization **4**-bit **0**-cu versiya demÉ™kdir. Bu, É™n Ã§ox istifadÉ™ olunan vÉ™ É™n yÃ¼ngÃ¼l kvantlaÅŸdÄ±rma nÃ¶vÃ¼dÃ¼r. |

**NÉ™ticÉ™:** Bu prosesin sonunda É™ldÉ™ edilÉ™n **`az_llm_100m_q4_0.gguf`** faylÄ±, modelin bÃ¼tÃ¼n biliklÉ™rini 50-70MB hÉ™cmindÉ™ ehtiva edir vÉ™ Ollama-da istifadÉ™ Ã¼Ã§Ã¼n hazÄ±rdÄ±r.
