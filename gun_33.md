# GÃ¼n 33: GGUF FormatÄ±na Ã‡evirmÉ™ (KvantlaÅŸdÄ±rma) ğŸ“¦

## 33.1. GGUF-a Ã‡evirmÉ™ Prosesi

Bizim mÉ™qsÉ™dimiz modelimizi **Ollama**-da istifadÉ™ etmÉ™kdir. Ollama isÉ™ **GGUF** formatÄ±nÄ± tÉ™lÉ™b edir. GGUF-a Ã§evirmÉ™ prosesi iki É™sas mÉ™rhÉ™lÉ™dÉ™n ibarÉ™tdir:

1.  **Hugging Face Modelini Llama.cpp FormatÄ±na Ã‡evirmÉ™k:** HF modelini Llama.cpp-nin baÅŸa dÃ¼ÅŸdÃ¼yÃ¼ xam formatda (mÉ™sÉ™lÉ™n, FP32) saxlamaq.
2.  **Llama.cpp ilÉ™ KvantlaÅŸdÄ±rmaq:** Bu xam formatÄ± Int4 kimi daha kiÃ§ik dÉ™qiqliyÉ™ Ã§evirmÉ™k.

Biz bu prosesi Hugging Face-in **`llama-cpp-python`** kitabxanasÄ± vasitÉ™silÉ™ hÉ™yata keÃ§irÉ™cÉ™yik.

## 33.2. Praktika: GGUF KvantlaÅŸdÄ±rmasÄ±

**`quantize_to_gguf.py`**

```python
import os
import subprocess
from transformers import AutoModelForCausalLM, AutoTokenizer

# GiriÅŸ vÉ™ Ã‡Ä±xÄ±ÅŸ QovluqlarÄ±
HF_MODEL_PATH = "az_llm_hf"
OUTPUT_DIR = "az_llm_gguf"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 1. Hugging Face Modelini YÃ¼klÉ™mÉ™k
print("1. Hugging Face modelini yÃ¼klÉ™mÉ™k...")
# AutoModelForCausalLM istifadÉ™ edÉ™rÉ™k modelimizi yÃ¼klÉ™yirik
model = AutoModelForCausalLM.from_pretrained(HF_MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH)

# 2. Modelin Ã‡É™kilÉ™rini FP32 FormatÄ±nda Saxlamaq (Llama.cpp Ã¼Ã§Ã¼n)
# Bu, llama.cpp-nin Ã§evirmÉ™ skripti Ã¼Ã§Ã¼n ilkin addÄ±mdÄ±r.
# SadÉ™lik Ã¼Ã§Ã¼n, biz bunu É™l ilÉ™ deyil, mÃ¶vcud alÉ™tlÉ™rlÉ™ edÉ™cÉ™yik.

# 3. Llama.cpp-nin Ã‡evirmÉ™ Skriptini Ä°cra EtmÉ™k
# Biz bu addÄ±mÄ± simulyasiya edirik, Ã§Ã¼nki real llama.cpp skriptlÉ™ri burada yoxdur.
# Lakin, llama-cpp-python kitabxanasÄ± bu funksionallÄ±ÄŸÄ± tÉ™min edir.

# Tutaq ki, bizdÉ™ llama.cpp-nin `convert.py` skripti var.
# Bu skript HF modelini xam FP32 GGUF-a Ã§evirir.
# Æmr: python convert.py az_llm_hf --outtype f32 --outfile az_llm_gguf/az_llm_f32.gguf

# 4. KvantlaÅŸdÄ±rma (Int4)
# KvantlaÅŸdÄ±rma Ã¼Ã§Ã¼n `quantize` alÉ™tini istifadÉ™ edirik.
# Bizim nÃ¼munÉ™mizdÉ™, bu prosesi É™vÉ™z edÉ™n sadÉ™ bir funksiya yaradÄ±rÄ±q.

# ÆslindÉ™ bu proses terminalda icra olunur:
# ./quantize az_llm_gguf/az_llm_f32.gguf az_llm_gguf/az_llm_q4_0.gguf q4_0

# NÃ¼munÉ™: KvantlaÅŸdÄ±rma É™mrini simulyasiya etmÉ™k
# Bizim modelimiz 134M parametrdir.
# Q4_0 (4-bit kvantlaÅŸdÄ±rma) É™n Ã§ox istifadÉ™ olunan yÃ¼ngÃ¼l formadÄ±r.
FINAL_GGUF_FILE = os.path.join(OUTPUT_DIR, "az_llm_100m_q4_0.gguf")

print(f"3. Modelin GGUF formatÄ±na Ã§evrilmÉ™si vÉ™ kvantlaÅŸdÄ±rÄ±lmasÄ± (Q4_0)...")

# ÆgÉ™r llama-cpp-python quraÅŸdÄ±rÄ±lÄ±bsa, bu prosesi avtomatlaÅŸdÄ±ran skriptlÉ™r mÃ¶vcuddur.
# Bizim vÉ™ziyyÉ™timizdÉ™, bu prosesin uÄŸurla baÅŸa Ã§atdÄ±ÄŸÄ±nÄ± fÉ™rz edirik.

# NÉ™ticÉ™ faylÄ±nÄ±n yaradÄ±lmasÄ± (simulyasiya)
with open(FINAL_GGUF_FILE, 'w') as f:
    f.write("Bu fayl 4-bit kvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ GGUF modelini ehtiva edir.")

print(f"KvantlaÅŸdÄ±rma tamamlandÄ±. Yekun GGUF faylÄ±: '{FINAL_GGUF_FILE}'")
print(f"Modelin Ã¶lÃ§Ã¼sÃ¼ tÉ™xminÉ™n 134MB olacaq (134M parametr * 4 bit / 8 bit/bayt).")

if __name__ == "__main__":
    convert_weights()
```

## 33.3. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **11** | `HF_MODEL_PATH = "az_llm_hf"` | GÃ¼n 32-dÉ™ hazÄ±rladÄ±ÄŸÄ±mÄ±z Hugging Face modelinin qovluÄŸu. |
| **16** | `model = AutoModelForCausalLM.from_pretrained(HF_MODEL_PATH)` | HF modelini yÃ¼klÉ™yir. |
| **28** | `FINAL_GGUF_FILE = os.path.join(OUTPUT_DIR, "az_llm_100m_q4_0.gguf")` | KvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ modelin adÄ±. **`q4_0`** 4-bit kvantlaÅŸdÄ±rma demÉ™kdir. |
| **35** | `with open(FINAL_GGUF_FILE, 'w') as f: ...` | Bu hissÉ™ real kvantlaÅŸdÄ±rma prosesini simulyasiya edir. Realda bu, Llama.cpp-nin alÉ™tlÉ™ri ilÉ™ icra olunan mÃ¼rÉ™kkÉ™b bir É™mÉ™liyyatdÄ±r. |
| **36** | `Modelin Ã¶lÃ§Ã¼sÃ¼ tÉ™xminÉ™n 134MB olacaq...` | **Kritik:** 134 Milyon parametr $\times$ 4 bit/parametr $\div$ 8 bit/bayt $\approx$ 67 MB. (Qeyd: GGUF-da É™lavÉ™ mÉ™lumatlar da saxlanÄ±ldÄ±ÄŸÄ± Ã¼Ã§Ã¼n Ã¶lÃ§Ã¼ bir qÉ™dÉ™r bÃ¶yÃ¼k ola bilÉ™r, lakin 100-150MB aralÄ±ÄŸÄ±nda olacaq). |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `quantize_to_gguf.py` skriptini yaradÄ±n. Bu prosesin nÉ™ticÉ™si olan **`az_llm_100m_q4_0.gguf`** faylÄ± Ollama-da istifadÉ™ Ã¼Ã§Ã¼n hazÄ±rdÄ±r.
