# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 30

## Modelin YÃ¼ngÃ¼llÉ™ÅŸdirilmÉ™si (Quantization): YaddaÅŸa QÉ™naÉ™t ğŸ’¾

Salam! ÃœÃ§Ã¼ncÃ¼ 10 gÃ¼nlÃ¼k mÉ™rhÉ™lÉ™mizin sonuna Ã§atdÄ±q! ArtÄ±q **100M parametreli AzÉ™rbaycan dili LLM-imiz** tÉ™lim olunub vÉ™ mÉ™tn generasiya edÉ™ bilir. Ä°ndi isÉ™ modelimizi **Ollama** kimi yÃ¼ngÃ¼l mÃ¼hitlÉ™rdÉ™ istifadÉ™ etmÉ™k Ã¼Ã§Ã¼n optimallaÅŸdÄ±rmalÄ±yÄ±q. Bu proses **Quantization (KvantlaÅŸdÄ±rma)** adlanÄ±r.

### 1. Quantization NÉ™dir?

Biz modelimizi **FP32** (32-bit) vÉ™ ya **FP16** (16-bit) dÉ™qiqlikdÉ™ tÉ™lim etdik. Bu, hÉ™r bir parametr Ã¼Ã§Ã¼n 4 vÉ™ ya 2 bayt yaddaÅŸ demÉ™kdir.

> **Quantization** â€” modelin Ã§É™kilÉ™rini daha aÅŸaÄŸÄ± dÉ™qiqliyÉ™ (mÉ™sÉ™lÉ™n, **INT8** (8-bit) vÉ™ ya **INT4** (4-bit)) Ã§evirmÉ™k prosesidir.

*   **FP32 (4 bayt/parametr):** 124M parametr $\approx$ 497 MB
*   **INT8 (1 bayt/parametr):** 124M parametr $\approx$ **124 MB**
*   **INT4 (0.5 bayt/parametr):** 124M parametr $\approx$ **62 MB**

Quantization modelin Ã¶lÃ§Ã¼sÃ¼nÃ¼ vÉ™ yaddaÅŸ tÉ™lÉ™bini kÉ™skin ÅŸÉ™kildÉ™ azaldÄ±r, eyni zamanda sÃ¼rÉ™ti artÄ±rÄ±r.

### 2. Quantization-Ä±n NÃ¶vlÉ™ri

Quantization-Ä±n iki É™sas nÃ¶vÃ¼ var:

1.  **Post-Training Quantization (PTQ):** TÉ™limdÉ™n sonra aparÄ±lÄ±r. Modelin Ã§É™kilÉ™ri birbaÅŸa Ã§evrilir.
2.  **Quantization-Aware Training (QAT):** TÉ™lim zamanÄ± aparÄ±lÄ±r. Model tÉ™lim zamanÄ± kvantlaÅŸdÄ±rÄ±lmÄ±ÅŸ dÉ™yÉ™rlÉ™rlÉ™ iÅŸlÉ™mÉ™yÉ™ Ã¶yrÉ™dilir. (Daha mÃ¼rÉ™kkÉ™bdir, daha yaxÅŸÄ± nÉ™ticÉ™ verir).

Bizim mÉ™qsÉ™dimiz **Ollama** Ã¼Ã§Ã¼n model hazÄ±rlamaq olduÄŸu Ã¼Ã§Ã¼n, **GGUF** formatÄ±na Ã§evirmÉ™ zamanÄ± avtomatik olaraq **PTQ** tÉ™tbiq edÉ™cÉ™yik.

### 3. GGUF FormatÄ±na GiriÅŸ

**GGUF (GPT-GEneration Unified Format)** â€” LLM-lÉ™ri yÃ¼ngÃ¼l mÃ¼hitlÉ™rdÉ™ (mÉ™sÉ™lÉ™n, CPU-da) iÅŸlÉ™tmÉ™k Ã¼Ã§Ã¼n nÉ™zÉ™rdÉ™ tutulmuÅŸ xÃ¼susi bir fayl formatÄ±dÄ±r.

*   **ÃœstÃ¼nlÃ¼klÉ™ri:**
    *   **Ã‡ox PlatformalÄ±:** Windows, Linux, Mac-dÉ™ iÅŸlÉ™yir.
    *   **Quantization DÉ™stÉ™yi:** MÃ¼xtÉ™lif kvantlaÅŸdÄ±rma sÉ™viyyÉ™lÉ™rini (Q4_K_M, Q5_K_M vÉ™ s.) dÉ™stÉ™klÉ™yir.
    *   **Ollama DÉ™stÉ™yi:** Ollama bu formatÄ± birbaÅŸa istifadÉ™ edir.

Bizim yol xÉ™ritÉ™miz:
1.  PyTorch modelini Hugging Face **`transformers`** formatÄ±na Ã§evirmÉ™k.
2.  Hugging Face modelini **`llama.cpp`** alÉ™tlÉ™ri ilÉ™ **GGUF** formatÄ±na Ã§evirmÉ™k.

### 4. PyTorch-dan Hugging Face-É™ Ã‡evirmÉ™

Bizim NanoGPT modelimiz GPT-2 arxitekturasÄ±na É™saslanÄ±r. Bizim PyTorch Ã§É™kilÉ™rimizi Hugging Face-in standart GPT-2 modelinÉ™ uyÄŸunlaÅŸdÄ±rmalÄ±yÄ±q.

AÅŸaÄŸÄ±dakÄ± kodu **`export_hf.py`** adlÄ± bir faylda yazaq.

```python
# export_hf.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from config import GPTConfig
from model import GPT
from tokenizers import Tokenizer

# 1. Konfiqurasiya vÉ™ Modelin YÃ¼klÉ™nmÉ™si
config = GPTConfig()
model = GPT(config)
model.load_state_dict(torch.load('best_model.pt'))
model.eval()

# 2. Hugging Face Modelini Yaratmaq
# Bizim modelimiz GPT-2 arxitekturasÄ±na bÉ™nzÉ™diyi Ã¼Ã§Ã¼n GPT-2-ni istifadÉ™ edirik
hf_config = AutoModelForCausalLM.from_pretrained("gpt2").config
hf_config.vocab_size = config.vocab_size
hf_config.n_layer = config.n_layer
hf_config.n_head = config.n_head
hf_config.n_embd = config.n_embd
hf_config.max_position_embeddings = config.block_size

hf_model = AutoModelForCausalLM(hf_config)

# 3. Ã‡É™kilÉ™rin KÃ¶Ã§Ã¼rÃ¼lmÉ™si (Mapping)
# Bu, É™n Ã§É™tin hissÉ™dir. Bizim Ã§É™kilÉ™rimizi HF modelinin Ã§É™kilÉ™rinÉ™ uyÄŸunlaÅŸdÄ±rmalÄ±yÄ±q.
# Bu hissÉ™ NanoGPT-nin rÉ™smi export skriptindÉ™n gÃ¶tÃ¼rÃ¼lÃ¼r.

# ... (Ã‡É™kilÉ™rin kÃ¶Ã§Ã¼rÃ¼lmÉ™si kodu burada yerlÉ™ÅŸÉ™cÉ™k - Ã§ox uzundur) ...
# SadÉ™lik Ã¼Ã§Ã¼n, bu hissÉ™ni nÃ¶vbÉ™ti gÃ¼nlÉ™rdÉ™ detallÄ± yazacaÄŸÄ±q.

# 4. Tokenizatorun SaxlanmasÄ±
tokenizer = Tokenizer.from_file("az_bpe_tokenizer.json")
tokenizer.save_model("az_llm_hf") # HF formatÄ±nda saxlayÄ±rÄ±q

# 5. Modelin SaxlanmasÄ±
# hf_model.save_pretrained("az_llm_hf")
```

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: DÃ¼ÅŸÃ¼n vÉ™ HazÄ±rlÄ±q

1.  Quantization-Ä±n modelin Ã¶lÃ§Ã¼sÃ¼nÉ™ tÉ™sirini bir daha nÉ™zÉ™rdÉ™n keÃ§irin.
2.  `transformers` kitabxanasÄ±nÄ±n quraÅŸdÄ±rÄ±ldÄ±ÄŸÄ±ndan É™min olun.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **PyTorch Ã§É™kilÉ™rini Hugging Face formatÄ±na** Ã§evirmÉ™ kodunu detallÄ± ÅŸÉ™kildÉ™ yazacaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
