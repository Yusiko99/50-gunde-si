# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 19

## Modelin Test EdilmÉ™si: Ä°lk SÄ±naqlar ğŸ§ª

Salam! DÃ¼nÉ™n modelimizin parametr sayÄ±nÄ±n riyazi hesablamasÄ±nÄ± Ã¶yrÉ™ndik. Bu gÃ¼n isÉ™ modelimizin tÉ™limÉ™ baÅŸlamazdan É™vvÉ™l dÃ¼zgÃ¼n iÅŸlÉ™diyini yoxlamaq Ã¼Ã§Ã¼n **Ä°rÉ™li Ã–tÃ¼rmÉ™ (Forward Pass)** vÉ™ **GeriyÉ™ Ã–tÃ¼rmÉ™ (Backward Pass)** testlÉ™rini icra edÉ™cÉ™yik.

Bu testlÉ™r, kodumuzda hÉ™r hansÄ± bir riyazi xÉ™tanÄ±n (mÉ™sÉ™lÉ™n, matris Ã¶lÃ§Ã¼lÉ™rinin uyÄŸunsuzluÄŸu) olub-olmadÄ±ÄŸÄ±nÄ± yoxlamaq Ã¼Ã§Ã¼n vacibdir.

### 1. Ä°rÉ™li Ã–tÃ¼rmÉ™ (Forward Pass)

**Ä°rÉ™li Ã–tÃ¼rmÉ™** â€” giriÅŸ mÉ™lumatÄ±nÄ±n (token ID-lÉ™ri) modelin bÃ¼tÃ¼n qatlarÄ±ndan keÃ§É™rÉ™k Ã§Ä±xÄ±ÅŸa (nÃ¶vbÉ™ti token ehtimallarÄ±na, yÉ™ni **logits**-É™) Ã§evrilmÉ™si prosesidir.

Bizim modelimizdÉ™ bu, `model(idx)` funksiyasÄ± ilÉ™ hÉ™yata keÃ§irilir.

#### Test 1: Ã–lÃ§Ã¼lÉ™rin YoxlanÄ±lmasÄ±

AÅŸaÄŸÄ±dakÄ± kodu **`test_model.py`** adlÄ± bir faylda yazaq.

```python
# test_model.py
import torch
from config import GPTConfig
from model import GPT # DÃ¼nÉ™n yaratdÄ±ÄŸÄ±mÄ±z tam GPT sinfi

# 1. KonfiqurasiyanÄ± yÃ¼klÉ™yirik
config = GPTConfig()

# 2. Modeli yaradÄ±rÄ±q
model = GPT(config)
# Modelin GPU-da iÅŸlÉ™mÉ™si Ã¼Ã§Ã¼n onu CUDA-ya gÃ¶ndÉ™ririk
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

# 3. SÄ±naq GiriÅŸi (Dummy Input)
# 4 cÃ¼mlÉ™ (Batch Size), hÉ™r biri 10 token uzunluÄŸunda
# Token ID-lÉ™ri 0-dan vocab_size-1 aralÄ±ÄŸÄ±nda tÉ™sadÃ¼fi seÃ§ilir
dummy_input = torch.randint(0, config.vocab_size, (4, 10)).to(device)

print(f"GiriÅŸ Ã¶lÃ§Ã¼sÃ¼ (Batch, T): {dummy_input.shape}")

# 4. Ä°rÉ™li Ã–tÃ¼rmÉ™ (Forward Pass)
# ModelÉ™ giriÅŸ mÉ™lumatÄ±nÄ± veririk
logits, loss = model(dummy_input)

print(f"Ã‡Ä±xÄ±ÅŸ Logits Ã¶lÃ§Ã¼sÃ¼: {logits.shape}")
# GÃ¶zlÉ™nilÉ™n nÉ™ticÉ™: (Batch, T, vocab_size) -> (4, 10, 32000)
```

**Kodun Ä°zahÄ±:**
*   `model.to(device)`: Modeli GPU-ya (vÉ™ ya CPU-ya) kÃ¶Ã§Ã¼rÃ¼r.
*   `torch.randint(...)`: TÉ™sadÃ¼fi token ID-lÉ™rindÉ™n ibarÉ™t sÄ±naq mÉ™lumatÄ± yaradÄ±r.
*   `logits, loss = model(dummy_input)`: Modelin `forward` metodunu Ã§aÄŸÄ±rÄ±r. `targets` verilmÉ™diyi Ã¼Ã§Ã¼n `loss` `None` olacaq.
*   ÆgÉ™r Ã§Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼sÃ¼ **`(4, 10, 32000)`** olarsa, demÉ™li, modelin bÃ¼tÃ¼n qatlarÄ± dÃ¼zgÃ¼n iÅŸlÉ™yir vÉ™ matris Ã¶lÃ§Ã¼lÉ™ri uyÄŸundur.

### 2. GeriyÉ™ Ã–tÃ¼rmÉ™ (Backward Pass) vÉ™ Ä°tki (Loss) Testi

**GeriyÉ™ Ã–tÃ¼rmÉ™** â€” modelin Ã§Ä±xÄ±ÅŸÄ± ilÉ™ hÉ™dÉ™f Ã§Ä±xÄ±ÅŸ arasÄ±ndakÄ± fÉ™rqi (Ä°tki, yÉ™ni Loss) hesablayaraq, bu fÉ™rqin modelin parametrlÉ™rinÉ™ gÃ¶rÉ™ qradiyentlÉ™rini (tÃ¶rÉ™mÉ™lÉ™rini) hesablamaq prosesidir. Bu, tÉ™limin É™sasÄ±nÄ± tÉ™ÅŸkil edir.

#### Test 2: Ä°tki HesablanmasÄ±

Ä°ndi modelÉ™ hÉ™dÉ™f tokenlÉ™ri (`targets`) verÉ™rÉ™k `loss`-un hesablanmasÄ±nÄ± yoxlayaq.

```python
# test_model.py (DavamÄ±)

# 5. SÄ±naq HÉ™dÉ™flÉ™ri (Dummy Targets)
# HÉ™dÉ™flÉ™r dÉ™ token ID-lÉ™rindÉ™n ibarÉ™t olmalÄ±dÄ±r
dummy_targets = torch.randint(0, config.vocab_size, (4, 10)).to(device)

# 6. Ä°tki HesablanmasÄ±
logits, loss = model(dummy_input, targets=dummy_targets)

print(f"\nÄ°tki (Loss) dÉ™yÉ™ri: {loss.item():.4f}")
# GÃ¶zlÉ™nilÉ™n nÉ™ticÉ™: Loss dÉ™yÉ™ri tÉ™xminÉ™n ln(vocab_size) olmalÄ±dÄ±r.
# ln(32000) â‰ˆ 10.37. YÉ™ni, tÉ™xminÉ™n 10.3-É™ yaxÄ±n bir rÉ™qÉ™m gÃ¶zlÉ™yirik.

# 7. GeriyÉ™ Ã–tÃ¼rmÉ™ (Backward Pass)
# QradiyentlÉ™ri hesablamaq
loss.backward()

print("GeriyÉ™ Ã–tÃ¼rmÉ™ uÄŸurla icra edildi.")

# 8. QradiyentlÉ™rin YoxlanÄ±lmasÄ±
# Modelin bir parametrinin qradiyentini yoxlayaq
param_grad = model.lm_head.weight.grad
print(f"LM Head Ã§É™kilÉ™rinin qradiyent Ã¶lÃ§Ã¼sÃ¼: {param_grad.shape}")
# GÃ¶zlÉ™nilÉ™n nÉ™ticÉ™: (vocab_size, n_embd) -> (32000, 768)

# 9. TÉ™mizlÉ™mÉ™
# NÃ¶vbÉ™ti testlÉ™r Ã¼Ã§Ã¼n qradiyentlÉ™ri sÄ±fÄ±rlayÄ±rÄ±q
model.zero_grad()
```

**Kodun Ä°zahÄ±:**
*   `dummy_targets`: Modelin proqnozlaÅŸdÄ±rmalÄ± olduÄŸu doÄŸru nÃ¶vbÉ™ti tokenlÉ™rdir.
*   `loss.item()`: Loss dÉ™yÉ™rini PyTorch Tensor-dan adi Python rÉ™qÉ™minÉ™ Ã§evirir.
*   `loss.backward()`: **Æsas GeriyÉ™ Ã–tÃ¼rmÉ™ É™mri.** Bu, modelin bÃ¼tÃ¼n parametrlÉ™ri Ã¼Ã§Ã¼n qradiyentlÉ™ri hesablayÄ±r.
*   `model.lm_head.weight.grad`: `lm_head` qatÄ±nÄ±n Ã§É™kilÉ™ri Ã¼Ã§Ã¼n hesablanmÄ±ÅŸ qradiyentlÉ™ri yoxlayÄ±rÄ±q. ÆgÉ™r bu dÉ™yÉ™r `None` deyilsÉ™, demÉ™li, GeriyÉ™ Ã–tÃ¼rmÉ™ dÃ¼zgÃ¼n iÅŸlÉ™yib.

### 3. NiyÉ™ Loss TÉ™xminÉ™n 10.3-dÃ¼r?

Model tÉ™lim olunmadÄ±ÄŸÄ± Ã¼Ã§Ã¼n, hÉ™r bir tokeni tÉ™sadÃ¼fi ÅŸÉ™kildÉ™ proqnozlaÅŸdÄ±rÄ±r. 32000 tokenlik bir sÃ¶zlÃ¼kdÉ™, hÉ™r bir tokenin seÃ§ilmÉ™ ehtimalÄ± **1/32000**-dir.

**Cross-Entropy Loss**-un dÃ¼sturu sadÉ™lÉ™ÅŸdirilmiÅŸ ÅŸÉ™kildÉ™ belÉ™dir: `Loss = -log(Ehtimal)`.
*   Loss = `-log(1/32000)`
*   Loss = `log(32000)`
*   `ln(32000)` â‰ˆ **10.37**

TÉ™limin É™vvÉ™lindÉ™ bu rÉ™qÉ™min É™trafÄ±nda bir dÉ™yÉ™r gÃ¶rmÉ™k, modelimizin riyazi olaraq dÃ¼zgÃ¼n qurulduÄŸunu gÃ¶stÉ™rir.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  **`test_model.py`** faylÄ±nÄ± yaradÄ±n vÉ™ icra edin.
2.  Ã‡Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼lÉ™rinin vÉ™ Loss dÉ™yÉ™rinin gÃ¶zlÉ™nilÉ™n nÉ™ticÉ™lÉ™rÉ™ uyÄŸun olduÄŸunu yoxlayÄ±n.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah modelin tÉ™limdÉ™n É™vvÉ™l necÉ™ mÉ™tn yaratdÄ±ÄŸÄ±nÄ± gÃ¶rmÉ™k Ã¼Ã§Ã¼n **MÉ™tn GenerasiyasÄ± (Sampling)** mexanizmini Ã¶yrÉ™nÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 800 sÃ¶z.
