# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 15

## Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention) ğŸ‘ï¸â€ğŸ—¨ï¸

Salam! DÃ¼nÉ™n NanoGPT modelimizin tÉ™mÉ™l qatlarÄ±nÄ± (Embedding, Linear) qurduq. Bu gÃ¼n isÉ™ modelin É™n gÃ¼clÃ¼ vÉ™ mÃ¼rÉ™kkÉ™b hissÉ™sinÉ™ â€“ **Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention)** mexanizminÉ™ keÃ§irik.

### 1. NiyÉ™ "Ã‡oxbaÅŸlÄ±"?

DÃ¼nÉ™n Ã¶yrÉ™ndiyimiz **Self-Attention** mexanizmi bir sÃ¶zÃ¼n cÃ¼mlÉ™dÉ™ki digÉ™r sÃ¶zlÉ™rlÉ™ olan É™laqÉ™sini tapÄ±r. Lakin bu, yalnÄ±z **bir nÃ¶v** É™laqÉ™ni tapÄ±r.

MÉ™sÉ™lÉ™n, "AzÉ™rbaycanÄ±n **gÃ¶zÉ™l** paytaxtÄ± **BakÄ±**dÄ±r." cÃ¼mlÉ™sindÉ™:
*   Bir diqqÉ™t baÅŸÄ± "**gÃ¶zÉ™l**" sÃ¶zÃ¼nÃ¼n "**BakÄ±**" sÃ¶zÃ¼ ilÉ™ É™laqÉ™sini (sifÉ™t-isim É™laqÉ™si) tapa bilÉ™r.
*   BaÅŸqa bir diqqÉ™t baÅŸÄ± isÉ™ "**AzÉ™rbaycanÄ±n**" sÃ¶zÃ¼nÃ¼n "**paytaxtÄ±**" sÃ¶zÃ¼ ilÉ™ É™laqÉ™sini (yiyÉ™lik-mÉ™nsubiyyÉ™t É™laqÉ™si) tapa bilÉ™r.

> **Ã‡oxbaÅŸlÄ± DiqqÉ™t** â€” eyni anda bir neÃ§É™ (bizim halÄ±mÄ±zda **12**) fÉ™rqli diqqÉ™t mexanizmini paralel ÅŸÉ™kildÉ™ iÅŸlÉ™tmÉ™k demÉ™kdir. HÉ™r bir "baÅŸ" mÉ™tnin fÉ™rqli bir aspektinÉ™, fÉ™rqli bir É™laqÉ™ nÃ¶vÃ¼nÉ™ fokuslanÄ±r.

Bu, modelin mÉ™tnin bÃ¼tÃ¼n incÉ™liklÉ™rini, qrammatik vÉ™ semantik É™laqÉ™lÉ™rini eyni anda Ã¶yrÉ™nmÉ™sinÉ™ imkan verir.

### 2. Ã‡oxbaÅŸlÄ± DiqqÉ™tin Ä°ÅŸ Prinsipi

1.  **BÃ¶lÃ¼nmÉ™:** GiriÅŸ vektoru (`n_embd=768`) **`n_head=12`** sayda bÉ™rabÉ™r hissÉ™yÉ™ bÃ¶lÃ¼nÃ¼r. HÉ™r bir hissÉ™nin Ã¶lÃ§Ã¼sÃ¼ `768 / 12 = 64` olur.
2.  **Paralel Hesablama:** HÉ™r bir kiÃ§ik hissÉ™ Ã¼zÉ™rindÉ™ mÃ¼stÉ™qil olaraq **Self-Attention** (MaskalanmÄ±ÅŸ) É™mÉ™liyyatÄ± aparÄ±lÄ±r.
3.  **BirlÉ™ÅŸdirmÉ™:** BÃ¼tÃ¼n 12 baÅŸÄ±n Ã§Ä±xÄ±ÅŸlarÄ± (hÉ™r biri 64 Ã¶lÃ§Ã¼lÃ¼) yenidÉ™n birlÉ™ÅŸdirilir vÉ™ É™vvÉ™lki Ã¶lÃ§Ã¼yÉ™ (`768`) qaytarÄ±lÄ±r.
4.  **Son XÉ™tti Qat:** BirlÉ™ÅŸdirilmiÅŸ Ã§Ä±xÄ±ÅŸ son bir xÉ™tti qatdan keÃ§irilir.

### 3. PyTorch-da Ã‡oxbaÅŸlÄ± DiqqÉ™tin QurulmasÄ±

Biz dÃ¼nÉ™nki **SelfAttention** sinfini **MultiHeadAttention** sinfinin iÃ§indÉ™ istifadÉ™ edÉ™cÉ™yik.

AÅŸaÄŸÄ±dakÄ± kodu **`attention.py`** faylÄ±na É™lavÉ™ edÉ™k (vÉ™ ya yenidÉ™n yazaq).

```python
# attention.py (DavamÄ±)
import torch
import torch.nn as nn
from torch.nn import functional as F
from config import GPTConfig

# DÃ¼nÉ™nki SelfAttention sinfi (sadÉ™lik Ã¼Ã§Ã¼n burada tÉ™krar yazÄ±lmÄ±r, amma ehtiyac var)
# ... (SelfAttention sinfi buraya É™lavÉ™ olunmalÄ±dÄ±r) ...

class MultiHeadAttention(nn.Module):
    """ Ã‡oxbaÅŸlÄ± MaskalanmÄ±ÅŸ Ã–z-DiqqÉ™t Mexanizmi """

    def __init__(self, config):
        super().__init__()
        # Modelin hiperparametrlÉ™rini konfiqurasiyadan alÄ±rÄ±q
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        self.head_size = self.n_embd // self.n_head # HÉ™r baÅŸÄ±n Ã¶lÃ§Ã¼sÃ¼: 768 / 12 = 64

        # BÃ¼tÃ¼n Q, K, V proyeksiyalarÄ±nÄ± eyni anda edÉ™n xÉ™tti qat
        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=config.bias)
        # BirlÉ™ÅŸdirilmiÅŸ Ã§Ä±xÄ±ÅŸÄ± emal edÉ™n son xÉ™tti qat
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=config.bias)
        self.attn_dropout = nn.Dropout(self.dropout)
        self.resid_dropout = nn.Dropout(self.dropout)

        # MaskanÄ± yaratmaq (yalnÄ±z bir dÉ™fÉ™)
        # Bu, modelin Ã¶zÃ¼ndÉ™n sonrakÄ± tokenlÉ™rÉ™ baxmasÄ±nÄ±n qarÅŸÄ±sÄ±nÄ± alÄ±r
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # Batch, ArdÄ±cÄ±llÄ±q UzunluÄŸu, GÃ¶mÃ¼lmÉ™ Ã–lÃ§Ã¼sÃ¼ (768)

        # 1. Q, K, V-ni Hesablamaq
        # c_attn(x) -> (B, T, 3 * C)
        # split(3) -> (B, T, C), (B, T, C), (B, T, C)
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)

        # 2. Ã‡oxbaÅŸlÄ± ÅÉ™klÉ™ Salmaq
        # (B, T, C) -> (B, T, n_head, head_size) -> (B, n_head, T, head_size)
        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)

        # 3. Scaled Dot-Product Attention
        # (B, n_head, T, head_size) @ (B, n_head, head_size, T) -> (B, n_head, T, T)
        att = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)

        # 4. Maskalanma (Masking)
        # Ã–zÃ¼ndÉ™n sonrakÄ± tokenlÉ™rÉ™ diqqÉ™ti sÄ±fÄ±ra endiririk
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))

        # 5. Softmax vÉ™ Dropout
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)

        # 6. DÉ™yÉ™rin Ã‡É™kilmÉ™si (wei @ V)
        # out -> (B, n_head, T, head_size)
        out = att @ v

        # 7. BaÅŸlarÄ± BirlÉ™ÅŸdirmÉ™k
        # (B, n_head, T, head_size) -> (B, T, n_head, head_size) -> (B, T, C)
        out = out.transpose(1, 2).contiguous().view(B, T, C)

        # 8. Son Proyeksiya
        out = self.resid_dropout(self.c_proj(out))
        return out
```

### 4. Kodun Ä°zahÄ± (Æsas MÉ™qamlar)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 23 | `self.head_size = self.n_embd // self.n_head` | HÉ™r baÅŸÄ±n Ã¶lÃ§Ã¼sÃ¼nÃ¼ hesablayÄ±r (768 / 12 = 64). |
| 26 | `self.c_attn = nn.Linear(..., 3 * self.n_embd, ...)` | **Æsas fÉ™rq budur!** Q, K, V-ni ayrÄ±-ayrÄ± 3 xÉ™tti qatdan keÃ§irmÉ™k É™vÉ™zinÉ™, bir bÃ¶yÃ¼k qatdan keÃ§irib sonra 3 bÉ™rabÉ™r hissÉ™yÉ™ bÃ¶lÃ¼rÃ¼k. Bu, daha sÉ™mÉ™rÉ™lidir. |
| 35 | `q, k, v = self.c_attn(x).split(self.n_embd, dim=2)` | GiriÅŸdÉ™n Ã§Ä±xan 3 * 768 Ã¶lÃ§Ã¼lÃ¼ vektoru Q, K, V (hÉ™r biri 768 Ã¶lÃ§Ã¼lÃ¼) olaraq bÃ¶lÃ¼rÃ¼k. |
| 39 | `k = k.view(...).transpose(1, 2)` | Vektoru **Ã‡oxbaÅŸlÄ±** formata salÄ±rÄ±q: `(B, T, 12, 64)` -> `(B, 12, T, 64)`. Ä°ndi 12 baÅŸ paralel iÅŸlÉ™yÉ™ bilÉ™r. |
| 44 | `att = (q @ k.transpose(-2, -1)) * ...` | DiqqÉ™t Ã§É™kilÉ™rini hesablayÄ±rÄ±q. |
| 53 | `out = att @ v` | DiqqÉ™t Ã§É™kilÉ™rini dÉ™yÉ™rlÉ™rÉ™ tÉ™tbiq edirik. |
| 57 | `out = out.transpose(1, 2).contiguous().view(B, T, C)` | 12 baÅŸÄ±n Ã§Ä±xÄ±ÅŸÄ±nÄ± yenidÉ™n birlÉ™ÅŸdirib É™vvÉ™lki `(B, T, 768)` formasÄ±na qaytarÄ±rÄ±q. |
| 60 | `out = self.resid_dropout(self.c_proj(out))` | Son xÉ™tti qatdan keÃ§irib **Dropout** tÉ™tbiq edirik.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: DÃ¼ÅŸÃ¼n vÉ™ Praktika

1.  **`attention.py`** faylÄ±nÄ± yaradÄ±n vÉ™ `MultiHeadAttention` sinfini ora kopyalayÄ±n.
2.  NiyÉ™ Q, K, V-ni ayrÄ±-ayrÄ± qatlardan keÃ§irmÉ™k É™vÉ™zinÉ™, bir bÃ¶yÃ¼k qatdan keÃ§irib bÃ¶lmÉ™k daha sÉ™mÉ™rÉ™lidir? (Cavab: GPU-lar bÃ¶yÃ¼k matris É™mÉ™liyyatlarÄ±nÄ± kiÃ§ik É™mÉ™liyyatlardan daha sÃ¼rÉ™tli icra edir).

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **Transformer Blokunun** bÃ¼tÃ¼n komponentlÉ™rini (Ã‡oxbaÅŸlÄ± DiqqÉ™t, LayerNorm, Feed-Forward) birlÉ™ÅŸdirÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
