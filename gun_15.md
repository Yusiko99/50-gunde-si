# GÃ¼n 15: Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention) ğŸ¤¯

## 15.1. TÉ™k BaÅŸlÄ± DiqqÉ™tin MÉ™hdudiyyÉ™ti

DÃ¼nÉ™n biz **TÉ™k BaÅŸlÄ± DiqqÉ™t (Single Attention Head)** mexanizmini Ã¶yrÉ™ndik. Bu mexanizm modelÉ™ bir sÃ¶zÃ¼n digÉ™r sÃ¶zlÉ™rlÉ™ olan **bir nÃ¶v** É™laqÉ™sini tapmaÄŸa kÃ¶mÉ™k edir. Lakin dil Ã§ox mÃ¼rÉ™kkÉ™bdir vÉ™ bir sÃ¶zÃ¼n eyni anda bir neÃ§É™ fÉ™rqli É™laqÉ™si ola bilÉ™r:

*   **Sintaktik ÆlaqÉ™:** CÃ¼mlÉ™nin qrammatik quruluÅŸu.
*   **Semantik ÆlaqÉ™:** SÃ¶zlÉ™rin mÉ™nasÄ±.
*   **Referensial ÆlaqÉ™:** ÆvÉ™zliklÉ™rin aid olduÄŸu isimlÉ™r.

TÉ™k bir diqqÉ™t baÅŸÄ± bÃ¼tÃ¼n bu É™laqÉ™lÉ™ri eyni anda Ã¶yrÉ™nmÉ™kdÉ™ Ã§É™tinlik Ã§É™kir.

## 15.2. Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention - MHA)

**Ã‡oxbaÅŸlÄ± DiqqÉ™t** bu problemi hÉ™ll edir. O, sadÉ™cÉ™ olaraq, diqqÉ™t mexanizmini **paralel ÅŸÉ™kildÉ™ bir neÃ§É™ dÉ™fÉ™** (bizim modelimizdÉ™ 12 dÉ™fÉ™) icra edir.

*   HÉ™r bir **"baÅŸ"** (Head) fÉ™rqli bir É™laqÉ™ nÃ¶vÃ¼nÃ¼ Ã¶yrÉ™nmÉ™yÉ™ fokuslanÄ±r.
*   MÉ™sÉ™lÉ™n, bir baÅŸ sintaktik É™laqÉ™yÉ™, digÉ™ri isÉ™ semantik É™laqÉ™yÉ™ diqqÉ™t yetirÉ™ bilÉ™r.

**MHA-nÄ±n Ä°ÅŸ Prinsipi:**

1.  **Paralel Hesablama:** GiriÅŸ mÉ™lumatÄ± eyni anda **N sayda** (bizim halda 12) mÃ¼stÉ™qil diqqÉ™t baÅŸÄ±na gÃ¶ndÉ™rilir.
2.  **NÉ™ticÉ™lÉ™rin BirlÉ™ÅŸdirilmÉ™si:** HÉ™r bir baÅŸ Ã¶z nÉ™ticÉ™sini (V matrisinin Ã§É™kili cÉ™mi) Ã§Ä±xarÄ±r.
3.  **XÉ™tti Lay:** BÃ¼tÃ¼n nÉ™ticÉ™lÉ™r birlÉ™ÅŸdirilir (Concatenate) vÉ™ yekun bir **XÉ™tti Lay (Linear Layer)**-dÉ™n keÃ§irilÉ™rÉ™k modelin É™sas Ã¶lÃ§Ã¼sÃ¼nÉ™ (768) qaytarÄ±lÄ±r.

Bu, modelÉ™ eyni anda mÉ™tnin mÃ¼xtÉ™lif aspektlÉ™rinÉ™ **"diqqÉ™t yetirmÉ™yÉ™"** imkan verir.

## 15.3. Praktika: Multi-Head Attention-Ä±n QurulmasÄ±

DÃ¼nÉ™nki `Head` sinfini istifadÉ™ edÉ™rÉ™k `MultiHeadAttention` sinfini quraq.

**`multi_head_attention.py`**

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
# DÃ¼nÉ™nki Head sinfini bura kopyalayÄ±n vÉ™ ya import edin

# Modelin É™sas hiperparametrlÉ™ri (GÃ¼n 13-dÉ™n)
n_embd = 768  # Embedding Ã¶lÃ§Ã¼sÃ¼
n_head = 12   # BaÅŸlarÄ±n sayÄ±
block_size = 256 # Kontekst uzunluÄŸu

# Head sinfi (GÃ¼n 14-dÉ™n)
class Head(nn.Module):
    # ... (Head sinfinin kodu olduÄŸu kimi qalÄ±r) ...
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        
    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)   
        q = self.query(x) 
        
        head_size = k.shape[-1]
        wei = q @ k.transpose(-2, -1) * head_size**-0.5 
        
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        
        wei = F.softmax(wei, dim=-1) 
        
        v = self.value(x) 
        out = wei @ v     
        
        return out


class MultiHeadAttention(nn.Module):
    """Ã‡oxbaÅŸlÄ± DiqqÉ™t Mexanizmi"""
    
    def __init__(self, num_heads, head_size):
        super().__init__()
        # N sayda (12) Head sinfini paralel ÅŸÉ™kildÉ™ yaradÄ±rÄ±q
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        
        # BÃ¼tÃ¼n nÉ™ticÉ™lÉ™ri birlÉ™ÅŸdirdikdÉ™n sonra tÉ™tbiq olunacaq yekun xÉ™tti lay
        self.proj = nn.Linear(n_embd, n_embd)
        
        # RTX 2050 Ã¼Ã§Ã¼n kritik: Dropout
        # TÉ™lim zamanÄ± neyronlarÄ±n bir hissÉ™sini tÉ™sadÃ¼fi olaraq sÃ¶ndÃ¼rÃ¼r.
        # Bu, modelin hÉ™ddindÉ™n artÄ±q Ã¶yrÉ™nmÉ™sinin (Overfitting) qarÅŸÄ±sÄ±nÄ± alÄ±r.
        self.dropout = nn.Dropout(0.1) 

    def forward(self, x):
        # 1. BÃ¼tÃ¼n baÅŸlarÄ± paralel icra etmÉ™k
        # NÉ™ticÉ™: [ (B, T, head_size), (B, T, head_size), ... ]
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        
        # 2. BirlÉ™ÅŸdirilmiÅŸ nÉ™ticÉ™ni yekun xÉ™tti laydan keÃ§irmÉ™k
        out = self.dropout(self.proj(out))
        
        return out

# NÃ¼munÉ™: Multi-Head Attention yaratmaq
mha = MultiHeadAttention(num_heads=n_head, head_size=n_embd // n_head)
print(mha)
```

## 15.4. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **49** | `self.heads = nn.ModuleList([...])` | **`nn.ModuleList`** PyTorch-da bir neÃ§É™ eyni sinfi (bizim halda 12 É™dÉ™d `Head` sinfini) bir siyahÄ±da saxlamaÄŸa imkan verir. |
| **52** | `self.proj = nn.Linear(n_embd, n_embd)` | BÃ¼tÃ¼n 12 baÅŸÄ±n nÉ™ticÉ™si birlÉ™ÅŸdirildikdÉ™n sonra, bu lay nÉ™ticÉ™ni yenidÉ™n modelin É™sas Ã¶lÃ§Ã¼sÃ¼nÉ™ (768) qaytarÄ±r. |
| **56** | `self.dropout = nn.Dropout(0.1)` | **Dropout** tÉ™limi sabitlÉ™ÅŸdirmÉ™k Ã¼Ã§Ã¼n vacibdir. 0.1 o demÉ™kdir ki, hÉ™r addÄ±mda neyronlarÄ±n 10%-i tÉ™sadÃ¼fi olaraq sÃ¶ndÃ¼rÃ¼lÉ™cÉ™k. |
| **60** | `out = torch.cat([h(x) for h in self.heads], dim=-1)` | BÃ¼tÃ¼n 12 baÅŸÄ±n Ã§Ä±xÄ±ÅŸÄ±nÄ± **sonuncu Ã¶lÃ§Ã¼ (dim=-1)** Ã¼zrÉ™ birlÉ™ÅŸdirir (Concatenate). NÉ™ticÉ™nin Ã¶lÃ§Ã¼sÃ¼: (Batch, Time, 12 * 64) = (B, T, 768). |
| **62** | `out = self.dropout(self.proj(out))` | BirlÉ™ÅŸdirilmiÅŸ nÉ™ticÉ™ni `proj` layÄ±ndan keÃ§irir vÉ™ Dropout tÉ™tbiq edir. |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `multi_head_attention.py` skriptini yaradÄ±n. `torch.cat` É™mÉ™liyyatÄ±nÄ±n nÉ™ticÉ™nin Ã¶lÃ§Ã¼sÃ¼nÃ¼ necÉ™ dÉ™yiÅŸdiyini anlamaÄŸa Ã§alÄ±ÅŸÄ±n. Bu, Transformer Blokunun É™sasÄ±nÄ± tÉ™ÅŸkil edir.
