# G羹n 13: Transformer: LLM-lrin Beyni 

## 13.1. Transformer Arxitekturas覺 Ndir?

2017-ci ild Google trfindn tqdim ediln **Transformer** arxitekturas覺, B繹y羹k Dil Modellrinin (LLM) sas覺n覺 tkil edir. Bu arxitektura, vvlki modellrdn (RNN, LSTM) frqli olaraq, mtnin emal覺nda **ard覺c覺ll覺qdan as覺l覺l覺覺** aradan qald覺rd覺 v **paralel hesablama** imkanlar覺n覺 kskin kild art覺rd覺.

Transformer-in sas 羹st羹nl羹y羹, mtnin istniln hisssin eyni anda baxa bilmsidir.

## 13.2. Transformer-in sas Komponentlri

Transformer iki sas hissdn ibartdir:

| Komponent | Mqsd | Bizim Modelimizdki Rolu |
| :--- | :--- | :--- |
| **Encoder (Kodlay覺c覺)** | Giri mtnini (c羹mlri) oxuyur v onun mnas覺n覺 baa d羹羹r. | Bizim modelimizd istifad **edilmir**. |
| **Decoder (Dekodlay覺c覺)** | Encoder-dn gln mlumat覺 istifad edrk 癟覺x覺 mtnini (cavab覺) yarad覺r. | **Bizim modelimizin sas覺n覺 tkil edir.** |

Bizim **GPT (Generative Pre-trained Transformer)** modelimiz, ad覺ndan da g繹r羹nd羹y羹 kimi, yaln覺z **Decoder** hisssindn istifad edir. Bu, modelin **bir sonrak覺 tokeni proqnozlad覺rmaq** 羹癟羹n nzrd tutulmu bir arxitekturad覺r.

## 13.3. Decoder-in Daxili Quruluu

Decoder-in sas覺nda iki vacib mexanizm dayan覺r:

1.  **Masked Multi-Head Attention (Maskalanm覺 oxbal覺 Diqqt):** Mtnin bir hisssin baxarkn, modelin hans覺 s繹zlr daha 癟ox diqqt yetirmli olduunu m羹yynldirir. **"Maskalanm覺"** olmas覺 o demkdir ki, model proqnozlad覺rd覺覺 s繹zdn sonrak覺 s繹zlr baxa bilmz. Bu, modelin "f覺r覺ldaq癟覺l覺q" etmsinin qar覺s覺n覺 al覺r.
2.  **Feed-Forward Network (襤rli-t羹rm bksi):** Diqqt mexanizmindn gln mlumat覺 emal edir v modelin 繹yrnm qabiliyytini art覺r覺r.

Bu iki blok ard覺c覺l olaraq bir ne癟 df (bizim 100M modelimizd 12 df) tkrarlan覺r.

## 13.4. NanoGPT-y Giri

Biz modelimizi Andrej Karpathy trfindn yarad覺lm覺 **NanoGPT** layihsinin sadldirilmi versiyas覺na saslanaraq quraca覺q. NanoGPT, GPT-nin sas prinsiplrini **minimum kodla** izah etmk 羹癟羹n nzrd tutulmudur.

**NanoGPT-nin sas X羹susiyytlri:**

*   **Sadlik:** M羹rkkb optimallad覺rmalar olmadan, tmiz PyTorch kodu.
*   **yrnmy Fokus:** Hr bir hissnin funksiyas覺n覺 asanl覺qla anlamaa imkan verir.

Biz NanoGPT-nin arxitekturas覺n覺 g繹t羹rck, onu Azrbaycan dilin uyunlad覺racaq v RTX 2050 羹癟羹n optimallad覺raca覺q.

**Modelimizin Parametrlri (100M hdfi 羹癟羹n):**

| Parametr | Dyr | 襤zah覺 |
| :--- | :--- | :--- |
| **Block Size (Context Length)** | 256 | Modelin bir dfy emal ed bilcyi maksimum token say覺. |
| **Embedding Dimension (n_embd)** | 768 | Hr bir tokenin rqmsal tsvirinin 繹l癟羹s羹. |
| **Number of Heads (n_head)** | 12 | Multi-Head Attention-dak覺 "ba" say覺. |
| **Number of Layers (n_layer)** | 12 | Tkrarlanan Transformer Bloklar覺n覺n say覺. |
| **Vocabulary Size (vocab_size)** | 32000 | Tokenizatorumuzun l羹t 繹l癟羹s羹. |

Bu konfiqurasiya il modelimizin parametr say覺 txminn **100 milyon** olacaq.

**G羹ndlik Tap覺r覺q:** Transformer arxitekturas覺 haqq覺nda q覺sa bir video izlyin. X羹susil **"Masked Self-Attention"** anlay覺覺n覺 drindn baa d羹my 癟al覺覺n.
