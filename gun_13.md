# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 13

## NanoGPT-yÉ™ GiriÅŸ: SadÉ™likdÉ™ki GÃ¼c ğŸ’¡

Salam! DÃ¼nÉ™n DiqqÉ™t Mexanizminin (Attention) É™saslarÄ±nÄ± Ã¶yrÉ™ndik. Bu gÃ¼n isÉ™ bu mexanizmi istifadÉ™ edÉ™n vÉ™ bizim 100M parametreli modelimizin É™sasÄ±nÄ± tÉ™ÅŸkil edÉ™n arxitekturaya â€“ **NanoGPT**-yÉ™ giriÅŸ edirik.

### 1. NanoGPT NÉ™dir?

**NanoGPT** É™slindÉ™ OpenAI tÉ™rÉ™findÉ™n yaradÄ±lmÄ±ÅŸ **GPT-2** modelinin **minimalist** vÉ™ **sadÉ™lÉ™ÅŸdirilmiÅŸ** bir tÉ™tbiqidir. Bu layihÉ™ Andrej Karpathy tÉ™rÉ™findÉ™n yaradÄ±lmÄ±ÅŸdÄ±r vÉ™ GPT modellÉ™rinin necÉ™ iÅŸlÉ™diyini sÄ±fÄ±rdan, É™n sadÉ™ ÅŸÉ™kildÉ™ Ã¶yrÉ™nmÉ™k Ã¼Ã§Ã¼n ideal bir baÅŸlanÄŸÄ±cdÄ±r.

> **NanoGPT** â€” GPT (Generative Pre-trained Transformer) arxitekturasÄ±nÄ±n bÃ¼tÃ¼n É™sas komponentlÉ™rini ehtiva edÉ™n, lakin kod bazasÄ± Ã§ox kiÃ§ik vÉ™ asan baÅŸa dÃ¼ÅŸÃ¼lÉ™n bir PyTorch tÉ™tbiqidir.

Bizim 100M parametreli modelimiz Ã¼Ã§Ã¼n NanoGPT-ni seÃ§mÉ™yimizin É™sas sÉ™bÉ™blÉ™ri:

1.  **SadÉ™lik:** Kodun hÉ™r sÉ™tri aydÄ±n vÉ™ izahlÄ±dÄ±r. Bu, Python-a yeni baÅŸlayanlar Ã¼Ã§Ã¼n ideal bir Ã¶yrÉ™nmÉ™ vasitÉ™sidir.
2.  **GPT-yÉ™ BÉ™nzÉ™rlik:** NanoGPT, GPT-2-nin bÃ¼tÃ¼n É™sas xÃ¼susiyyÉ™tlÉ™rini (MaskalanmÄ±ÅŸ DiqqÉ™t, Transformer BloklarÄ±) saxlayÄ±r.
3.  **Ã–lÃ§Ã¼:** NanoGPT kiÃ§ik vÉ™ orta Ã¶lÃ§Ã¼lÃ¼ (mÉ™sÉ™lÉ™n, 100M) modellÉ™ri tÉ™lim etmÉ™k Ã¼Ã§Ã¼n nÉ™zÉ™rdÉ™ tutulub. Bu, bizim **NVIDIA T4 (12 GB VRAM)** kimi ÅŸÉ™xsi cihazÄ±mÄ±zda tÉ™lim etmÉ™k Ã¼Ã§Ã¼n mÃ¼kÉ™mmÉ™ldir.

### 2. NanoGPT-nin Ãœmumi Strukturu

NanoGPT modeli bir neÃ§É™ É™sas hissÉ™dÉ™n ibarÉ™tdir, hansÄ± ki, biz onlarÄ± nÃ¶vbÉ™ti gÃ¼nlÉ™rdÉ™ PyTorch-da sÄ±fÄ±rdan quracaÄŸÄ±q:

| HissÉ™ | Funksiya | PyTorch-da TÉ™tbiqi |
| :--- | :--- | :--- |
| **Token GÃ¶mÃ¼lmÉ™si (Token Embedding)** | HÉ™r bir token ID-sini (rÉ™qÉ™mini) modelin emal edÉ™ bilÉ™cÉ™yi rÉ™qÉ™msal vektora Ã§evirir. | `nn.Embedding` |
| **MÃ¶vqe GÃ¶mÃ¼lmÉ™si (Positional Embedding)** | Tokenin cÃ¼mlÉ™dÉ™ki mÃ¶vqeyini (sÄ±rasÄ±nÄ±) modelÉ™ bildirir. | `nn.Embedding` |
| **Transformer BloklarÄ±** | Æsas emal iÅŸini gÃ¶rÉ™n, DiqqÉ™t vÉ™ Ä°rÉ™li Ã–tÃ¼rmÉ™ qatlarÄ±nÄ± ehtiva edÉ™n bloklar. | `nn.Module` sinfi |
| **Qat NormallaÅŸdÄ±rmasÄ± (Layer Normalization)** | HÉ™r bir blokun Ã§Ä±xÄ±ÅŸÄ±nÄ± normallaÅŸdÄ±rÄ±r. | `nn.LayerNorm` |
| **Son XÉ™tti Qat (Linear Head)** | Transformer BloklarÄ±nÄ±n Ã§Ä±xÄ±ÅŸÄ±nÄ± yenidÉ™n sÃ¶zlÃ¼k hÉ™cminÉ™ (token ID-lÉ™rinÉ™) Ã§evirir. | `nn.Linear` |

### 3. 100M Parametr Ã¼Ã§Ã¼n HiperparametrlÉ™r

Modelin Ã¶lÃ§Ã¼sÃ¼ (parametrlÉ™rin sayÄ±) onun **hiperparametrlÉ™ri** ilÉ™ mÃ¼É™yyÉ™n edilir. Bizim hÉ™dÉ™fimiz **~100 Milyon** parametrdir.

Æsas hiperparametrlÉ™r bunlardÄ±r:

| Hiperparametr | Ä°zah | 100M Ã¼Ã§Ã¼n TÉ™xmini DÉ™yÉ™r |
| :--- | :--- | :--- |
| **`n_layer`** | Transformer BloklarÄ±nÄ±n sayÄ± (dÉ™rinlik). | **12** |
| **`n_head`** | HÉ™r bir DiqqÉ™t MexanizmindÉ™ki baÅŸlarÄ±n sayÄ±. | **12** |
| **`n_embd`** | GÃ¶mÃ¼lmÉ™ Ã¶lÃ§Ã¼sÃ¼ (gizli Ã¶lÃ§Ã¼). HÉ™r bir tokenin vektoru bu Ã¶lÃ§Ã¼dÉ™dir. | **768** |
| **`block_size`** | Modelin baxa bilÉ™cÉ™yi maksimum ardÄ±cÄ±llÄ±q uzunluÄŸu (kontekst pÉ™ncÉ™rÉ™si). | **512** vÉ™ ya **1024** |
| **`vocab_size`** | Tokenizatorumuzun sÃ¶zlÃ¼k hÉ™cmi. | **32000** |

**Hesablama:** GPT-2 (117M) modeli 12 qat, 12 baÅŸ vÉ™ 768 gizli Ã¶lÃ§Ã¼dÉ™n istifadÉ™ edir. Bizim NanoGPT tÉ™tbiqimiz dÉ™ bu parametrlÉ™rlÉ™ tÉ™xminÉ™n **124 Milyon** parametrÉ™ sahib olacaq. Bu, bizim **~100M** hÉ™dÉ™fimizÉ™ Ã§ox yaxÄ±ndÄ±r vÉ™ bizim T4 GPU-muz Ã¼Ã§Ã¼n idarÉ™olunandÄ±r.

### 4. Modelin KonfiqurasiyasÄ±

Biz bÃ¼tÃ¼n bu hiperparametrlÉ™ri bir yerdÉ™ saxlayacaÄŸÄ±q. Bu, modelin qurulmasÄ±nÄ± vÉ™ tÉ™limini asanlaÅŸdÄ±racaq.

```python
# config.py
import math

# Modelin KonfiqurasiyasÄ± (GPT-2 Small É™sasÄ±nda)
class GPTConfig:
    # MÉ™lumatla baÄŸlÄ± parametrlÉ™r
    vocab_size = 32000      # Tokenizatorumuzun sÃ¶zlÃ¼k hÉ™cmi
    block_size = 512        # Maksimum ardÄ±cÄ±llÄ±q uzunluÄŸu (kontekst pÉ™ncÉ™rÉ™si)

    # Modelin arxitekturasÄ± ilÉ™ baÄŸlÄ± parametrlÉ™r
    n_layer = 12            # Transformer qatlarÄ±nÄ±n sayÄ±
    n_head = 12             # DiqqÉ™t baÅŸlarÄ±nÄ±n sayÄ±
    n_embd = 768            # GÃ¶mÃ¼lmÉ™ Ã¶lÃ§Ã¼sÃ¼ (gizli Ã¶lÃ§Ã¼)

    # TÉ™limlÉ™ baÄŸlÄ± parametrlÉ™r
    dropout = 0.1           # Dropout nisbÉ™ti (overfitting-in qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n)
    bias = False            # BÉ™zi qatlarda bias istifadÉ™ edib-etmÉ™mÉ™k

    def __init__(self, **kwargs):
        # ÆlavÉ™ parametrlÉ™ri dÉ™ qÉ™bul etmÉ™k Ã¼Ã§Ã¼n
        for k, v in kwargs.items():
            setattr(self, k, v)

# ParametrlÉ™rin tÉ™xmini hesablanmasÄ± (SadÉ™lÉ™ÅŸdirilmiÅŸ)
# Parametr sayÄ± tÉ™xminÉ™n 12 * (12 * 768 * 768 * 4 + 768 * 3072 * 2) + 32000 * 768
# Bu, tÉ™xminÉ™n 124 Milyon parametrÉ™ bÉ™rabÉ™rdir.
```

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  **`config.py`** faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± kodu ora kopyalayÄ±n.
2.  **`n_layer`**, **`n_head`**, **`n_embd`** dÉ™yÉ™rlÉ™rini dÉ™yiÅŸdirÉ™rÉ™k modelin parametr sayÄ±nÄ±n necÉ™ dÉ™yiÅŸÉ™cÉ™yini dÃ¼ÅŸÃ¼nÃ¼n. MÉ™sÉ™lÉ™n, `n_embd`-ni 512-yÉ™ endirsÉ™k, parametr sayÄ± necÉ™ dÉ™yiÅŸÉ™r?

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **PyTorch-da Æsas BloklarÄ±** â€“ GÃ¶mÃ¼lmÉ™ QatÄ±nÄ± (Embedding Layer) vÉ™ XÉ™tti QatlarÄ± necÉ™ quracaÄŸÄ±mÄ±zÄ± Ã¶yrÉ™nÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
