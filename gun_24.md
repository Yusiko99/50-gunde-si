# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 24

## OptimallaÅŸdÄ±rÄ±cÄ± vÉ™ Ã–yrÉ™nmÉ™ SÃ¼rÉ™ti: TÉ™limin SÃ¼kanÄ± âš™ï¸

Salam! DÃ¼nÉ™n LLM-imizin tÉ™lim dÃ¶vrÃ¼nÃ¼ iÅŸÉ™ saldÄ±q. Bu gÃ¼n isÉ™ tÉ™limin É™n kritik iki elementini â€“ **OptimallaÅŸdÄ±rÄ±cÄ±** vÉ™ **Ã–yrÉ™nmÉ™ SÃ¼rÉ™tini (Learning Rate)** daha dÉ™rindÉ™n araÅŸdÄ±racaÄŸÄ±q.

### 1. AdamW OptimallaÅŸdÄ±rÄ±cÄ±sÄ±

Bizim `train.py` skriptimizdÉ™ **AdamW** optimallaÅŸdÄ±rÄ±cÄ±sÄ±ndan istifadÉ™ etdik.

> **AdamW** â€” DÉ™rin Ã–yrÉ™nmÉ™ modellÉ™ri Ã¼Ã§Ã¼n É™n populyar vÉ™ effektiv optimallaÅŸdÄ±rÄ±cÄ±lardan biridir. O, hÉ™r bir parametr Ã¼Ã§Ã¼n fÉ™rdi ÅŸÉ™kildÉ™ Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini tÉ™nzimlÉ™yir.

AdamW-nin É™sas Ã¼stÃ¼nlÃ¼klÉ™ri:
*   **Momentum:** ÆvvÉ™lki addÄ±mlarÄ±n istiqamÉ™tini yadda saxlayÄ±r, bu da tÉ™limi daha sÃ¼rÉ™tli vÉ™ stabil edir.
*   **Adaptive Learning Rate:** HÉ™r bir parametr Ã¼Ã§Ã¼n fÉ™rqli Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti tÉ™tbiq edir.
*   **Weight Decay (L2 Regularization):** Modelin hÉ™ddindÉ™n artÄ±q uyÄŸunlaÅŸmasÄ±nÄ±n (Overfitting) qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n Ã§É™kilÉ™ri kiÃ§ik saxlayÄ±r.

**Kodda TÉ™tbiqi:**

```python
# train.py-dan xatÄ±rlatma
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
```

### 2. Ã–yrÉ™nmÉ™ SÃ¼rÉ™ti (Learning Rate - LR)

Ã–yrÉ™nmÉ™ SÃ¼rÉ™ti, optimallaÅŸdÄ±rÄ±cÄ±nÄ±n hÉ™r addÄ±mda modelin Ã§É™kilÉ™rini nÉ™ qÉ™dÉ™r dÉ™yiÅŸÉ™cÉ™yini mÃ¼É™yyÉ™n edÉ™n É™n vacib hiperparametrdir.

*   **Ã‡ox YÃ¼ksÉ™k LR:** Model hÉ™dÉ™fi "qaÃ§Ä±rar", itki (Loss) dÉ™yÉ™ri ya partlayar, ya da tÉ™sadÃ¼fi dÉ™yiÅŸÉ™r.
*   **Ã‡ox AÅŸaÄŸÄ± LR:** Model Ã§ox yavaÅŸ Ã¶yrÉ™nÉ™r, tÉ™lim Ã§ox uzun Ã§É™kÉ™r.

Bizim 100M parametreli modelimiz Ã¼Ã§Ã¼n `LEARNING_RATE = 6e-4` (yÉ™ni 0.0006) yaxÅŸÄ± bir baÅŸlanÄŸÄ±cdÄ±r.

### 3. Learning Rate Scheduler (Ã–yrÉ™nmÉ™ SÃ¼rÉ™ti CÉ™dvÉ™li)

TÉ™lim prosesi boyunca Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini sabit saxlamaq optimal deyil. Æn yaxÅŸÄ± nÉ™ticÉ™lÉ™r Ã¼Ã§Ã¼n Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini tÉ™limin gediÅŸatÄ±na uyÄŸun olaraq dÉ™yiÅŸdirmÉ™k lazÄ±mdÄ±r. Buna **Learning Rate Scheduling** deyilir.

Biz iki É™sas strategiyadan istifadÉ™ edÉ™cÉ™yik:

#### A. Warmup (Ä°sinmÉ™)

TÉ™limin É™vvÉ™lindÉ™ modelin Ã§É™kilÉ™ri tÉ™sadÃ¼fi olduÄŸu Ã¼Ã§Ã¼n, yÃ¼ksÉ™k Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti modelin stabilliyini poza bilÉ™r.

> **Warmup** â€” tÉ™limin ilk bir neÃ§É™ yÃ¼z addÄ±mÄ±nda Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini **sÄ±fÄ±rdan** tÉ™dricÉ™n É™sas LR dÉ™yÉ™rinÉ™ (`6e-4`) qÉ™dÉ™r artÄ±rmaqdÄ±r.

Bu, modelin tÉ™limÉ™ yumÅŸaq baÅŸlamasÄ±nÄ± tÉ™min edir.

#### B. Cosine Decay (Kosinus AzalmasÄ±)

Warmup bitdikdÉ™n sonra, Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti É™sas LR dÉ™yÉ™rindÉ™n baÅŸlayaraq tÉ™limin sonuna qÉ™dÉ™r **kosinus funksiyasÄ±** ÅŸÉ™klindÉ™ tÉ™dricÉ™n sÄ±fÄ±ra doÄŸru azaldÄ±lÄ±r.

> **Cosine Decay** â€” Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini tÉ™limin sonuna yaxÄ±nlaÅŸdÄ±qca yavaÅŸ-yavaÅŸ azaltmaqla modelin É™n yaxÅŸÄ± nÉ™ticÉ™yÉ™ (Loss-un É™n aÅŸaÄŸÄ± nÃ¶qtÉ™sinÉ™) daha dÉ™qiq Ã§atmasÄ±na kÃ¶mÉ™k edir.

### 4. PyTorch-da Scheduler-in TÉ™tbiqi

Biz bu scheduler-i Hugging Face-in `accelerate` kitabxanasÄ± ilÉ™ birlikdÉ™ istifadÉ™ edÉ™cÉ™yik.

AÅŸaÄŸÄ±dakÄ± kodu `train.py` skriptinÉ™ É™lavÉ™ edirik.

```python
# train.py (Scheduler hissÉ™si)
from transformers import get_cosine_schedule_with_warmup # Yeni import

# ... (É™vvÉ™lki kodlar) ...

# 1. HiperparametrlÉ™r
# ...
WARMUP_ITERS = 100 # Ä°lk 100 addÄ±m Warmup olacaq
# ...

# 3. Model, DataLoader vÉ™ Optimizer-i HazÄ±rlamaq
# ... (É™vvÉ™lki kodlar) ...

# 4. Scheduler-i Yaratmaq
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=WARMUP_ITERS,
    num_training_steps=MAX_ITERS,
)

# 5. Akselerator ilÉ™ hazÄ±rlamaq (Scheduler-i dÉ™ É™lavÉ™ edirik)
model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(
    model, optimizer, train_loader, val_loader, scheduler # Scheduler É™lavÉ™ olundu
)

# 6. TÉ™lim DÃ¶vrÃ¼ (YenilÉ™nmiÅŸ)
# ...
for iter_num in tqdm(range(MAX_ITERS), desc="TÉ™lim Prosesi"):
    # ... (Validasiya vÉ™ MÉ™lumat YÃ¼klÉ™mÉ™) ...

    # C. Ä°rÉ™li Ã–tÃ¼rmÉ™ vÉ™ Ä°tki HesablanmasÄ±
    with accelerator.accumulate(model):
        # ... (loss hesablanmasÄ±) ...
        
        accelerator.backward(loss)
        
        if accelerator.sync_gradients:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            
        optimizer.step()
        scheduler.step() # Scheduler-i hÉ™r addÄ±mda yenilÉ™yirik
        optimizer.zero_grad()
```

**Kodun Ä°zahÄ±:**
*   `from transformers import get_cosine_schedule_with_warmup`: Hugging Face `transformers` kitabxanasÄ±ndan bu funksiyanÄ± daxil edirik. (Qeyd: `pip install transformers` tÉ™lÉ™b oluna bilÉ™r).
*   `scheduler = get_cosine_schedule_with_warmup(...)`: Scheduler obyektini yaradÄ±rÄ±q.
*   `scheduler.step()`: HÉ™r tÉ™lim addÄ±mÄ±ndan sonra Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini tÉ™nzimlÉ™yir.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `llm_50gun` mÃ¼hitindÉ™ `transformers` kitabxanasÄ±nÄ± quraÅŸdÄ±rÄ±n: `pip install transformers`.
2.  `train.py` skriptinÉ™ `scheduler` hissÉ™sini É™lavÉ™ edin vÉ™ tÉ™limi yenidÉ™n baÅŸladÄ±n.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **GPU-da TÉ™limin BaÅŸlanmasÄ±** vÉ™ **QarÄ±ÅŸÄ±q DÉ™qiqlik (Mixed Precision)** mÃ¶vzusunu daha É™traflÄ± araÅŸdÄ±racaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
