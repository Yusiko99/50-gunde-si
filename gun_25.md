# GÃ¼n 25: RTX 2050-dÉ™ TÉ™limin BaÅŸlanmasÄ± (OptimallaÅŸdÄ±rma) ğŸš€

## 25.1. NiyÉ™ `accelerate`?

ÆvvÉ™lki gÃ¼nlÉ™rdÉ™ biz **NanoGPT** modelini vÉ™ tÉ™lim dÃ¶vrÃ¼nÃ¼ PyTorch-da qurduq. Ä°ndi isÉ™ bu tÉ™lim dÃ¶vrÃ¼nÃ¼ sizin **RTX 2050 (4GB VRAM)** kartÄ±nÄ±z Ã¼Ã§Ã¼n optimallaÅŸdÄ±rmalÄ±yÄ±q.

**`accelerate`** kitabxanasÄ± Hugging Face tÉ™rÉ™findÉ™n yaradÄ±lmÄ±ÅŸdÄ±r vÉ™ bizÉ™ **Distributed Training (PaylanmÄ±ÅŸ TÉ™lim)**, **Mixed Precision (FP16)** vÉ™ **Gradient Accumulation** kimi mÃ¼rÉ™kkÉ™b optimallaÅŸdÄ±rmalarÄ± **sadÉ™cÉ™ bir neÃ§É™ sÉ™tir kodla** tÉ™tbiq etmÉ™yÉ™ imkan verir.

## 25.2. `accelerate` ilÉ™ TÉ™lim DÃ¶vrÃ¼nÃ¼n HazÄ±rlanmasÄ±

Bizim `train.py` skriptimizdÉ™ dÉ™yiÅŸikliklÉ™r edÉ™rÉ™k `accelerate` istifadÉ™ edÉ™cÉ™yik.

**`train_accelerate.py` (Æsas dÉ™yiÅŸikliklÉ™r)**

```python
# ... (ÆvvÉ™lki importlar vÉ™ model/data yÃ¼klÉ™nmÉ™si) ...
from accelerate import Accelerator

# 1. Accelerator-un yaradÄ±lmasÄ±
# Mixed Precision-Ä± avtomatik tÉ™tbiq edÉ™cÉ™k
accelerator = Accelerator(
    gradient_accumulation_steps=4, # Gradient Accumulation addÄ±mÄ±
    mixed_precision='fp16' # RTX 2050 Ã¼Ã§Ã¼n kritik optimallaÅŸdÄ±rma
)

# 2. Model, OptimallaÅŸdÄ±rÄ±cÄ± vÉ™ DataLoader-in Accelerator-a Ã¶tÃ¼rÃ¼lmÉ™si
model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, val_dataloader
)

# ... (TÉ™lim dÃ¶vrÃ¼) ...

# 3. Gradient Accumulation-Ä±n tÉ™tbiqi
for step, batch in enumerate(train_dataloader):
    with accelerator.accumulate(model):
        # ... (Forward pass vÉ™ loss hesablanmasÄ±) ...
        
        # Loss-u geri yaymaq (Backpropagation)
        accelerator.backward(loss)
        
        # QradiyentlÉ™ri yenilÉ™mÉ™k
        optimizer.step()
        optimizer.zero_grad()
        
    # ... (Monitorinq vÉ™ Checkpoint) ...
```

## 25.3. RTX 2050 Ã¼Ã§Ã¼n Kritik ParametrlÉ™r

Sizin 4GB VRAM-Ä±nÄ±z Ã¼Ã§Ã¼n É™n vacib konfiqurasiya addÄ±mlarÄ± bunlardÄ±r:

### A. Mixed Precision (FP16)

`accelerator = Accelerator(mixed_precision='fp16')` É™mri modelin Ã§É™kilÉ™rini vÉ™ É™mÉ™liyyatlarÄ±nÄ± 16-bit dÉ™qiqlikdÉ™ aparmaÄŸa mÉ™cbur edir. Bu, **VRAM istifadÉ™sini tÉ™xminÉ™n 50% azaldÄ±r**.

### B. Gradient Accumulation (Qradiyent YÄ±ÄŸÄ±mÄ±)

`gradient_accumulation_steps=4` tÉ™yin etdik.

*   **Mini Batch Size (HÉ™qiqi Batch Size):** Tutaq ki, VRAM-Ä±nÄ±z yalnÄ±z **Batch Size = 4**-É™ icazÉ™ verir.
*   **Gradient Accumulation Steps:** 4
*   **Effektiv Batch Size:** $4 \times 4 = 16$

Bu o demÉ™kdir ki, model hÉ™r 4 kiÃ§ik Batch-dÉ™n sonra bir dÉ™fÉ™ Ã§É™kilÉ™rini yenilÉ™yÉ™cÉ™k. Bu, 4GB VRAM-da belÉ™, daha bÃ¶yÃ¼k Batch Size-Ä±n tÉ™sirini simulyasiya etmÉ™yÉ™ imkan verir.

## 25.4. TÉ™limin BaÅŸlanmasÄ±

TÉ™limi baÅŸlatmaq Ã¼Ã§Ã¼n sadÉ™cÉ™ `python train.py` É™vÉ™zinÉ™ `accelerate` istifadÉ™ edirik:

**AddÄ±m 1: Konfiqurasiya FaylÄ±nÄ±n YaradÄ±lmasÄ±**

Terminalda `accelerate config` É™mrini icra edin. Bu, kitabxananÄ±n sizin sisteminizi (GPU, VRAM) tanÄ±yÄ±b uyÄŸun parametrlÉ™ri tÉ™yin etmÉ™sinÉ™ kÃ¶mÉ™k edir.

**Æsas Konfiqurasiya SeÃ§imlÉ™ri:**

| Sual | Cavab (RTX 2050 Ã¼Ã§Ã¼n) | Ä°zahÄ± |
| :--- | :--- | :--- |
| **How many GPUs are you using?** | 1 | TÉ™k GPU istifadÉ™ edirik. |
| **Do you wish to use FP16 or BF16?** | **fp16** | **Kritik:** VRAM-Ä± 50% azaltmaq Ã¼Ã§Ã¼n FP16-nÄ± seÃ§irik. |
| **Do you want to use DeepSpeed?** | No | DeepSpeed daha bÃ¶yÃ¼k modellÉ™r Ã¼Ã§Ã¼ndÃ¼r. |

**AddÄ±m 2: TÉ™limin BaÅŸlanmasÄ±**

```bash
accelerate launch train_accelerate.py
```

Bu É™mr `accelerate` konfiqurasiyanÄ±zÄ± oxuyacaq, FP16 vÉ™ Gradient Accumulation-Ä± tÉ™tbiq edÉ™cÉ™k vÉ™ tÉ™limi optimallaÅŸdÄ±rÄ±lmÄ±ÅŸ ÅŸÉ™kildÉ™ baÅŸladacaq.

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `train_accelerate.py` skriptini GÃ¼n 23-dÉ™ki `train.py` skriptinÉ™ É™saslanaraq yenilÉ™yin. Terminalda `accelerate config` É™mrini icra edin vÉ™ konfiqurasiya faylÄ±nÄ± yaradÄ±n.
