# GÃ¼n 7: Dataset Ä°nÅŸasÄ± II: Web Scraping (MÉ™lumatÄ±n Ã‡É™kilmÉ™si) ğŸ•¸ï¸

## 7.1. Web Scraping NÉ™dir?

**Web Scraping (VebdÉ™n MÉ™lumat Ã‡É™kmÉ™)** â€“ veb-saytlardan avtomatik olaraq mÉ™lumat toplama prosesidir. Bizim mÉ™qsÉ™dimiz, GÃ¼n 6-da mÃ¼É™yyÉ™nlÉ™ÅŸdirdiyimiz URL-lÉ™rdÉ™n mÉ™tn mÉ™lumatlarÄ±nÄ± Ã§É™kmÉ™kdir.

Bu proses Ã¼Ã§Ã¼n iki É™sas Python kitabxanasÄ±ndan istifadÉ™ edÉ™cÉ™yik:

1.  **`requests`:** Veb-saytÄ±n HTML mÉ™zmununu É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n.
2.  **`BeautifulSoup`:** HTML mÉ™zmununu analiz etmÉ™k vÉ™ yalnÄ±z lazÄ±m olan mÉ™tn hissÉ™lÉ™rini (mÉ™sÉ™lÉ™n, mÉ™qalÉ™nin mÉ™tni) Ã§Ä±xarmaq Ã¼Ã§Ã¼n.

## 7.2. Etik vÉ™ HÃ¼quqi MÃ¼lahizÉ™lÉ™r

Web Scraping edÉ™rkÉ™n **etik vÉ™ hÃ¼quqi mÉ™suliyyÉ™tlÉ™rinizi** unutmayÄ±n:

*   **`robots.txt`:** HÉ™r hansÄ± bir saytÄ± Ã§É™kmÉ™zdÉ™n É™vvÉ™l, hÉ™min saytÄ±n `robots.txt` faylÄ±nÄ± yoxlayÄ±n. Bu fayl, saytÄ±n hansÄ± hissÉ™lÉ™rinin Ã§É™kilmÉ™sinÉ™ icazÉ™ verildiyini gÃ¶stÉ™rir.
*   **Server YÃ¼kÃ¼:** SorÄŸularÄ± Ã§ox sÃ¼rÉ™tli gÃ¶ndÉ™rmÉ™yin. Bu, saytÄ±n serverini yÃ¼klÉ™yÉ™ bilÉ™r. SorÄŸular arasÄ±nda kiÃ§ik bir gecikmÉ™ (mÉ™sÉ™lÉ™n, 1 saniyÉ™) qoymaq mÉ™slÉ™hÉ™tdir.
*   **MÃ¼É™llif HÃ¼quqlarÄ±:** TopladÄ±ÄŸÄ±nÄ±z mÉ™lumatÄ± yalnÄ±z **tÉ™lim mÉ™qsÉ™dlÉ™ri** Ã¼Ã§Ã¼n istifadÉ™ edin vÉ™ heÃ§ bir halda kommersiya mÉ™qsÉ™dlÉ™ri Ã¼Ã§Ã¼n yenidÉ™n yayÄ±mlamayÄ±n.

## 7.3. Praktika: SadÉ™ Scraping Skripti

GÉ™lin, sadÉ™ bir veb-saytdan mÉ™lumat Ã§É™kÉ™n Python skripti yazaq.

**`scraper.py`**

```python
import requests
from bs4 import BeautifulSoup
import time
import random

# 1. MÉ™nbÉ™ URL-lÉ™ri
# Bu siyahÄ±nÄ± GÃ¼n 6-da hazÄ±rladÄ±ÄŸÄ±nÄ±z URL-lÉ™rlÉ™ É™vÉ™z edin.
URLS = [
    "https://az.wikipedia.org/wiki/Az%C9%99rbaycan_dili",
    "https://report.az/siyaset/", # NÃ¼munÉ™ olaraq
    # ... digÉ™r URL-lÉ™r
]

# 2. MÉ™lumatÄ± saxlayacaÄŸÄ±mÄ±z fayl
OUTPUT_FILE = "raw_corpus.txt"

def scrape_page(url):
    """VerilmiÅŸ URL-dÉ™n mÉ™tn mÉ™lumatÄ±nÄ± Ã§É™kir."""
    try:
        # 3. Veb-sayta sorÄŸu gÃ¶ndÉ™rmÉ™k
        # BÉ™zi saytlar botlarÄ± bloklayÄ±r, buna gÃ¶rÉ™ dÉ™ User-Agent É™lavÉ™ edirik.
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # XÉ™ta olarsa, xÉ™bÉ™rdarlÄ±q et

        # 4. HTML-i analiz etmÉ™k
        soup = BeautifulSoup(response.content, 'html.parser')

        # 5. Æsas mÉ™tn hissÉ™lÉ™rini tapmaq
        # Bu hissÉ™ hÉ™r sayt Ã¼Ã§Ã¼n fÉ™rqli olacaq.
        # NÃ¼munÉ™: <p> teqlÉ™rinin iÃ§indÉ™ki mÉ™tn
        paragraphs = soup.find_all('p')
        
        page_text = ""
        for p in paragraphs:
            # MÉ™tnin Ã§ox qÄ±sa olub-olmadÄ±ÄŸÄ±nÄ± yoxlayÄ±rÄ±q
            if len(p.text.strip()) > 50:
                page_text += p.text.strip() + "\n\n"
        
        return page_text

    except requests.exceptions.RequestException as e:
        print(f"XÉ™ta baÅŸ verdi: {url} - {e}")
        return None

def main_scraper():
    """Æsas scraping prosesini idarÉ™ edir."""
    print(f"Scraping prosesi baÅŸladÄ±. MÉ™lumatlar '{OUTPUT_FILE}' faylÄ±na yazÄ±lacaq.")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for url in URLS:
            print(f"-> {url} Ã§É™kilir...")
            text = scrape_page(url)
            
            if text:
                f.write(f"--- URL: {url} ---\n")
                f.write(text)
                f.write("\n\n")
                print(f"   [UÄŸurlu] {len(text.split())} sÃ¶z yazÄ±ldÄ±.")
            else:
                print(f"   [UÄŸursuz] MÉ™lumat Ã§É™kilmÉ™di.")
            
            # 7. Serveri yÃ¼klÉ™mÉ™mÉ™k Ã¼Ã§Ã¼n gecikmÉ™
            delay = random.uniform(1, 3) # 1 ilÉ™ 3 saniyÉ™ arasÄ±nda tÉ™sadÃ¼fi gecikmÉ™
            time.sleep(delay)

    print("Scraping prosesi tamamlandÄ±.")

if __name__ == "__main__":
    main_scraper()
```

## 7.4. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **3** | `import requests` | Veb-saytlara HTTP sorÄŸularÄ± gÃ¶ndÉ™rmÉ™k Ã¼Ã§Ã¼n kitabxana. |
| **4** | `from bs4 import BeautifulSoup` | HTML-i analiz etmÉ™k vÉ™ mÉ™lumat Ã§Ä±xarmaq Ã¼Ã§Ã¼n kitabxana. |
| **5-6** | `import time, random` | Serveri yÃ¼klÉ™mÉ™mÉ™k Ã¼Ã§Ã¼n gecikmÉ™ yaratmaq Ã¼Ã§Ã¼n. |
| **14** | `def scrape_page(url):` | HÉ™r bir URL Ã¼Ã§Ã¼n mÉ™lumat Ã§É™kmÉ™ funksiyasÄ±. |
| **19** | `headers = {...}` | SaytÄ±n bizi bot kimi qÉ™bul etmÉ™mÉ™si Ã¼Ã§Ã¼n brauzer mÉ™lumatlarÄ±nÄ± gÃ¶ndÉ™ririk. |
| **22** | `response = requests.get(...)` | URL-É™ GET sorÄŸusu gÃ¶ndÉ™ririk. |
| **23** | `response.raise_for_status()` | SorÄŸu uÄŸursuz olarsa (mÉ™sÉ™lÉ™n, 404 xÉ™tasÄ±), proqramÄ± dayandÄ±rÄ±r. |
| **26** | `soup = BeautifulSoup(...)` | HTML mÉ™zmununu `BeautifulSoup` obyektinÉ™ Ã§evirir. |
| **30** | `paragraphs = soup.find_all('p')` | SÉ™hifÉ™dÉ™ki bÃ¼tÃ¼n `<p>` (paraqraf) teqlÉ™rini tapÄ±r. **Qeyd:** Bu, hÉ™r sayt Ã¼Ã§Ã¼n dÉ™yiÅŸmÉ™lidir! |
| **34** | `if len(p.text.strip()) > 50:` | Ã‡ox qÄ±sa paraqraflarÄ± (mÉ™sÉ™lÉ™n, baÅŸlÄ±qlarÄ±) atmaq Ã¼Ã§Ã¼n sadÉ™ tÉ™mizlÉ™mÉ™. |
| **48** | `time.sleep(delay)` | TÉ™sadÃ¼fi gecikmÉ™ tÉ™tbiq edÉ™rÉ™k serverÉ™ dostyana yanaÅŸÄ±rÄ±q. |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `scraper.py` faylÄ±nÄ± yaradÄ±n vÉ™ `URLS` siyahÄ±sÄ±nÄ± GÃ¼n 6-da tÉ™yin etdiyiniz É™n azÄ± 3-5 AzÉ™rbaycan saytÄ± ilÉ™ É™vÉ™z edin. Skripti iÅŸÉ™ salÄ±n vÉ™ `raw_corpus.txt` faylÄ±nÄ±n yarandÄ±ÄŸÄ±nÄ± yoxlayÄ±n.
