# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 17

## GPT Modelinin Tam QuruluÅŸu: NanoGPT ğŸ—ï¸

Salam! Son bir neÃ§É™ gÃ¼ndÉ™ NanoGPT modelimizin bÃ¼tÃ¼n É™sas komponentlÉ™rini â€“ GÃ¶mÃ¼lmÉ™ QatlarÄ±nÄ±, Ã‡oxbaÅŸlÄ± DiqqÉ™ti vÉ™ Transformer Blokunu (Block) qurduq. Bu gÃ¼n isÉ™ bÃ¼tÃ¼n bu hissÉ™lÉ™ri birlÉ™ÅŸdirÉ™rÉ™k **GPT (NanoGPT)** modelinin tam sinfini yaradacaÄŸÄ±q.

Bu, bizim **100 Milyon parametreli AzÉ™rbaycan dili LLM-imizin** rÉ™smi olaraq PyTorch-da doÄŸulduÄŸu gÃ¼ndÃ¼r!

### 1. GPT Modelinin Ãœmumi Strukturu

GPT modeli sadÉ™ bir ardÄ±cÄ±llÄ±qla iÅŸlÉ™yir:

1.  **GiriÅŸ:** Token ID-lÉ™ri (rÉ™qÉ™mlÉ™r ardÄ±cÄ±llÄ±ÄŸÄ±).
2.  **GÃ¶mÃ¼lmÉ™:** Token vÉ™ MÃ¶vqe GÃ¶mÃ¼lmÉ™lÉ™ri toplanÄ±r.
3.  **Transformer BloklarÄ±:** GÃ¶mÃ¼lmÃ¼ÅŸ vektorlar ardÄ±cÄ±l olaraq **`n_layer`** (bizim halÄ±mÄ±zda 12) sayda Transformer Blokundan keÃ§ir.
4.  **Ã‡Ä±xÄ±ÅŸ:** Son NormallaÅŸdÄ±rma (LayerNorm) vÉ™ XÉ™tti BaÅŸlÄ±q (LM Head) vasitÉ™silÉ™ nÃ¶vbÉ™ti tokenin ehtimalÄ± hesablanÄ±r.

### 2. PyTorch-da GPT Sinfinin TamamlanmasÄ±

Biz GÃ¼n 14-dÉ™ **`gpt_model_base.py`** adlÄ± bir fayl yaratmÄ±ÅŸdÄ±q. Ä°ndi hÉ™min faylÄ± **`model.py`** adlandÄ±raraq vÉ™ `Block` sinfini daxil edÉ™rÉ™k tamamlayÄ±rÄ±q.

```python
# model.py
import torch
import torch.nn as nn
from torch.nn import functional as F
from config import GPTConfig
from block import Block # DÃ¼nÉ™n yaratdÄ±ÄŸÄ±mÄ±z Transformer Bloku

class GPT(nn.Module):
    """ NanoGPT arxitekturasÄ±na É™saslanan BÃ¶yÃ¼k Dil Modeli """

    def __init__(self, config):
        super().__init__()
        self.config = config

        # Modelin bÃ¼tÃ¼n parametrlÉ™rini ehtiva edÉ™n É™sas konteyner
        self.transformer = nn.ModuleDict(dict(
            # 1. Token vÉ™ MÃ¶vqe GÃ¶mÃ¼lmÉ™lÉ™ri
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            # 2. Transformer BloklarÄ± (12 É™dÉ™d)
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            # 3. Son NormallaÅŸdÄ±rma
            ln_f = nn.LayerNorm(config.n_embd),
        ))
        # 4. Dil Modeli BaÅŸÄ± (LM Head)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # ParametrlÉ™rin sayÄ±nÄ± hesablayÄ±rÄ±q
        self.apply(self._init_weights)
        print(f"Modelin Ã¼mumi parametr sayÄ±: {self.get_num_params():,}")

    def get_num_params(self, non_embedding=True):
        """ Modelin parametr sayÄ±nÄ± hesablayÄ±r """
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            # GÃ¶mÃ¼lmÉ™ qatlarÄ±nÄ±n parametrlÉ™rini Ã§Ä±xarÄ±rÄ±q (bÉ™zÉ™n yÃ¼ngÃ¼llÉ™ÅŸdirmÉ™ Ã¼Ã§Ã¼n)
            n_params -= self.transformer.wpe.weight.numel()
        return n_params

    def _init_weights(self, module):
        """ ParametrlÉ™rin ilkin dÉ™yÉ™rlÉ™rini tÉ™yin edir """
        if isinstance(module, nn.Linear):
            # XÉ™tti qatlar Ã¼Ã§Ã¼n normal paylanma ilÉ™ ilkin dÉ™yÉ™rlÉ™r
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            # GÃ¶mÃ¼lmÉ™ qatlarÄ± Ã¼Ã§Ã¼n normal paylanma ilÉ™ ilkin dÉ™yÉ™rlÉ™r
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            # LayerNorm Ã¼Ã§Ã¼n vahid dÉ™yÉ™rlÉ™r
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    def forward(self, idx, targets=None):
        # idx: Token ID-lÉ™rindÉ™n ibarÉ™t Tensor (B, T)
        B, T = idx.size()

        # 1. GÃ¶mÃ¼lmÉ™lÉ™ri Hesablamaq
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # (T)
        token_emb = self.transformer.wte(idx) # (B, T, n_embd)
        pos_emb = self.transformer.wpe(pos)   # (T, n_embd)
        x = self.transformer.drop(token_emb + pos_emb) # (B, T, n_embd)

        # 2. Transformer BloklarÄ±ndan KeÃ§irmÉ™k
        for block in self.transformer.h:
            x = block(x)

        # 3. Son NormallaÅŸdÄ±rma
        x = self.transformer.ln_f(x)

        # 4. Ã‡Ä±xÄ±ÅŸ (Logits)
        logits = self.lm_head(x) # (B, T, vocab_size)
        loss = None

        # ÆgÉ™r hÉ™dÉ™f tokenlÉ™r (targets) verilibsÉ™, itkini (loss) hesablayÄ±rÄ±q
        if targets is not None:
            # Logits-i (B*T, vocab_size) vÉ™ targets-i (B*T) ÅŸÉ™klindÉ™ dÃ¼zÉ™ldirik
            logits = logits.view(-1, logits.size(-1))
            targets = targets.view(-1)
            # Ã‡arpaz Entropiya Ä°tkisi (Cross-Entropy Loss)
            loss = F.cross_entropy(logits, targets)

        return logits, loss
```

### 3. Kodun Ä°zahÄ± (Æsas MÉ™qamlar)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 18 | `self.transformer = nn.ModuleDict(dict(...))` | BÃ¼tÃ¼n Transformer komponentlÉ™rini bir lÃ¼ÄŸÉ™tdÉ™ saxlayÄ±rÄ±q. |
| 25 | `h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])` | **12 É™dÉ™d** Transformer Blokunu ardÄ±cÄ±l olaraq yaradÄ±rÄ±q. |
| 31 | `self.apply(self._init_weights)` | Modelin bÃ¼tÃ¼n qatlarÄ±na **ilkin dÉ™yÉ™rlÉ™ri** tÉ™tbiq edirik. Bu, tÉ™limin stabil baÅŸlamasÄ± Ã¼Ã§Ã¼n vacibdir. |
| 42 | `def _init_weights(self, module):` | **ParametrlÉ™rin Ä°lkin DÉ™yÉ™rlÉ™ri:** Modelin Ã¶yrÉ™nmÉ™yÉ™ baÅŸlamasÄ± Ã¼Ã§Ã¼n bÃ¼tÃ¼n Ã§É™kilÉ™rÉ™ (weights) kiÃ§ik, tÉ™sadÃ¼fi dÉ™yÉ™rlÉ™r verilir. |
| 62 | `for block in self.transformer.h:` | GÃ¶mÃ¼lmÉ™lÉ™rdÉ™n gÉ™lÉ™n mÉ™lumatÄ± ardÄ±cÄ±l olaraq 12 blokdan keÃ§iririk. |
| 71 | `if targets is not None:` | ÆgÉ™r modelÉ™ hÉ™dÉ™f tokenlÉ™r verilibsÉ™, **Ä°tki FunksiyasÄ±nÄ± (Loss Function)** hesablayÄ±rÄ±q. |
| 75 | `loss = F.cross_entropy(logits, targets)` | **Cross-Entropy Loss** istifadÉ™ edirik. Bu, generativ dil modellÉ™ri Ã¼Ã§Ã¼n standart itki funksiyasÄ±dÄ±r.

### 4. Parametr SayÄ±nÄ±n HesablanmasÄ±

Bizim konfiqurasiyamÄ±z (`n_layer=12`, `n_head=12`, `n_embd=768`, `vocab_size=32000`) ilÉ™ modelin parametr sayÄ± tÉ™xminÉ™n:

**Modelin Ã¼mumi parametr sayÄ±: 124,417,536**

Bu, bizim **~100 Milyon** parametr hÉ™dÉ™fimizÉ™ tam uyÄŸundur!

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  **`model.py`** faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± kodu ora kopyalayÄ±n.
2.  `config.py`, `block.py`, `attention.py` fayllarÄ±nÄ±n eyni qovluqda olduÄŸundan É™min olun.
3.  Modeli yaradÄ±n vÉ™ parametr sayÄ±nÄ±n yuxarÄ±dakÄ± rÉ™qÉ™mÉ™ yaxÄ±n olduÄŸunu yoxlayÄ±n.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah modelin tÉ™limdÉ™n É™vvÉ™l necÉ™ mÉ™tn yaratdÄ±ÄŸÄ±nÄ± gÃ¶rmÉ™k Ã¼Ã§Ã¼n **MÉ™tn GenerasiyasÄ± (Sampling)** mexanizmini Ã¶yrÉ™nÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 800 sÃ¶z.
