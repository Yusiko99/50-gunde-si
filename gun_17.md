# GÃ¼n 17: GPT Modelinin Tam QuruluÅŸu ğŸ—ï¸

## 17.1. BÃ¼tÃ¼n HissÉ™lÉ™rin BirlÉ™ÅŸdirilmÉ™si

ÆvvÉ™lki gÃ¼nlÉ™rdÉ™ biz LLM-in É™sas komponentlÉ™rini qurduq:
1.  **Tokenizator** (MÉ™tni rÉ™qÉ™mlÉ™rÉ™ Ã§evirir).
2.  **Head** (TÉ™k DiqqÉ™t BaÅŸÄ±).
3.  **MultiHeadAttention** (Ã‡oxbaÅŸlÄ± DiqqÉ™t).
4.  **Block** (Transformer Bloku).

Bu gÃ¼n isÉ™ bÃ¼tÃ¼n bu hissÉ™lÉ™ri birlÉ™ÅŸdirÉ™rÉ™k **GPT (Generative Pre-trained Transformer)** modelimizin yekun sinfini yaradacaÄŸÄ±q.

## 17.2. GPT Modelinin ArxitekturasÄ±

GPT modelinin quruluÅŸu aÅŸaÄŸÄ±dakÄ± ardÄ±cÄ±llÄ±qdan ibarÉ™tdir:

1.  **Token Embedding:** GiriÅŸ token ID-lÉ™rini rÉ™qÉ™msal vektorlara (Embedding) Ã§evirir.
2.  **Position Embedding:** TokenlÉ™rin cÃ¼mlÉ™dÉ™ki mÃ¶vqeyini Ã¶yrÉ™nir vÉ™ Token Embedding-É™ É™lavÉ™ edir.
3.  **Transformer BloklarÄ±:** 12 É™dÉ™d `Block` ardÄ±cÄ±l olaraq tÉ™tbiq olunur.
4.  **Final Layer Norm:** BÃ¼tÃ¼n bloklardan sonra yekun normallaÅŸdÄ±rma.
5.  **Linear Head:** NÉ™ticÉ™ni lÃ¼ÄŸÉ™t Ã¶lÃ§Ã¼sÃ¼nÉ™ (32000) Ã§evirir vÉ™ hansÄ± tokenin nÃ¶vbÉ™ti gÉ™lÉ™cÉ™yini proqnozlaÅŸdÄ±rÄ±r.

## 17.3. Praktika: `GPTModel` Sinfinin QurulmasÄ±

**`model.py`**

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
# Block sinfini (GÃ¼n 16-dan) bura kopyalayÄ±n vÉ™ ya import edin

# Modelin É™sas hiperparametrlÉ™ri (GÃ¼n 13-dÉ™n)
n_embd = 768      # Embedding Ã¶lÃ§Ã¼sÃ¼
n_head = 12       # BaÅŸlarÄ±n sayÄ±
n_layer = 12      # BloklarÄ±n sayÄ±
block_size = 256  # Kontekst uzunluÄŸu
vocab_size = 32000 # LÃ¼ÄŸÉ™t Ã¶lÃ§Ã¼sÃ¼

class GPTModel(nn.Module):
    """Æsas GPT Model Sinifi"""
    
    def __init__(self):
        super().__init__()
        
        # 1. Token vÉ™ MÃ¶vqe Embedding-lÉ™ri
        # TokenlÉ™rin rÉ™qÉ™msal tÉ™sviri
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        # TokenlÉ™rin mÃ¶vqeyinin rÉ™qÉ™msal tÉ™sviri
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        
        # 2. ArdÄ±cÄ±l Transformer BloklarÄ±
        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])
        
        # 3. Yekun NormallaÅŸdÄ±rma
        self.ln_f = nn.LayerNorm(n_embd)
        
        # 4. ProqnozlaÅŸdÄ±rma BaÅŸÄ± (Linear Head)
        # NÉ™ticÉ™ni lÃ¼ÄŸÉ™t Ã¶lÃ§Ã¼sÃ¼nÉ™ Ã§evirir
        self.lm_head = nn.Linear(n_embd, vocab_size)
        
        # Modelin Ã§É™kilÉ™rini ilkinlÉ™ÅŸdirmÉ™k
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """Modelin Ã§É™kilÉ™rini daha yaxÅŸÄ± tÉ™lim Ã¼Ã§Ã¼n ilkinlÉ™ÅŸdirmÉ™k."""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape # idx: (Batch, Time)
        
        # 1. Token vÉ™ MÃ¶vqe Embedding-lÉ™ri
        # idx: token ID-lÉ™ri (B, T)
        tok_emb = self.token_embedding_table(idx) # (B, T, C)
        # pos: mÃ¶vqe ID-lÉ™ri (0-dan T-1-É™ qÉ™dÉ™r)
        pos = torch.arange(T, device=idx.device) # (T)
        pos_emb = self.position_embedding_table(pos) # (T, C)
        
        # 2. Embedding-lÉ™ri birlÉ™ÅŸdirmÉ™k
        x = tok_emb + pos_emb # (B, T, C)
        
        # 3. Transformer BloklarÄ±ndan keÃ§irmÉ™k
        x = self.blocks(x) # (B, T, C)
        
        # 4. Yekun NormallaÅŸdÄ±rma
        x = self.ln_f(x) # (B, T, C)
        
        # 5. ProqnozlaÅŸdÄ±rma BaÅŸÄ±
        logits = self.lm_head(x) # (B, T, vocab_size)
        
        loss = None
        if targets is not None:
            # Loss-u hesablamaq Ã¼Ã§Ã¼n Ã¶lÃ§Ã¼lÉ™ri dÃ¼zÉ™ltmÉ™k
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            # Cross-Entropy Loss funksiyasÄ±
            loss = F.cross_entropy(logits, targets)

        return logits, loss

# NÃ¼munÉ™: Modelin yaradÄ±lmasÄ±
model = GPTModel()
print(model)
```

## 17.4. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **31** | `self.token_embedding_table = nn.Embedding(vocab_size, n_embd)` | HÉ™r bir token ID-si Ã¼Ã§Ã¼n 768 Ã¶lÃ§Ã¼lÃ¼ vektor yaradÄ±r. |
| **33** | `self.position_embedding_table = nn.Embedding(block_size, n_embd)` | HÉ™r bir mÃ¶vqe (0-dan 255-É™ qÉ™dÉ™r) Ã¼Ã§Ã¼n 768 Ã¶lÃ§Ã¼lÃ¼ vektor yaradÄ±r. |
| **36** | `self.blocks = nn.Sequential(...)` | 12 É™dÉ™d `Block` sinfini ardÄ±cÄ±l olaraq yÄ±ÄŸÄ±r. |
| **42** | `self.apply(self._init_weights)` | Modelin Ã§É™kilÉ™rini tÉ™limÉ™ baÅŸlamazdan É™vvÉ™l standart normal paylanmaya uyÄŸun olaraq ilkinlÉ™ÅŸdirir. |
| **60** | `x = tok_emb + pos_emb` | Tokenin mÉ™lumatÄ±nÄ± (nÉ™ olduÄŸu) vÉ™ mÃ¶vqe mÉ™lumatÄ±nÄ± (harada olduÄŸu) birlÉ™ÅŸdirir. |
| **74** | `loss = F.cross_entropy(logits, targets)` | **Cross-Entropy Loss** funksiyasÄ± modelin proqnozlarÄ± ilÉ™ hÉ™qiqi nÃ¶vbÉ™ti tokenlÉ™r arasÄ±ndakÄ± fÉ™rqi hesablayÄ±r. Bu, modelin Ã¶yrÉ™nmÉ™sinÉ™ rÉ™hbÉ™rlik edÉ™n É™sas funksiyadÄ±r. |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `model.py` skriptini yaradÄ±n. Modelin quruluÅŸunu vÉ™ `forward` funksiyasÄ±nÄ±n mÉ™lumatÄ± necÉ™ emal etdiyini tam baÅŸa dÃ¼ÅŸÃ¼n.
