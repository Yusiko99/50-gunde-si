# GÃ¼n 14: DiqqÉ™t Mexanizmi (Attention) ğŸ’¡

## 14.1. DiqqÉ™t NÉ™dir?

**DiqqÉ™t Mexanizmi (Attention Mechanism)** Transformer arxitekturasÄ±nÄ±n É™n vacib hissÉ™sidir. Dil modellÉ™ri Ã¼Ã§Ã¼n bu, cÃ¼mlÉ™dÉ™ki bir sÃ¶zÃ¼n mÉ™nasÄ±nÄ± mÃ¼É™yyÉ™nlÉ™ÅŸdirmÉ™k Ã¼Ã§Ã¼n digÉ™r sÃ¶zlÉ™rÉ™ nÉ™ qÉ™dÉ™r É™hÉ™miyyÉ™t vermÉ™li olduÄŸunu Ã¶yrÉ™nmÉ™k demÉ™kdir.

**NÃ¼munÉ™:** "MÉ™n **kitabÄ±** oxudum vÉ™ o, Ã§ox maraqlÄ± idi."
Burada "o" É™vÉ™zliyi "kitabÄ±" sÃ¶zÃ¼nÉ™ aiddir. Model "o" sÃ¶zÃ¼nÃ¼ emal edÉ™rkÉ™n, "kitabÄ±" sÃ¶zÃ¼nÉ™ daha Ã§ox diqqÉ™t yetirmÉ™lidir. DiqqÉ™t mexanizmi mÉ™hz bu É™laqÉ™ni tapÄ±r.

## 14.2. Query, Key, Value (Soru, AÃ§ar, DÉ™yÉ™r)

DiqqÉ™t mexanizmi Ã¼Ã§ É™sas matrisdÉ™n istifadÉ™ edir:

| Matris | Rolu | Ä°zahÄ± |
| :--- | :--- | :--- |
| **Query (Q)** | **Soru** | "MÉ™n nÉ™ axtarÄ±ram?" (MÉ™sÉ™lÉ™n, cari sÃ¶zÃ¼n tÉ™sviri). |
| **Key (K)** | **AÃ§ar** | "MÉ™ndÉ™ nÉ™ var?" (MÉ™sÉ™lÉ™n, bÃ¼tÃ¼n digÉ™r sÃ¶zlÉ™rin tÉ™sviri). |
| **Value (V)** | **DÉ™yÉ™r** | "MÉ™lumat nÉ™dir?" (MÉ™sÉ™lÉ™n, bÃ¼tÃ¼n digÉ™r sÃ¶zlÉ™rin mÉ™lumatÄ±). |

**Ä°ÅŸ Prinsipi:**
1.  **UyÄŸunluq HesablanmasÄ±:** HÉ™r bir **Query** (Q) bÃ¼tÃ¼n **Key**-lÉ™r (K) ilÉ™ mÃ¼qayisÉ™ edilir. Bu, hansÄ± sÃ¶zlÉ™rin cari sÃ¶zlÉ™ É™laqÉ™li olduÄŸunu gÃ¶stÉ™rÉ™n bir **DiqqÉ™t BalÄ± (Attention Score)** yaradÄ±r.
2.  **Softmax:** DiqqÉ™t BalÄ± **Softmax** funksiyasÄ±ndan keÃ§irilÉ™rÉ™k **DiqqÉ™t Ã‡É™kisi (Attention Weight)**-É™ Ã§evrilir. Bu Ã§É™kilÉ™rin cÉ™mi 1-É™ bÉ™rabÉ™r olur.
3.  **DÉ™yÉ™rin Ã‡É™kilmÉ™si:** Bu Ã§É™kilÉ™r **Value** (V) matrisi ilÉ™ vurulur. NÉ™ticÉ™dÉ™, model É™n Ã§ox É™laqÉ™li sÃ¶zlÉ™rin mÉ™lumatÄ±nÄ± Ã¶zÃ¼ndÉ™ cÉ™mlÉ™ÅŸdirÉ™n yeni bir tÉ™svir É™ldÉ™ edir.

## 14.3. Masked Self-Attention (MaskalanmÄ±ÅŸ Ã–z-DiqqÉ™t)

Bizim GPT modelimiz **Generative (YaradÄ±cÄ±)** modeldir. O, hÉ™r dÉ™fÉ™ bir token yaradÄ±r vÉ™ bu tokeni yaradarkÉ™n **yalnÄ±z Ã¶zÃ¼ndÉ™n É™vvÉ™lki** tokenlÉ™rÉ™ baxa bilÉ™r.

*   **Self-Attention:** Model cÃ¼mlÉ™dÉ™ki hÉ™r bir sÃ¶zÃ¼n digÉ™r sÃ¶zlÉ™rÉ™ diqqÉ™t yetirmÉ™sidir.
*   **Masked:** ProqnozlaÅŸdÄ±rma zamanÄ± modelin gÉ™lÉ™cÉ™kdÉ™ki tokenlÉ™rÉ™ "baxmasÄ±nÄ±n" qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n **DiqqÉ™t BalÄ± Matrisinin** yuxarÄ± Ã¼Ã§bucaÄŸÄ± **sÄ±fÄ±r** (vÉ™ ya Ã§ox kiÃ§ik mÉ™nfi É™dÉ™d) ilÉ™ doldurulur.

Bu, modelin tÉ™lim prosesini daha Ã§É™tin, lakin daha realistik edir.

## 14.4. Praktika: PyTorch-da DiqqÉ™t Mexanizmi

GÉ™lin, PyTorch-da sadÉ™ bir DiqqÉ™t Mexanizminin É™sasÄ±nÄ± quraq.

**`attention.py`**

```python
import torch
import torch.nn as nn
from torch.nn import functional as F

# Modelin É™sas hiperparametrlÉ™ri (GÃ¼n 13-dÉ™n)
n_embd = 768  # Embedding Ã¶lÃ§Ã¼sÃ¼
block_size = 256 # Kontekst uzunluÄŸu

class Head(nn.Module):
    """TÉ™k bir diqqÉ™t baÅŸÄ± (Single Attention Head)"""
    
    def __init__(self, head_size):
        super().__init__()
        # Q, K, V matrislÉ™rini yaratmaq Ã¼Ã§Ã¼n xÉ™tti laylar
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        
        # MaskanÄ± yaddaÅŸda saxlamaq (buffer)
        # Bu, modelin gÉ™lÉ™cÉ™yÉ™ baxmasÄ±nÄ±n qarÅŸÄ±sÄ±nÄ± alÄ±r
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        
    def forward(self, x):
        # x-in Ã¶lÃ§Ã¼sÃ¼: (Batch, Time, Channel) -> (B, T, C)
        B, T, C = x.shape
        
        # Q, K, V matrislÉ™rini hesablamaq
        k = self.key(x)   # (B, T, head_size)
        q = self.query(x) # (B, T, head_size)
        
        # 1. DiqqÉ™t BalÄ±nÄ± Hesablamaq (Q @ K.transpose)
        # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)
        wei = q @ k.transpose(-2, -1) * C**-0.5 # Skalalama
        
        # 2. Maskalanma (Masking)
        # GÉ™lÉ™cÉ™k tokenlÉ™rÉ™ diqqÉ™t yetirmÉ™yin qarÅŸÄ±sÄ±nÄ± alÄ±r
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        
        # 3. Softmax
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        
        # 4. DÉ™yÉ™rin Ã‡É™kilmÉ™si (wei @ V)
        v = self.value(x) # (B, T, head_size)
        out = wei @ v     # (B, T, head_size)
        
        return out

# NÃ¼munÉ™: TÉ™k bir diqqÉ™t baÅŸÄ± yaratmaq
head = Head(head_size=n_embd // 12) # 768 / 12 = 64
print(head)
```

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `attention.py` skriptini yaradÄ±n. Kodu oxuyun vÉ™ **`wei = wei.masked_fill(...)`** sÉ™trinin Masked Self-Attention-Ä± necÉ™ tÉ™tbiq etdiyini dÉ™rindÉ™n baÅŸa dÃ¼ÅŸÃ¼n.
