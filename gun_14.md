# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 14

## PyTorch-da Æsas Bloklar: TÉ™mÉ™l Qatlar ğŸ§±

Salam! DÃ¼nÉ™n 100M parametreli NanoGPT modelimizin konfiqurasiyasÄ±nÄ± tÉ™yin etdik. Bu gÃ¼n isÉ™ modelin É™n tÉ™mÉ™lini tÉ™ÅŸkil edÉ™n PyTorch bloklarÄ±nÄ± â€“ **`nn.Module`**, **`Tensor`** vÉ™ **GÃ¶mÃ¼lmÉ™ QatÄ±nÄ± (Embedding Layer)** Ã¶yrÉ™nÉ™cÉ™yik.

### 1. nn.Module vÉ™ Tensor AnlayÄ±ÅŸlarÄ±

DÉ™rin Ã–yrÉ™nmÉ™ modellÉ™ri PyTorch-da **`nn.Module`** sinfi vasitÉ™silÉ™ qurulur.

> **`nn.Module`** â€” PyTorch-da bÃ¼tÃ¼n neyron ÅŸÉ™bÉ™kÉ™ qatlarÄ± vÉ™ modellÉ™ri Ã¼Ã§Ã¼n É™sas sinifdir. HÉ™r bir qat vÉ™ ya model bu sinifdÉ™n miras almalÄ±dÄ±r.

Bu sinif iki É™sas metodu tÉ™lÉ™b edir:
1.  **`__init__`**: Modelin qatlarÄ±nÄ±n vÉ™ parametrlÉ™rinin tÉ™yin olunduÄŸu yer.
2.  **`forward`**: MÉ™lumatÄ±n (Tensor-un) modeldÉ™n necÉ™ keÃ§diyini (irÉ™li Ã¶tÃ¼rmÉ™) tÉ™yin edÉ™n yer.

> **Tensor** â€” PyTorch-da mÉ™lumatlarÄ± saxlamaq Ã¼Ã§Ã¼n istifadÉ™ olunan É™sas riyazi strukturdur. O, NumPy massivlÉ™rinÉ™ bÉ™nzÉ™yir, lakin GPU-da iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n optimallaÅŸdÄ±rÄ±lÄ±b.

### 2. GÃ¶mÃ¼lmÉ™ QatÄ± (Embedding Layer)

Bizim tokenizatorumuz hÉ™r bir sÃ¶zÃ¼ unikal bir rÉ™qÉ™mÉ™ (ID) Ã§evirir. Lakin bu rÉ™qÉ™mlÉ™r (mÉ™sÉ™lÉ™n, 1, 2, 3) model Ã¼Ã§Ã¼n heÃ§ bir mÉ™na daÅŸÄ±mÄ±r. Modelin sÃ¶zlÉ™ri baÅŸa dÃ¼ÅŸmÉ™si Ã¼Ã§Ã¼n onlarÄ± **mÉ™nalÄ± rÉ™qÉ™msal vektorlara** Ã§evirmÉ™liyik. Bu iÅŸi **GÃ¶mÃ¼lmÉ™ QatÄ±** gÃ¶rÃ¼r.

> **GÃ¶mÃ¼lmÉ™ QatÄ± (`nn.Embedding`)** â€” hÉ™r bir token ID-sini modelin Ã¶yrÉ™nÉ™ bilÉ™cÉ™yi sabit Ã¶lÃ§Ã¼lÃ¼ (bizim halÄ±mÄ±zda `n_embd=768`) bir vektora Ã§evirir. Bu vektorlar tÉ™lim zamanÄ± avtomatik olaraq yenilÉ™nir vÉ™ oxÅŸar mÉ™nalÄ± sÃ¶zlÉ™r (mÉ™sÉ™lÉ™n, "kitab" vÉ™ "dÉ™rs") oxÅŸar vektorlara sahib olur.

#### GÃ¶mÃ¼lmÉ™ QatÄ±nÄ±n QurulmasÄ±

Bizim NanoGPT modelimizin ilk qatÄ± **Token GÃ¶mÃ¼lmÉ™si** vÉ™ **MÃ¶vqe GÃ¶mÃ¼lmÉ™si** olacaq.

```python
# gpt_model_base.py (GPT modelinin É™sas sinfi)
import torch
import torch.nn as nn
from config import GPTConfig # DÃ¼nÉ™n yaratdÄ±ÄŸÄ±mÄ±z konfiqurasiya

class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # 1. Token GÃ¶mÃ¼lmÉ™si (Token Embedding)
        # SÃ¶zlÃ¼k hÉ™cmi (vocab_size) x GÃ¶mÃ¼lmÉ™ Ã¶lÃ§Ã¼sÃ¼ (n_embd) matrisi yaradÄ±r
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)

        # 2. MÃ¶vqe GÃ¶mÃ¼lmÉ™si (Positional Embedding)
        # Maksimum ardÄ±cÄ±llÄ±q uzunluÄŸu (block_size) x GÃ¶mÃ¼lmÉ™ Ã¶lÃ§Ã¼sÃ¼ (n_embd) matrisi yaradÄ±r
        self.wpe = nn.Embedding(config.block_size, config.n_embd)

        # 3. Dropout (Overfitting-in qarÅŸÄ±sÄ±nÄ± almaq Ã¼Ã§Ã¼n)
        self.drop = nn.Dropout(config.dropout)

        # 4. Transformer BloklarÄ± (NÃ¶vbÉ™ti gÃ¼nlÉ™rdÉ™ É™lavÉ™ olunacaq)
        # self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])

        # 5. Son NormallaÅŸdÄ±rma vÉ™ XÉ™tti Qat (Head)
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

    def forward(self, idx):
        # idx: Token ID-lÉ™rindÉ™n ibarÉ™t Tensor (B, T)
        B, T = idx.size() # Batch size (B) vÉ™ ArdÄ±cÄ±llÄ±q uzunluÄŸu (T)

        # 1. MÃ¶vqe ID-lÉ™rini yaratmaq
        # 0-dan T-1-É™ qÉ™dÉ™r rÉ™qÉ™mlÉ™r ardÄ±cÄ±llÄ±ÄŸÄ± (mÉ™sÉ™lÉ™n, [0, 1, 2, 3, ...])
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # (T)

        # 2. GÃ¶mÃ¼lmÉ™lÉ™ri Hesablamaq
        # Token ID-lÉ™rini vektorlara Ã§evirir
        token_emb = self.wte(idx) # (B, T, n_embd)
        # MÃ¶vqe ID-lÉ™rini vektorlara Ã§evirir
        pos_emb = self.wpe(pos)   # (T, n_embd)

        # 3. Token vÉ™ MÃ¶vqe GÃ¶mÃ¼lmÉ™lÉ™rini Toplamaq
        x = self.drop(token_emb + pos_emb) # (B, T, n_embd)

        # 4. Transformer BloklarÄ±ndan KeÃ§irmÉ™k (HÉ™lÉ™lik boÅŸdur)
        # for block in self.h:
        #     x = block(x)

        # 5. Son Qatlar
        x = self.ln_f(x)
        logits = self.lm_head(x) # (B, T, vocab_size)

        return logits
```

### 3. Kodun Ä°zahÄ± (HÉ™r SÉ™trin DetallÄ± Ä°zahÄ±)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 12 | `self.wte = nn.Embedding(...)` | **Token GÃ¶mÃ¼lmÉ™si:** `vocab_size` (32000) sayda token Ã¼Ã§Ã¼n `n_embd` (768) Ã¶lÃ§Ã¼lÃ¼ vektorlar yaradÄ±r. |
| 16 | `self.wpe = nn.Embedding(...)` | **MÃ¶vqe GÃ¶mÃ¼lmÉ™si:** `block_size` (512) sayda mÃ¶vqe Ã¼Ã§Ã¼n `n_embd` (768) Ã¶lÃ§Ã¼lÃ¼ vektorlar yaradÄ±r. |
| 24 | `B, T = idx.size()` | GiriÅŸ mÉ™lumatÄ±nÄ±n Ã¶lÃ§Ã¼lÉ™rini (Batch size vÉ™ ArdÄ±cÄ±llÄ±q uzunluÄŸu) alÄ±r. |
| 28 | `pos = torch.arange(0, T, ...)` | 0-dan T-1-É™ qÉ™dÉ™r mÃ¶vqe indekslÉ™rini yaradÄ±r. |
| 31 | `token_emb = self.wte(idx)` | Token ID-lÉ™rini (idx) mÉ™nalÄ± vektorlara Ã§evirir. |
| 33 | `pos_emb = self.wpe(pos)` | MÃ¶vqe indekslÉ™rini mÉ™nalÄ± vektorlara Ã§evirir. |
| 36 | `x = self.drop(token_emb + pos_emb)` | **Token vÉ™ MÃ¶vqe GÃ¶mÃ¼lmÉ™lÉ™rini toplayÄ±rÄ±q.** Bu, modelÉ™ hÉ™m sÃ¶zÃ¼n mÉ™nasÄ±nÄ±, hÉ™m dÉ™ cÃ¼mlÉ™dÉ™ki yerini eyni anda verir. |
| 43 | `self.lm_head = nn.Linear(...)` | **Dil Modeli BaÅŸÄ± (LM Head):** 768 Ã¶lÃ§Ã¼lÃ¼ vektoru yenidÉ™n 32000 Ã¶lÃ§Ã¼lÃ¼ vektorlara Ã§evirir. Bu 32000 rÉ™qÉ™m, nÃ¶vbÉ™ti tokenin hansÄ± token ID-si olmasÄ±nÄ±n ehtimalÄ±nÄ± gÃ¶stÉ™rir. |

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  **`config.py`** faylÄ±nÄ±n mÃ¶vcud olduÄŸundan É™min olun.
2.  **`gpt_model_base.py`** faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± kodu ora kopyalayÄ±n.
3.  KiÃ§ik bir sÄ±naq skripti yazÄ±n:
    ```python
    # SÄ±naq skripti
    from config import GPTConfig
    from gpt_model_base import GPT
    
    config = GPTConfig()
    model = GPT(config)
    
    # SÄ±naq giriÅŸi: 4 cÃ¼mlÉ™ (batch), hÉ™r biri 10 token uzunluÄŸunda
    dummy_input = torch.randint(0, config.vocab_size, (4, 10))
    
    output = model(dummy_input)
    print(f"Ã‡Ä±xÄ±ÅŸ Tensorunun Ã–lÃ§Ã¼sÃ¼: {output.shape}")
    # NÉ™ticÉ™ (4, 10, 32000) olmalÄ±dÄ±r.
    ```

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **Ã‡oxbaÅŸlÄ± DiqqÉ™t (Multi-Head Attention)** mexanizmini PyTorch-da sÄ±fÄ±rdan quracaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
