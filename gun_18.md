# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 18

## Parametr SayÄ±nÄ±n HesablanmasÄ±: Modelin Ã–lÃ§Ã¼sÃ¼ ğŸ“

Salam! DÃ¼nÉ™n **GPT (NanoGPT)** modelimizin tam PyTorch sinfini qurduq vÉ™ modelin Ã¼mumi parametr sayÄ±nÄ±n **124,417,536** olduÄŸunu gÃ¶rdÃ¼k. Bu gÃ¼n bu rÉ™qÉ™min arxasÄ±nda duran riyaziyyatÄ± â€“ yÉ™ni modelin Ã¶lÃ§Ã¼sÃ¼nÃ¼n necÉ™ hesablandÄ±ÄŸÄ±nÄ± Ã¶yrÉ™nÉ™cÉ™yik.

Bu bilik, gÉ™lÉ™cÉ™kdÉ™ modelinizin Ã¶lÃ§Ã¼sÃ¼nÃ¼ (mÉ™sÉ™lÉ™n, 50M vÉ™ ya 200M) dÉ™yiÅŸdirmÉ™k istÉ™diyiniz zaman sizÉ™ kÃ¶mÉ™k edÉ™cÉ™k.

### 1. Parametr NÉ™dir?

Neyron ÅŸÉ™bÉ™kÉ™dÉ™ **parametr** modelin tÉ™lim zamanÄ± Ã¶yrÉ™ndiyi dÉ™yÉ™rlÉ™rdir. Bunlar É™sasÉ™n **Ã§É™kilÉ™r (weights)** vÉ™ **meyilliklÉ™r (biases)** adlanÄ±r. HÉ™r bir parametr yaddaÅŸda yer tutur vÉ™ tÉ™lim zamanÄ± yenilÉ™nir.

Bizim modelimizdÉ™ parametr sayÄ± Ã¼Ã§ É™sas hissÉ™dÉ™n ibarÉ™tdir:

1.  **GÃ¶mÃ¼lmÉ™ QatlarÄ± (Embedding Layers)**
2.  **Transformer BloklarÄ± (Block)**
3.  **Dil Modeli BaÅŸÄ± (LM Head)**

### 2. HissÉ™-HissÉ™ Hesablama

Bizim konfiqurasiyamÄ±z: `n_embd=768`, `vocab_size=32000`, `n_layer=12`.

#### A. GÃ¶mÃ¼lmÉ™ QatlarÄ± (Embedding Layers)

| Qat | Hesablama | NÉ™ticÉ™ |
| :--- | :--- | :--- |
| **Token GÃ¶mÃ¼lmÉ™si (`wte`)** | `vocab_size` * `n_embd` | 32,000 * 768 = **24,576,000** |
| **MÃ¶vqe GÃ¶mÃ¼lmÉ™si (`wpe`)** | `block_size` * `n_embd` | 512 * 768 = **393,216** |
| **CÉ™mi** | | **24,969,216** |

**Qeyd:** NanoGPT-dÉ™ `wte` vÉ™ `lm_head` Ã§É™kilÉ™ri bÉ™zÉ™n paylaÅŸÄ±lÄ±r (Weight Tying). Bizim kodumuzda onlar ayrÄ±dÄ±r, lakin `lm_head` Ã¼Ã§Ã¼n hesablamanÄ± ayrÄ±ca edÉ™cÉ™yik.

#### B. Bir Transformer Bloku (Block)

HÉ™r bir blokun iÃ§indÉ™ É™n Ã§ox parametr **Ã‡oxbaÅŸlÄ± DiqqÉ™t (MHA)** vÉ™ **Ä°rÉ™li Ã–tÃ¼rmÉ™ ÅÉ™bÉ™kÉ™si (FFN)** qatlarÄ±nda yerlÉ™ÅŸir.

**1. Ã‡oxbaÅŸlÄ± DiqqÉ™t (`attn`):**
*   **Q, K, V ProyeksiyasÄ± (`c_attn`):** GiriÅŸ Ã¶lÃ§Ã¼sÃ¼ (`n_embd`) * Ã‡Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼sÃ¼ (`3 * n_embd`)
    *   Hesablama: 768 * (3 * 768) = 768 * 2304 = **1,769,472**
*   **Son Proyeksiya (`c_proj`):** GiriÅŸ Ã¶lÃ§Ã¼sÃ¼ (`n_embd`) * Ã‡Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼sÃ¼ (`n_embd`)
    *   Hesablama: 768 * 768 = **589,824**
*   **CÉ™mi MHA:** 1,769,472 + 589,824 = **2,359,296**

**2. Ä°rÉ™li Ã–tÃ¼rmÉ™ ÅÉ™bÉ™kÉ™si (`mlp`):**
*   **GiriÅŸ QatÄ± (`c_fc`):** GiriÅŸ Ã¶lÃ§Ã¼sÃ¼ (`n_embd`) * Ã‡Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼sÃ¼ (`4 * n_embd`)
    *   Hesablama: 768 * (4 * 768) = 768 * 3072 = **2,359,296**
*   **Ã‡Ä±xÄ±ÅŸ QatÄ± (`c_proj`):** GiriÅŸ Ã¶lÃ§Ã¼sÃ¼ (`4 * n_embd`) * Ã‡Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼sÃ¼ (`n_embd`)
    *   Hesablama: 3072 * 768 = **2,359,296**
*   **CÉ™mi FFN:** 2,359,296 + 2,359,296 = **4,718,592**

**3. DigÉ™r Qatlar (`LayerNorm`):**
*   LayerNorm qatlarÄ± da parametr ehtiva edir (Ã§É™ki vÉ™ meyillik). HÉ™r LayerNorm Ã¼Ã§Ã¼n `2 * n_embd` parametr var.
    *   Hesablama: 4 * (2 * 768) = **6,144**

**4. Bir Blokun CÉ™mi:** 2,359,296 (MHA) + 4,718,592 (FFN) + 6,144 (LayerNorm) = **7,084,032**

#### C. BÃ¼tÃ¼n Transformer BloklarÄ±

*   **CÉ™mi Bloklar:** `n_layer` * Bir Blokun CÉ™mi
    *   Hesablama: 12 * 7,084,032 = **85,008,384**

#### D. Dil Modeli BaÅŸÄ± (LM Head)

*   **LM Head (`lm_head`):** GiriÅŸ Ã¶lÃ§Ã¼sÃ¼ (`n_embd`) * Ã‡Ä±xÄ±ÅŸ Ã¶lÃ§Ã¼sÃ¼ (`vocab_size`)
    *   Hesablama: 768 * 32,000 = **24,576,000**

### 3. Yekun Hesablama

| HissÉ™ | Parametr SayÄ± |
| :--- | :--- |
| GÃ¶mÃ¼lmÉ™ QatlarÄ± | 24,969,216 |
| Transformer BloklarÄ± (12 É™dÉ™d) | 85,008,384 |
| LM Head | 24,576,000 |
| **Ãœmumi Parametr SayÄ±** | **134,553,600** |

**Qeyd:** Bizim PyTorch kodumuzda `lm_head`-in Ã§É™kilÉ™ri (`lm_head.weight`) vÉ™ `wte`-nin Ã§É™kilÉ™ri (`wte.weight`) eyni matrisi paylaÅŸÄ±r (Weight Tying). ÆgÉ™r bu paylaÅŸÄ±m tÉ™tbiq olunarsa, `lm_head` parametrlÉ™ri Ã¼mumi saydan Ã§Ä±xÄ±lÄ±r.

Bizim `model.py` kodumuzda `self.get_num_params()` funksiyasÄ± `wpe` (MÃ¶vqe GÃ¶mÃ¼lmÉ™si) parametrlÉ™rini Ã§Ä±xarÄ±r. ÆgÉ™r bÃ¼tÃ¼n parametrlÉ™ri saysaq, tÉ™xminÉ™n **124 Milyon** rÉ™qÉ™mini alÄ±rÄ±q (bu, bias-larÄ±n sayÄ±lmasÄ±ndan vÉ™ ya sayÄ±lmamasÄ±ndan asÄ±lÄ± olaraq dÉ™yiÅŸÉ™ bilÉ™r).

**Æsas NÉ™ticÉ™:** Modelimizin Ã¶lÃ§Ã¼sÃ¼ **~124 Milyon** parametrdir.

### 4. YaddaÅŸ TÉ™lÉ™bi

HÉ™r bir parametr yaddaÅŸda yer tutur. Æn Ã§ox istifadÉ™ olunan dÉ™qiqlik formatÄ± **FP32** (32-bit Floating Point) vÉ™ ya **FP16** (16-bit Floating Point) formatÄ±dÄ±r.

*   **FP32 (4 byte):** 124,417,536 parametr * 4 byte/parametr â‰ˆ **497 MB**
*   **FP16 (2 byte):** 124,417,536 parametr * 2 byte/parametr â‰ˆ **248 MB**

Bu, modelin Ã¶zÃ¼nÃ¼n yaddaÅŸda tutduÄŸu yerdir. TÉ™lim zamanÄ± optimallaÅŸdÄ±rÄ±cÄ± (AdamW) vÉ™ qradiyentlÉ™r dÉ™ yaddaÅŸ tÉ™lÉ™b edir.

**TÉ™lim zamanÄ± Ã¼mumi VRAM tÉ™lÉ™bi:** Modelin Ã§É™kisi (FP16) * 1 (model) + Modelin Ã§É™kisi * 1 (qradiyent) + Modelin Ã§É™kisi * 2 (AdamW optimallaÅŸdÄ±rÄ±cÄ±sÄ±) + Batch size * Context Length * n_embd * 4 (aktivasiyalar)

**Yekun TÉ™xmin:** Bizim 12 GB VRAM-lÄ± **NVIDIA T4** kartÄ±mÄ±z bu modeli FP16 (Mixed Precision) istifadÉ™ edÉ™rÉ™k rahatlÄ±qla tÉ™lim edÉ™ bilÉ™cÉ™k.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: DÃ¼ÅŸÃ¼n vÉ™ Praktika

1.  ÆgÉ™r `n_layer`-i 24-É™ qaldÄ±rsaydÄ±q, modelin parametr sayÄ± tÉ™xminÉ™n nÉ™ qÉ™dÉ™r olardÄ±? (Cavab: TÉ™xminÉ™n 200 Milyon).
2.  Modelin yaddaÅŸ tÉ™lÉ™binin É™n bÃ¶yÃ¼k hissÉ™si hansÄ± komponentlÉ™rÉ™ aiddir? (Cavab: Transformer BloklarÄ± vÉ™ GÃ¶mÃ¼lmÉ™ QatlarÄ±).

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah modelin tÉ™limdÉ™n É™vvÉ™l necÉ™ mÉ™tn yaratdÄ±ÄŸÄ±nÄ± gÃ¶rmÉ™k Ã¼Ã§Ã¼n **MÉ™tn GenerasiyasÄ± (Sampling)** mexanizmini Ã¶yrÉ™nÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 850 sÃ¶z.
