# GÃ¼n 18: Parametr SayÄ±nÄ±n HesablanmasÄ± ğŸ”¢

## 18.1. Parametr NÉ™dir?

**ParametrlÉ™r** modelin tÉ™lim zamanÄ± Ã¶yrÉ™ndiyi dÉ™yiÅŸÉ™nlÉ™rdir. Bu, modelin yaddaÅŸÄ± vÉ™ biliyidir. Modelin nÉ™ qÉ™dÉ™r gÃ¼clÃ¼ olduÄŸunu gÃ¶stÉ™rÉ™n É™sas gÃ¶stÉ™ricilÉ™rdÉ™n biridir. Bizim hÉ™dÉ™fimiz **100 Milyon (100M)** parametrdir.

ParametrlÉ™r É™sasÉ™n **XÉ™tti Laylarda (Linear Layers)** vÉ™ **Embedding CÉ™dvÉ™llÉ™rindÉ™ (Embedding Tables)** yerlÉ™ÅŸir.

## 18.2. ParametrlÉ™rin HesablanmasÄ±

GÉ™lin, GÃ¼n 17-dÉ™ qurduÄŸumuz `GPTModel` sinfinin parametr sayÄ±nÄ± hesablayaq.

**Modelin Æsas HiperparametrlÉ™ri:**
*   `vocab_size` (V): 32000
*   `n_embd` (C): 768
*   `n_layer` (L): 12
*   `n_head` (H): 12

### A. Embedding LaylarÄ±

1.  **Token Embedding (`token_embedding_table`):**
    *   HÉ™r bir token Ã¼Ã§Ã¼n `C` Ã¶lÃ§Ã¼lÃ¼ vektor.
    *   Parametr SayÄ±: $V \times C = 32000 \times 768 = 24,576,000$

2.  **Position Embedding (`position_embedding_table`):**
    *   `block_size` (256) mÃ¶vqe Ã¼Ã§Ã¼n `C` Ã¶lÃ§Ã¼lÃ¼ vektor.
    *   Parametr SayÄ±: $256 \times 768 = 196,608$

### B. Transformer BloklarÄ± (12 É™dÉ™d)

HÉ™r bir `Block` (Blok) aÅŸaÄŸÄ±dakÄ±lardan ibarÉ™tdir:

1.  **Multi-Head Attention (MHA):**
    *   **Q, K, V LaylarÄ±:** HÉ™r biri $C \times C$ Ã¶lÃ§Ã¼dÉ™dir. $3 \times (C \times C)$
    *   **Proj LayÄ±:** $C \times C$ Ã¶lÃ§Ã¼dÉ™dir.
    *   **MHA-da CÉ™mi:** $4 \times (C \times C) = 4 \times (768 \times 768) = 2,359,296$

2.  **Feed-Forward Network (FFN):**
    *   **Lay 1:** $C \times (4C)$ Ã¶lÃ§Ã¼dÉ™dir.
    *   **Lay 2:** $(4C) \times C$ Ã¶lÃ§Ã¼dÉ™dir.
    *   **FFN-dÉ™ CÉ™mi:** $2 \times (C \times 4C) = 8 \times C^2 = 8 \times (768 \times 768) = 4,718,592$

3.  **Layer Norm LaylarÄ±:** ParametrlÉ™ri Ã§ox azdÄ±r (tÉ™xminÉ™n $2 \times C$ hÉ™r lay Ã¼Ã§Ã¼n). Ãœmumi hesablamada nÉ™zÉ™rÉ™ alÄ±nmÄ±r.

*   **Bir Blokda CÉ™mi:** $2,359,296 + 4,718,592 = 7,077,888$
*   **12 Blokda CÉ™mi:** $12 \times 7,077,888 = 84,934,656$

### C. Yekun ProqnozlaÅŸdÄ±rma BaÅŸÄ±

1.  **Linear Head (`lm_head`):**
    *   Parametr SayÄ±: $C \times V = 768 \times 32000 = 24,576,000$

### D. Ãœmumi Parametr SayÄ±

| HissÉ™ | Parametr SayÄ± |
| :--- | :--- |
| Token Embedding | 24,576,000 |
| Position Embedding | 196,608 |
| 12 Transformer Bloku | 84,934,656 |
| Linear Head | 24,576,000 |
| **Ãœmumi CÉ™mi** | **134,283,264** |

**NÉ™ticÉ™:** Bizim modelimiz tÉ™xminÉ™n **134 Milyon** parametrÉ™ malikdir. Bu, sizin hÉ™dÉ™flÉ™diyiniz **100M** parametrÉ™ Ã§ox yaxÄ±ndÄ±r vÉ™ bu Ã¶lÃ§Ã¼ ilÉ™ **RTX 2050 (4GB VRAM)** Ã¼zÉ™rindÉ™ tÉ™lim etmÉ™k mÃ¼mkÃ¼ndÃ¼r.

## 18.3. Praktika: PyTorch ilÉ™ Hesablama

PyTorch-da parametr sayÄ±nÄ± avtomatik hesablamaq Ã¼Ã§Ã¼n funksiya yazaq.

**`count_params.py`**

```python
import torch
# GPTModel sinfini (GÃ¼n 17-dÉ™n) bura kopyalayÄ±n vÉ™ ya import edin

def count_parameters(model):
    """Modelin Ã¼mumi parametr sayÄ±nÄ± hesablayÄ±r."""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return total_params, trainable_params

# Modelin yaradÄ±lmasÄ±
model = GPTModel()

total, trainable = count_parameters(model)

print(f"Ãœmumi Parametr SayÄ±: {total:,}")
print(f"TÉ™lim EdilÉ™ BilÉ™n Parametr SayÄ±: {trainable:,}")
print(f"Model Ã–lÃ§Ã¼sÃ¼ (Milyon): {total / 1_000_000:.2f} M")
```

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `count_params.py` skriptini iÅŸÉ™ salÄ±n vÉ™ hesablamalarÄ±mÄ±zÄ±n doÄŸruluÄŸunu yoxlayÄ±n. Bu, modelin Ã¶lÃ§Ã¼sÃ¼nÃ¼ vÉ™ VRAM tÉ™lÉ™bini anlamaq Ã¼Ã§Ã¼n vacibdir.
