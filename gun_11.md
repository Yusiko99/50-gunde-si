# GÃ¼n 11: Tokenizasiya II: Tokenizatorun QurulmasÄ± (Praktika) ğŸ› ï¸

## 11.1. Tokenizatorun TÉ™limi

DÃ¼nÉ™n BPE (Byte Pair Encoding) nÉ™zÉ™riyyÉ™sini Ã¶yrÉ™ndik. Bu gÃ¼n isÉ™ **`tokenizers`** kitabxanasÄ±ndan istifadÉ™ edÉ™rÉ™k, **`normalized_corpus.txt`** faylÄ±ndakÄ± mÉ™lumatlar É™sasÄ±nda Ã¶z AzÉ™rbaycan dili tokenizatorumuzu tÉ™lim edÉ™cÉ™yik.

Bu tokenizator bizim LLM-in dilini baÅŸa dÃ¼ÅŸmÉ™si Ã¼Ã§Ã¼n É™sas vasitÉ™ olacaq.

**`train_tokenizer.py`**

```python
from tokenizers import Tokenizer, models, pre_tokenizers, trainers

# 1. GiriÅŸ vÉ™ Ã‡Ä±xÄ±ÅŸ FayllarÄ±
CORPUS_FILE = "normalized_corpus.txt"
VOCAB_SIZE = 32000 # LÃ¼ÄŸÉ™tin hÉ™dÉ™f Ã¶lÃ§Ã¼sÃ¼
OUTPUT_PREFIX = "az_llm"

def train_bpe_tokenizer():
    """BPE tokenizatorunu tÉ™lim edir."""
    
    # 2. Tokenizatorun Modeli: BPE
    tokenizer = Tokenizer(models.BPE())
    
    # 3. MÉ™tnin ilkin emalÄ± (Pre-tokenizer)
    # MÉ™tni sÃ¶zlÉ™rÉ™ bÃ¶lmÉ™k Ã¼Ã§Ã¼n sadÉ™ boÅŸluq É™saslÄ± pre-tokenizer istifadÉ™ edirik.
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
    
    # 4. TÉ™limÃ§i (Trainer)
    trainer = trainers.BpeTrainer(
        vocab_size=VOCAB_SIZE,
        special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"],
        # ÆsasÉ™n ingilis dilindÉ™ istifadÉ™ olunur, lakin biz dÉ™ É™lavÉ™ edirik.
        # [UNK] - NamÉ™lum token, [PAD] - Doldurma tokeni
        min_frequency=2 # Æn azÄ± 2 dÉ™fÉ™ rast gÉ™linÉ™n cÃ¼tlÃ¼klÉ™ri lÃ¼ÄŸÉ™tÉ™ É™lavÉ™ et
    )
    
    # 5. TÉ™lim prosesi
    print(f"Tokenizator '{CORPUS_FILE}' Ã¼zÉ™rindÉ™ tÉ™lim edilir...")
    tokenizer.train([CORPUS_FILE], trainer=trainer)
    print("TÉ™lim tamamlandÄ±.")
    
    # 6. Tokenizatoru yadda saxlamaq
    tokenizer.save(f"{OUTPUT_PREFIX}-tokenizer.json")
    print(f"Tokenizator '{OUTPUT_PREFIX}-tokenizer.json' faylÄ±na yazÄ±ldÄ±.")
    
    # 7. NÃ¼munÉ™ sÄ±naq
    test_sentence = "sÃ¼ni intellekt modelinin kvantlaÅŸdÄ±rÄ±lmasÄ± prosesi uÄŸurla baÅŸa Ã§atdÄ±"
    encoding = tokenizer.encode(test_sentence)
    
    print("\n--- NÃ¼munÉ™ SÄ±naq ---")
    print(f"Orijinal: {test_sentence}")
    print(f"TokenlÉ™r: {encoding.tokens}")
    print(f"ID-lÉ™r: {encoding.ids}")
    print(f"LÃ¼ÄŸÉ™t Ã–lÃ§Ã¼sÃ¼: {tokenizer.get_vocab_size()}")

if __name__ == "__main__":
    train_bpe_tokenizer()
```

## 11.2. Kodun Ä°zahÄ±

| SÉ™tr | Kod | Ä°zahÄ± |
| :--- | :--- | :--- |
| **10** | `tokenizer = Tokenizer(models.BPE())` | Yeni bir BPE (Byte Pair Encoding) modeli yaradÄ±rÄ±q. |
| **14** | `tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()` | **Pre-tokenizer** tokenizasiyadan É™vvÉ™l mÉ™tni ilkin olaraq bÃ¶lÃ¼r. `Whitespace` (BoÅŸluq) É™sasÄ±nda bÃ¶lmÉ™, sÃ¶zlÉ™ri boÅŸluqlara gÃ¶rÉ™ ayÄ±rÄ±r. |
| **17** | `trainer = trainers.BpeTrainer(...)` | BPE tÉ™limÃ§isini yaradÄ±rÄ±q. |
| **18** | `vocab_size=VOCAB_SIZE` | LÃ¼ÄŸÉ™tin maksimum Ã¶lÃ§Ã¼sÃ¼nÃ¼ 32000 olaraq tÉ™yin edirik. |
| **19** | `special_tokens=["[UNK]", ...]` | Modelin xÃ¼susi mÉ™qsÉ™dlÉ™r Ã¼Ã§Ã¼n istifadÉ™ edÉ™cÉ™yi tokenlÉ™r. MÉ™sÉ™lÉ™n, **`[UNK]`** (Unknown) lÃ¼ÄŸÉ™tdÉ™ olmayan sÃ¶zlÉ™r Ã¼Ã§Ã¼n istifadÉ™ olunacaq. |
| **26** | `tokenizer.train([CORPUS_FILE], trainer=trainer)` | Tokenizatoru `normalized_corpus.txt` faylÄ± Ã¼zÉ™rindÉ™ tÉ™lim edir. |
| **30** | `tokenizer.save(...)` | TÉ™lim edilmiÅŸ tokenizatoru JSON formatÄ±nda yadda saxlayÄ±r. Bu fayl modelimizlÉ™ birlikdÉ™ istifadÉ™ olunacaq. |
| **34** | `encoding = tokenizer.encode(test_sentence)` | Tokenizatorun necÉ™ iÅŸlÉ™diyini yoxlamaq Ã¼Ã§Ã¼n nÃ¼munÉ™ cÃ¼mlÉ™ni rÉ™qÉ™mlÉ™rÉ™ Ã§evirir. |

**GÃ¼ndÉ™lik TapÅŸÄ±rÄ±q:** `train_tokenizer.py` skriptini yaradÄ±n vÉ™ iÅŸÉ™ salÄ±n. NÉ™ticÉ™dÉ™ **`az_llm-tokenizer.json`** faylÄ± yaranmalÄ±dÄ±r. NÃ¼munÉ™ sÄ±naÄŸÄ±n nÉ™ticÉ™lÉ™rini diqqÉ™tlÉ™ yoxlayÄ±n. AzÉ™rbaycan dilindÉ™ki uzun sÃ¶zlÉ™rin (mÉ™sÉ™lÉ™n, "kvantlaÅŸdÄ±rÄ±lmasÄ±") necÉ™ hissÉ™lÉ™rÉ™ bÃ¶lÃ¼ndÃ¼yÃ¼nÃ¼ mÃ¼ÅŸahidÉ™ edin.
