# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 11

## Transformer: LLM-lÉ™rin Beyni ğŸ§ 

Salam! Ä°lk 10 gÃ¼nÃ¼ uÄŸurla tamamladÄ±q vÉ™ modelimizin qidasÄ±nÄ± â€“ **rÉ™qÉ™mlÉ™ÅŸdirilmiÅŸ AzÉ™rbaycan dili mÉ™lumatÄ±nÄ±** hazÄ±rladÄ±q. Ä°ndi isÉ™ bu mÉ™lumatÄ± emal edÉ™cÉ™k **beyni** â€“ yÉ™ni **Transformer** arxitekturasÄ±nÄ± qurmaÄŸa baÅŸlayÄ±rÄ±q.

### 1. Transformer NÉ™dir?

2017-ci ildÉ™ Google tÉ™rÉ™findÉ™n nÉ™ÅŸr olunan **"Attention Is All You Need"** adlÄ± mÉ™qalÉ™ SÃ¼ni Ä°ntellekt dÃ¼nyasÄ±nda inqilab etdi. Bu mÉ™qalÉ™ **Transformer** adlÄ± yeni bir neyron ÅŸÉ™bÉ™kÉ™ arxitekturasÄ±nÄ± tÉ™qdim etdi.

> **Transformer** â€” ardÄ±cÄ±l mÉ™lumatlarÄ± (mÉ™sÉ™lÉ™n, mÉ™tn) emal etmÉ™k Ã¼Ã§Ã¼n nÉ™zÉ™rdÉ™ tutulmuÅŸ, **tÉ™krarlanan (recurrent)** vÉ™ ya **konvolyusiya (convolutional)** É™mÉ™liyyatlarÄ± É™vÉ™zinÉ™ yalnÄ±z **DiqqÉ™t MexanizminÉ™ (Attention Mechanism)** É™saslanan bir neyron ÅŸÉ™bÉ™kÉ™ arxitekturasÄ±dÄ±r.

Transformer-dÉ™n É™vvÉ™lki modellÉ™r (RNN, LSTM) mÉ™tnlÉ™ri sÃ¶zbÉ™sÃ¶z, ardÄ±cÄ±l ÅŸÉ™kildÉ™ oxuyurdu. Bu, Ã§ox yavaÅŸ idi vÉ™ uzun cÃ¼mlÉ™lÉ™rdÉ™ É™vvÉ™ldÉ™ki sÃ¶zlÉ™rin mÉ™nasÄ±nÄ± unutmaÄŸa sÉ™bÉ™b olurdu.

**Transformer** isÉ™ bÃ¼tÃ¼n cÃ¼mlÉ™ni **bir anda** emal edÉ™ bilir. Bu, LLM-lÉ™rin sÃ¼rÉ™tini vÉ™ mÉ™tnin mÉ™nasÄ±nÄ± baÅŸa dÃ¼ÅŸmÉ™ qabiliyyÉ™tini kÉ™skin ÅŸÉ™kildÉ™ artÄ±rdÄ±.

### 2. Encoder vÉ™ Decoder: Transformer-in Ä°ki HissÉ™si

ÆslindÉ™, Transformer arxitekturasÄ± iki É™sas hissÉ™dÉ™n ibarÉ™tdir:

| HissÉ™ | MÉ™qsÉ™d | Misal |
| :--- | :--- | :--- |
| **Encoder (KodlayÄ±cÄ±)** | GiriÅŸ mÉ™tnini (input) baÅŸa dÃ¼ÅŸmÉ™k vÉ™ onun mÉ™nasÄ±nÄ± rÉ™qÉ™msal tÉ™msilÉ™ Ã§evirmÉ™k. | TÉ™rcÃ¼mÉ™dÉ™: "Salam" sÃ¶zÃ¼nÃ¼n mÉ™nasÄ±nÄ± baÅŸa dÃ¼ÅŸÃ¼r. |
| **Decoder (DekodlayÄ±cÄ±)** | Encoder-in baÅŸa dÃ¼ÅŸdÃ¼yÃ¼ mÉ™nanÄ± istÉ™nilÉ™n Ã§Ä±xÄ±ÅŸ mÉ™tninÉ™ (output) Ã§evirmÉ™k. | TÉ™rcÃ¼mÉ™dÉ™: "Salam"Ä±n mÉ™nasÄ±nÄ± ingiliscÉ™ "Hello" sÃ¶zÃ¼nÉ™ Ã§evirir. |

*   **TÉ™rcÃ¼mÉ™ ModellÉ™ri (Seq2Seq):** HÉ™m **Encoder**, hÉ™m dÉ™ **Decoder** istifadÉ™ edir (mÉ™sÉ™lÉ™n, T5, BART).
*   **Chatbot ModellÉ™ri (GPT):** Bizim yaratdÄ±ÄŸÄ±mÄ±z kimi **Generativ** (yeni mÉ™tn yaradan) modellÉ™r yalnÄ±z **Decoder** hissÉ™sindÉ™n istifadÉ™ edir.

#### NiyÉ™ YalnÄ±z Decoder?

Bizim LLM-imiz (NanoGPT) **Generativ Pre-trained Transformer (GPT)** ailÉ™sinÉ™ aiddir. Bu modellÉ™r **nÃ¶vbÉ™ti tokeni proqnozlaÅŸdÄ±rmaq** Ã¼Ã§Ã¼n tÉ™lim olunur.

Decoder hissÉ™si mÉ™hz bu iÅŸ Ã¼Ã§Ã¼n idealdÄ±r, Ã§Ã¼nki o, **MaskalanmÄ±ÅŸ DiqqÉ™t (Masked Attention)** mexanizminÉ™ malikdir. Bu mexanizm modelin **yalnÄ±z Ã¶zÃ¼ndÉ™n É™vvÉ™lki** sÃ¶zlÉ™rÉ™ baxaraq nÃ¶vbÉ™ti sÃ¶zÃ¼ proqnozlaÅŸdÄ±rmasÄ±na imkan verir.

### 3. Transformer Blokunun Æsas KomponentlÉ™ri

Transformer-in hÉ™r bir qatÄ± (layer) **Transformer Bloku** adlanÄ±r. Bu blokun iÃ§indÉ™ dÃ¶rd É™sas komponent var:

1.  **Multi-Head Attention (Ã‡oxbaÅŸlÄ± DiqqÉ™t):** MÉ™tnin fÉ™rqli hissÉ™lÉ™ri arasÄ±ndakÄ± É™laqÉ™lÉ™ri eyni anda Ã¶yrÉ™nir. (Sabah daha É™traflÄ± Ã¶yrÉ™nÉ™cÉ™yik).
2.  **Add & Norm (ÆlavÉ™ et vÉ™ NormallaÅŸdÄ±r):** DiqqÉ™t mexanizminin Ã§Ä±xÄ±ÅŸÄ±nÄ± giriÅŸÉ™ É™lavÉ™ edir (Residual Connection) vÉ™ sonra normallaÅŸdÄ±rÄ±r (Layer Normalization).
3.  **Feed-Forward Network (Ä°rÉ™li Ã–tÃ¼rmÉ™ ÅÉ™bÉ™kÉ™si):** HÉ™r bir tokeni fÉ™rdi ÅŸÉ™kildÉ™ emal edÉ™n kiÃ§ik bir neyron ÅŸÉ™bÉ™kÉ™sidir.
4.  **Positional Encoding (MÃ¶vqe KodlaÅŸdÄ±rmasÄ±):** Transformer-in ardÄ±cÄ±llÄ±q mÉ™lumatÄ±nÄ± (sÃ¶zlÉ™rin sÄ±rasÄ±nÄ±) itirmÉ™mÉ™si Ã¼Ã§Ã¼n hÉ™r bir tokenÉ™ onun cÃ¼mlÉ™dÉ™ki mÃ¶vqeyini bildirÉ™n rÉ™qÉ™msal mÉ™lumat É™lavÉ™ edir.

### 4. PyTorch-da Transformer-É™ Ä°lk BaxÄ±ÅŸ

PyTorch-da bu komponentlÉ™ri necÉ™ quracaÄŸÄ±mÄ±zÄ± Ã¶yrÉ™nÉ™cÉ™yik. MÉ™sÉ™lÉ™n, **Transformer Blokunun** PyTorch-da sadÉ™lÉ™ÅŸdirilmiÅŸ gÃ¶rÃ¼nÃ¼ÅŸÃ¼ belÉ™dir:

```python
import torch.nn as nn

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        # 1. Ã‡oxbaÅŸlÄ± DiqqÉ™t
        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)
        # 2. Ä°rÉ™li Ã–tÃ¼rmÉ™ ÅÉ™bÉ™kÉ™si
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(), # Aktivasiya funksiyasÄ±
            nn.Linear(4 * d_model, d_model)
        )
        # 3. NormallaÅŸdÄ±rma
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # DiqqÉ™t vÉ™ Residual Connection
        x = x + self.attn(self.norm1(x))
        # Ä°rÉ™li Ã–tÃ¼rmÉ™ vÉ™ Residual Connection
        x = x + self.ffn(self.norm2(x))
        return x
```

**Kodun Ä°zahÄ±:**
*   `import torch.nn as nn`: PyTorch-un neyron ÅŸÉ™bÉ™kÉ™ modullarÄ±nÄ± daxil edirik.
*   `class TransformerBlock(nn.Module)`: BÃ¼tÃ¼n neyron ÅŸÉ™bÉ™kÉ™ komponentlÉ™ri PyTorch-da `nn.Module` sinfindÉ™n miras alÄ±r.
*   `nn.MultiheadAttention`: PyTorch-un hazÄ±r Ã‡oxbaÅŸlÄ± DiqqÉ™t moduludur.
*   `nn.Linear`: XÉ™tti (Linear) qatdÄ±r, yÉ™ni matris vurulmasÄ±.
*   `nn.LayerNorm`: NormallaÅŸdÄ±rma qatÄ±dÄ±r.
*   `x = x + ...`: **Residual Connection** (QalÄ±q ÆlaqÉ™) adlanÄ±r. Bu, modelin dÉ™rinlÉ™ÅŸdikcÉ™ Ã¶yrÉ™nmÉ™ qabiliyyÉ™tini itirmÉ™mÉ™si Ã¼Ã§Ã¼n vacibdir.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: DÃ¼ÅŸÃ¼n vÉ™ AraÅŸdÄ±r

1.  **Residual Connection** (QalÄ±q ÆlaqÉ™) nÉ™ demÉ™kdir? NiyÉ™ dÉ™rin neyron ÅŸÉ™bÉ™kÉ™lÉ™rindÉ™ bu qÉ™dÉ™r vacibdir? (SadÉ™ dildÉ™ cavab tapmaÄŸa Ã§alÄ±ÅŸÄ±n).
2.  Transformer-in ardÄ±cÄ±l modellÉ™rdÉ™n (RNN/LSTM) É™sas fÉ™rqi nÉ™dir?

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah Transformer-in É™n vacib hissÉ™si olan **DiqqÉ™t Mexanizmini (Attention)** sÄ±fÄ±rdan qurmaÄŸa baÅŸlayacaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 750 sÃ¶z.
