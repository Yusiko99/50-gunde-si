# G√ºn 11: Tokenizasiya II: Tokenizatorun Qurulmasƒ± (Praktika) üõ†Ô∏è

## 11.1. Tokenizatorun T…ôlimi

G√ºn 10-da √∂yr…ôndiyimiz BPE alqoritmini indi t…ômizl…ônmi≈ü korpusumuz (`normalized_corpus.txt`) √ºz…ôrind…ô t…ôtbiq ed…ôc…ôyik.

**M…ôntiq:** Tokenizatorun t…ôlimi, modelin t…ôlimind…ôn f…ôrqli olaraq, modelin √ß…ôkil…ôrini deyil, dilin √∂z√ºn√ºn statistik x√ºsusiyy…ôtl…ôrini (…ôn √ßox rast g…ôlin…ôn alt-s√∂z birl…ô≈üm…ôl…ôri) √∂yr…ônir.

## 11.2. Praktika: BPE Tokenizatorunun T…ôlimi

**`train_tokenizer.py`**

```python
from tokenizers import Tokenizer, models, pre_tokenizers, trainers
import os

CORPUS_FILE = "normalized_corpus.txt"
VOCAB_SIZE = 32000
OUTPUT_FILE = "az_llm-tokenizer.json"

def train_bpe_tokenizer():
    """BPE tokenizatorunu t…ôlim edir v…ô yadda saxlayƒ±r."""
    
    # 1. Tokenizatorun Modelini T…ôyin Etm…ôk
    # BPE modelini bo≈ü bir l√ºƒü…ôtl…ô yaradƒ±rƒ±q.
    tokenizer = Tokenizer(models.BPE())
    
    # 2. Pre-Tokenizatoru T…ôyin Etm…ôk
    # M…ôtni ilkin olaraq s√∂zl…ôr…ô b√∂lm…ôk √º√ß√ºn istifad…ô olunur.
    # Whitespace: Bo≈üluq simvollarƒ± il…ô b√∂lm…ô.
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
    
    # 3. T…ôlim√ßini T…ôyin Etm…ôk
    trainer = trainers.BpeTrainer(
        vocab_size=VOCAB_SIZE,
        special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"],
        # ∆èlav…ô olaraq, GPT-3-d…ô istifad…ô olunan <|endoftext|> tokenini d…ô …ôlav…ô edirik.
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet() # B√ºt√ºn ASCII simvollarƒ±nƒ± ilkin …ôlifbaya daxil etm…ôk
    )
    
    # 4. T…ôlimi Ba≈ülatmaq
    # Tokenizatoru korpus faylƒ± √ºz…ôrind…ô t…ôlim edirik.
    tokenizer.train([CORPUS_FILE], trainer=trainer)
    
    # 5. Tokenizatoru Yadda Saxlamaq
    tokenizer.save(OUTPUT_FILE)
    
    print(f"Tokenizator uƒüurla t…ôlim edildi v…ô '{OUTPUT_FILE}' faylƒ±na yazƒ±ldƒ±.")
    print(f"Yekun l√ºƒü…ôt √∂l√ß√ºs√º: {tokenizer.get_vocab_size()}")

if __name__ == "__main__":
    if not os.path.exists(CORPUS_FILE):
        print(f"X…ôta: Korpus faylƒ± '{CORPUS_FILE}' tapƒ±lmadƒ±. Z…ôhm…ôt olmasa G√ºn 9-un tap≈üƒ±rƒ±qlarƒ±nƒ± tamamlayƒ±n.")
    else:
        train_bpe_tokenizer()
```

## 11.3. Kodun M…ôntiqi ƒ∞zahƒ±

| S…ôtr | Kod | M…ôntiqi ƒ∞zahƒ± |
| :--- | :--- | :--- |
| **14** | `tokenizer = Tokenizer(models.BPE())` | **BPE (Byte Pair Encoding)** alqoritminin …ôsasƒ±nƒ± t…ôyin edir. |
| **18** | `tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()` | **Pre-tokenizasiya** ‚Äì BPE-d…ôn …ôvv…ôl m…ôtnin nec…ô ilkin b√∂l√ºn…ôc…ôyini m√º…ôyy…ônl…ô≈üdirir. Bo≈üluqlarla b√∂lm…ô …ôn sad…ô v…ô effektiv √ºsuldur. |
| **21** | `vocab_size=VOCAB_SIZE` | **Kritik parametr.** BPE alqoritmi bu √∂l√ß√ºy…ô √ßatana q…ôd…ôr birl…ô≈üdirm…ô …ôm…ôliyyatlarƒ±nƒ± davam etdir…ôc…ôk. |
| **22** | `special_tokens=["[UNK]", ...]` | **X√ºsusi Tokenl…ôr** ‚Äì Modelin t…ôlimi v…ô i≈ül…ôm…ôsi √º√ß√ºn vacib olan tokenl…ôr. M…ôs…ôl…ôn, `[UNK]` (Nam…ôlum) l√ºƒü…ôtd…ô olmayan s√∂zl…ôri …ôv…ôz edir. |
| **24** | `initial_alphabet=pre_tokenizers.ByteLevel.alphabet()` | **M…ôntiq:** Tokenizatorun b√ºt√ºn m√ºmk√ºn simvollarƒ± (h…ôtta nadir simvollarƒ±) tanƒ±masƒ±nƒ± t…ômin edir. |
| **28** | `tokenizer.train([CORPUS_FILE], trainer=trainer)` | Tokenizatoru korpus √ºz…ôrind…ô t…ôlim edir. Bu proses …ôn √ßox t…ôkrarlanan alt-s√∂z c√ºtl…ôrini tapƒ±r v…ô l√ºƒü…ôti qurur. |

## 11.4. Tokenizatorun Test Edilm…ôsi

T…ôlimd…ôn sonra tokenizatorun d√ºzg√ºn i≈ül…ôdiyini yoxlamaq vacibdir.

```python
# Tokenizatoru y√ºkl…ôm…ôk
tokenizer = Tokenizer.from_file("az_llm-tokenizer.json")

# N√ºmun…ô m…ôtn
text = "S√ºni intellekt Az…ôrbaycan dilind…ô m…ôtn yarada bil…ôr."

# M…ôtni tokenizasiya etm…ôk
encoding = tokenizer.encode(text)

# N…ôtic…ôni yoxlamaq
print(f"Tokenl…ôr: {encoding.tokens}")
print(f"ID-l…ôr: {encoding.ids}")
```

**G√∂zl…ônil…ôn N…ôtic…ô:** M√ºr…ôkk…ôb Az…ôrbaycan s√∂zl…ôri (m…ôs…ôl…ôn, "intellekt", "Az…ôrbaycan") bir ne√ß…ô alt-s√∂z…ô b√∂l√ºnm…ôlidir. M…ôs…ôl…ôn, "Az…ôrbaycan" -> \["Az…ôr", "bay", "can"] kimi.
