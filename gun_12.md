# G羹n 12: Mlumat覺n Haz覺rlanmas覺: Rqmldirm 妤

## 12.1. Tlim Mlumat覺n覺n Formas覺

LLM-lr **N繹vbti Tokenin Proqnozlad覺r覺lmas覺 (Next Token Prediction)** tap覺r覺覺 羹zrind tlim ke癟ir. Bu o demkdir ki, model veriln hr hans覺 bir token ard覺c覺ll覺覺 羹癟羹n, model bu ard覺c覺ll覺qdan sonra glm ehtimal覺 n y羹ksk olan tokeni proqnozlad覺rmal覺d覺r.

**Mntiq:** Tlim mlumat覺 **giri (input)** v **hdf (target)** ard覺c覺ll覺qlar覺na b繹l羹nmlidir.

| Token Ard覺c覺ll覺覺 | Giri (X) | Hdf (Y) |
| :--- | :--- | :--- |
| **T1 T2 T3 T4 T5** | T1 T2 T3 T4 | T2 T3 T4 T5 |

Model T1- bax覺b T2-ni, T1 v T2-y bax覺b T3-羹 proqnozlad覺rma覺 繹yrnir.

## 12.2. Mlumat覺n Bloklara B繹l羹nmsi

LLM-lr yaln覺z m羹yyn bir uzunlua qdr olan ard覺c覺ll覺qlar覺 emal ed bilr. Bu uzunluq **Kontekst Pncrsi (Context Window)** v ya **Blok l癟羹s羹 (`block_size`)** adlan覺r. Bizim modelimiz 羹癟羹n bu 繹l癟羹 **256 token** olaraq tyin edilmidir.

**Mntiq:** Korpusdak覺 b羹t羹n mtn, 256 tokenlik bloklara b繹l羹nmlidir.

### A. Tkrar Y羹klm (Overlapping)

Korpusu bloklara b繹lrkn, mlumat itkisinin qar覺s覺n覺 almaq 羹癟羹n **tkrar y羹klm (overlapping)** texnikas覺 istifad olunur.

*   **Sad B繹lm:** `[T1..T256]`, `[T257..T512]`
*   **Tkrar Y羹klm:** `[T1..T256]`, `[T129..T384]`, `[T257..T512]`

Bu, modelin bir c羹mlnin ortas覺nda ksilmsi sbbindn konteksti itirmsinin qar覺s覺n覺 al覺r.

## 12.3. Praktika: Mlumat覺n Rqmldirilmsi

**`prepare_data.py`**

```python
from tokenizers import Tokenizer
import numpy as np
import torch
import os

TOKENIZER_FILE = "az_llm-tokenizer.json"
CORPUS_FILE = "normalized_corpus.txt"
BLOCK_SIZE = 256 # Modelin kontekst pncrsi

def prepare_data():
    """Korpusu tokenizasiya edir v 256 tokenlik bloklara b繹l羹r."""
    
    # 1. Tokenizatoru Y羹klmk
    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)
    
    # 2. Korpusu Oxumaq
    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:
        text = f.read()
        
    # 3. B羹t羹n Korpusu Tokenizasiya Etmk
    # Tokenizator b羹t羹n mtni bir dfy token ID-lrin 癟evirir.
    encoded = tokenizer.encode(text)
    all_token_ids = encoded.ids
    
    # 4. Tlim v Validasiya Mlumat覺na B繹lmk
    # Mlumat覺n 90%-i tlim, 10%-i validasiya 羹癟羹n istifad olunur.
    data = torch.tensor(all_token_ids, dtype=torch.long)
    n = int(0.9 * len(data))
    train_data = data[:n]
    val_data = data[n:]
    
    # 5. Bloklara B繹lm (Tkrar Y羹klmsiz Sad Versiya)
    # Tkrar y羹klm m羹rkkb olduu 羹癟羹n, sadlik 羹癟羹n ard覺c覺l bloklara b繹l羹r羹k.
    
    # Tlim mlumat覺n覺 bloklara b繹lmk
    train_blocks = []
    for i in range(0, len(train_data) - BLOCK_SIZE + 1, BLOCK_SIZE):
        train_blocks.append(train_data[i:i + BLOCK_SIZE])
        
    # Validasiya mlumat覺n覺 bloklara b繹lmk
    val_blocks = []
    for i in range(0, len(val_data) - BLOCK_SIZE + 1, BLOCK_SIZE):
        val_blocks.append(val_data[i:i + BLOCK_SIZE])
        
    # 6. Yekun Tensorlar覺 Yadda Saxlamaq
    train_tensor = torch.stack(train_blocks)
    val_tensor = torch.stack(val_blocks)
    
    torch.save(train_tensor, 'train_data.pt')
    torch.save(val_tensor, 'val_data.pt')
    
    print(f"Mlumat haz覺rl覺覺 tamamland覺.")
    print(f"Tlim bloklar覺n覺n say覺: {train_tensor.shape[0]}")
    print(f"Validasiya bloklar覺n覺n say覺: {val_tensor.shape[0]}")

if __name__ == "__main__":
    if not os.path.exists(TOKENIZER_FILE):
        print("Xta: Tokenizator fayl覺 tap覺lmad覺. Zhmt olmasa G羹n 11-i tamamlay覺n.")
    else:
        prepare_data()
```

## 12.4. Kodun Mntiqi 襤zah覺

| Str | Kod | Mntiqi 襤zah覺 |
| :--- | :--- | :--- |
| **24** | `data = torch.tensor(all_token_ids, dtype=torch.long)` | B羹t羹n token ID-lrini PyTorch-un `Long` tipli tensoruna 癟evirir. `Long` tipi tam ddlri saxlamaq 羹癟羹n istifad olunur. |
| **25** | `n = int(0.9 * len(data))` | **Mntiq:** Mlumat覺n 90%-i modelin 繹yrnmsi 羹癟羹n (Tlim), 10%-i is modelin 繹yrnmdiyini yoxlamaq 羹癟羹n (Validasiya) ayr覺l覺r. Bu, modelin **Overfitting** (hddindn art覺q zbrlm) edib-etmdiyini yoxlamaa k繹mk edir. |
| **32** | `range(0, len(train_data) - BLOCK_SIZE + 1, BLOCK_SIZE)` | **Mntiq:** Korpusu ard覺c覺l olaraq 256 tokenlik hisslr b繹l羹r. `+ 1` son blokun tam 256 token olmas覺n覺 tmin edir. |
| **41** | `torch.stack(train_blocks)` | B羹t羹n 256 tokenlik bloklar覺 bir b繹y羹k tensor klind birldirir. Bu, `(Bloklar覺n Say覺, BLOCK_SIZE)` 繹l癟羹s羹nd bir matris yarad覺r. |
