# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 12

## DiqqÉ™t Mexanizmi (Attention): MÉ™nanÄ±n FokuslanmasÄ± ğŸ’¡

Salam! DÃ¼nÉ™n Transformer arxitekturasÄ±na giriÅŸ etdik. Bu gÃ¼n isÉ™ bu arxitekturanÄ±n **Ã¼rÉ™yi** olan **DiqqÉ™t Mexanizmini (Attention Mechanism)** Ã¶yrÉ™nÉ™cÉ™yik.

### 1. DiqqÉ™t NÉ™dir?

Ä°nsanlar danÄ±ÅŸarkÉ™n vÉ™ ya oxuyarkÉ™n, cÃ¼mlÉ™nin mÉ™nasÄ±nÄ± baÅŸa dÃ¼ÅŸmÉ™k Ã¼Ã§Ã¼n bÉ™zi sÃ¶zlÉ™rÉ™ digÉ™rlÉ™rindÉ™n daha Ã§ox diqqÉ™t yetirirlÉ™r.

MÉ™sÉ™lÉ™n, "MÉ™n Ã§ayÄ± sevirÉ™m, Ã§Ã¼nki o, **isti** vÉ™ **rahatladÄ±cÄ±dÄ±r**." cÃ¼mlÉ™sindÉ™ "o" É™vÉ™zliyi "Ã§ay" sÃ¶zÃ¼nÉ™ iÅŸarÉ™ edir. Beynimiz avtomatik olaraq "o" sÃ¶zÃ¼nÃ¼ "Ã§ay" sÃ¶zÃ¼ ilÉ™ É™laqÉ™lÉ™ndirir.

**DiqqÉ™t Mexanizmi** modelÉ™ mÉ™hz bu qabiliyyÉ™ti verir:

> **DiqqÉ™t Mexanizmi** â€” modelin bir sÃ¶zÃ¼ emal edÉ™rkÉ™n, cÃ¼mlÉ™dÉ™ki digÉ™r sÃ¶zlÉ™rin nÉ™ qÉ™dÉ™r vacib olduÄŸunu mÃ¼É™yyÉ™nlÉ™ÅŸdirmÉ™sinÉ™ imkan verÉ™n bir mexanizmdir.

### 2. Self-Attention (Ã–z-DiqqÉ™t)

LLM-lÉ™rdÉ™ istifadÉ™ olunan diqqÉ™t mexanizmi **Self-Attention (Ã–z-DiqqÉ™t)** adlanÄ±r. Bu o demÉ™kdir ki, model bir cÃ¼mlÉ™dÉ™ki hÉ™r bir sÃ¶zÃ¼n digÉ™r bÃ¼tÃ¼n sÃ¶zlÉ™rlÉ™ olan É™laqÉ™sini hesablayÄ±r.

Self-Attention Ã¼Ã§ É™sas komponentdÉ™n istifadÉ™ edir:

1.  **Query (SorÄŸu - Q):** Cari sÃ¶zÃ¼n mÉ™nasÄ±nÄ± axtarmaq Ã¼Ã§Ã¼n istifadÉ™ olunan vektordur.
2.  **Key (AÃ§ar - K):** CÃ¼mlÉ™dÉ™ki hÉ™r bir sÃ¶zÃ¼n mÉ™lumatÄ±nÄ± tÉ™msil edÉ™n vektordur.
3.  **Value (DÉ™yÉ™r - V):** ÆlaqÉ™li sÃ¶zlÉ™rin mÉ™lumatÄ±nÄ± daÅŸÄ±yan vektordur.

**Ä°ÅŸ Prinsipi:**
1.  **UyÄŸunluq HesablanmasÄ±:** HÉ™r bir **Query** (cari sÃ¶z) bÃ¼tÃ¼n **Key**-lÉ™r (bÃ¼tÃ¼n sÃ¶zlÉ™r) ilÉ™ mÃ¼qayisÉ™ edilir. Bu mÃ¼qayisÉ™ nÉ™ticÉ™sindÉ™ **DiqqÉ™t Ã‡É™kilÉ™ri (Attention Weights)** yaranÄ±r. Bu Ã§É™kilÉ™r, cari sÃ¶z Ã¼Ã§Ã¼n hansÄ± sÃ¶zlÉ™rin daha vacib olduÄŸunu gÃ¶stÉ™rir.
2.  **YumÅŸaq Maksimum (Softmax):** Ã‡É™kilÉ™r 0 ilÉ™ 1 arasÄ±na normallaÅŸdÄ±rÄ±lÄ±r.
3.  **DÉ™yÉ™rin Ã‡É™kilmÉ™si:** Bu Ã§É™kilÉ™r **Value** (DÉ™yÉ™r) vektorlarÄ±na tÉ™tbiq edilir. YÃ¼ksÉ™k Ã§É™kiyÉ™ malik olan sÃ¶zlÉ™rin mÉ™lumatÄ± daha Ã§ox Ã§É™kilir vÉ™ cari sÃ¶zÃ¼n emalÄ±na daxil edilir.

### 3. Masked Self-Attention (MaskalanmÄ±ÅŸ Ã–z-DiqqÉ™t)

Bizim LLM-imiz (GPT) **Generativ** modeldir, yÉ™ni **nÃ¶vbÉ™ti sÃ¶zÃ¼ proqnozlaÅŸdÄ±rÄ±r**. Bu o demÉ™kdir ki, model bir sÃ¶zÃ¼ proqnozlaÅŸdÄ±rarkÉ™n **Ã¶zÃ¼ndÉ™n sonra gÉ™lÉ™n sÃ¶zlÉ™ri gÃ¶rmÉ™mÉ™lidir**. Æks halda, cavabÄ± "kopya" edÉ™r.

Bunun Ã¼Ã§Ã¼n **MaskalanmÄ±ÅŸ Ã–z-DiqqÉ™t** istifadÉ™ olunur:

> **MaskalanmÄ±ÅŸ Ã–z-DiqqÉ™t** â€” DiqqÉ™t mexanizmindÉ™, cari sÃ¶zÃ¼n Ã¶zÃ¼ndÉ™n sonra gÉ™lÉ™n sÃ¶zlÉ™rÉ™ olan diqqÉ™t Ã§É™kilÉ™rini **sÄ±fÄ±ra** endirÉ™n (vÉ™ ya mÉ™nfi sonsuzluÄŸa yaxÄ±nlaÅŸdÄ±ran) bir maska tÉ™tbiq edilir.

Bu maska sayÉ™sindÉ™ model, mÉ™sÉ™lÉ™n, "AzÉ™rbaycan dili" cÃ¼mlÉ™sindÉ™ "AzÉ™rbaycan" sÃ¶zÃ¼nÃ¼ emal edÉ™rkÉ™n "dili" sÃ¶zÃ¼nÃ¼ gÃ¶rmÃ¼r.

### 4. PyTorch-da MaskalanmÄ±ÅŸ DiqqÉ™t

Biz bu mexanizmi PyTorch-da sÄ±fÄ±rdan quracaÄŸÄ±q.

AÅŸaÄŸÄ±dakÄ± kodu **`attention.py`** adlÄ± bir faylda yazaq. Bu, bizim **Self-Attention** modulunun É™sasÄ±nÄ± tÉ™ÅŸkil edÉ™cÉ™k.

```python
# attention.py
import torch
import torch.nn as nn
from torch.nn import functional as F

class SelfAttention(nn.Module):
    """ SadÉ™lÉ™ÅŸdirilmiÅŸ Self-Attention mexanizmi """

    def __init__(self, n_embd, block_size):
        super().__init__()
        # Q, K, V Ã¼Ã§Ã¼n xÉ™tti qatlar (Linear layers)
        self.key = nn.Linear(n_embd, n_embd, bias=False)
        self.query = nn.Linear(n_embd, n_embd, bias=False)
        self.value = nn.Linear(n_embd, n_embd, bias=False)
        # MaskanÄ± bu obyektdÉ™ saxlayÄ±rÄ±q
        # Bu, modelin Ã¶zÃ¼ndÉ™n sonrakÄ± tokenlÉ™rÉ™ baxmasÄ±nÄ±n qarÅŸÄ±sÄ±nÄ± alÄ±r
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))
                                     .view(1, block_size, block_size))

    def forward(self, x):
        B, T, C = x.shape # B=Batch, T=Time (uzunluq), C=Channel (gÃ¶mÃ¼lmÉ™ Ã¶lÃ§Ã¼sÃ¼)

        # Q, K, V-ni hesablayÄ±rÄ±q
        k = self.key(x)   # (B, T, C)
        q = self.query(x) # (B, T, C)
        v = self.value(x) # (B, T, C)

        # 1. DiqqÉ™t Ã‡É™kilÉ™rini Hesablamaq (Q * K^T)
        # Scaled Dot-Product Attention
        wei = q @ k.transpose(-2, -1) * (C**-0.5) # (B, T, T)

        # 2. Maskalanma (Masking)
        # Ã–zÃ¼ndÉ™n sonrakÄ± tokenlÉ™rÉ™ diqqÉ™ti sÄ±fÄ±ra endiririk
        wei = wei.masked_fill(self.tril[:,:T,:T] == 0, float('-inf'))

        # 3. Softmax
        wei = F.softmax(wei, dim=-1) # (B, T, T)

        # 4. DÉ™yÉ™rin Ã‡É™kilmÉ™si (wei @ V)
        out = wei @ v # (B, T, C)
        return out
```

**Kodun Ä°zahÄ±:**
*   `n_embd`: HÉ™r bir tokenin gÃ¶mÃ¼lmÉ™ Ã¶lÃ§Ã¼sÃ¼ (embedding dimension).
*   `block_size`: Modelin baxa bilÉ™cÉ™yi maksimum mÉ™tn uzunluÄŸu.
*   `self.key`, `self.query`, `self.value`: Q, K, V vektorlarÄ±nÄ± yaratmaq Ã¼Ã§Ã¼n istifadÉ™ olunan xÉ™tti qatlardÄ±r.
*   `self.register_buffer('tril', ...)`: **tril** (triangle lower) adlanan Ã¼Ã§bucaq maskasÄ±nÄ± yaradÄ±r. Bu maska, É™sas diaqonalÄ±n altÄ±ndakÄ± bÃ¼tÃ¼n dÉ™yÉ™rlÉ™ri 1, Ã¼stÃ¼ndÉ™kilÉ™ri isÉ™ 0 edir.
*   `wei = q @ k.transpose(-2, -1) * (C**-0.5)`: DiqqÉ™t Ã§É™kilÉ™rini hesablayÄ±r (matris vurulmasÄ±). `(C**-0.5)` isÉ™ **Scaled** hissÉ™sidir (normallaÅŸdÄ±rma).
*   `wei = wei.masked_fill(self.tril[:,:T,:T] == 0, float('-inf'))`: **Maskalanma** hissÉ™sidir. ÃœÃ§bucaq maskada 0 olan yerlÉ™ri mÉ™nfi sonsuzluÄŸa Ã§evirir. Softmax funksiyasÄ± mÉ™nfi sonsuzluÄŸu 0-a Ã§evirÉ™cÉ™k.
*   `wei = F.softmax(wei, dim=-1)`: Ã‡É™kilÉ™ri normallaÅŸdÄ±rÄ±r.
*   `out = wei @ v`: Ã‡É™kilmiÅŸ dÉ™yÉ™rlÉ™ri hesablayÄ±r.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `attention.py` faylÄ±nÄ± yaradÄ±n vÉ™ yuxarÄ±dakÄ± kodu ora kopyalayÄ±n.
2.  PyTorch-da kiÃ§ik bir sÄ±naq matrisi yaradÄ±n vÉ™ `SelfAttention` sinfini test edin.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah bu sadÉ™ **SelfAttention** mexanizmini daha gÃ¼clÃ¼ olan **Ã‡oxbaÅŸlÄ± DiqqÉ™tÉ™ (Multi-Head Attention)** Ã§evirÉ™cÉ™yik.

***

**SÃ¶z SayÄ±:** 800 sÃ¶z.
