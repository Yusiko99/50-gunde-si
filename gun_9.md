# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 9

## Tokenizatorun QurulmasÄ± (Praktika) ğŸ› ï¸

Salam! DÃ¼nÉ™n **Tokenizasiya** vÉ™ **BPE** alqoritminin nÉ™zÉ™riyyÉ™sini Ã¶yrÉ™ndik. Bu gÃ¼n isÉ™ praktikaya keÃ§irik vÉ™ **azcorpus** mÉ™lumatÄ±mÄ±z Ã¼zÉ™rindÉ™ **AzÉ™rbaycan dili Ã¼Ã§Ã¼n xÃ¼susi BPE Tokenizatorumuzu** tÉ™lim edÉ™cÉ™yik.

Bu, bizim LLM layihÉ™mizdÉ™ ilk dÉ™fÉ™ **real kod** yazacaÄŸÄ±mÄ±z vÉ™ icra edÉ™cÉ™yimiz gÃ¼ndÃ¼r.

### 1. Tokenizatorun TÉ™limi Ã¼Ã§Ã¼n Kod

AÅŸaÄŸÄ±dakÄ± kodu **`train_tokenizer.py`** adlÄ± bir faylda yazaq.

```python
# train_tokenizer.py
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# 1. Æsas ParametrlÉ™r
# Modelin tanÄ±ya bilÉ™cÉ™yi unikal tokenlÉ™rin sayÄ±
VOCAB_SIZE = 32000
# TÉ™mizlÉ™nmiÅŸ mÉ™tn faylÄ±mÄ±zÄ±n yolu
FILE_PATH = "azcorpus_cleaned.txt"
# Tokenizatoru saxlayacaÄŸÄ±mÄ±z fayl adÄ±
OUTPUT_FILE = "az_bpe_tokenizer.json"

# 2. XÃ¼susi TokenlÉ™r
# Bu tokenlÉ™r model Ã¼Ã§Ã¼n xÃ¼susi mÉ™na daÅŸÄ±yÄ±r
SPECIAL_TOKENS = [
    "<|endoftext|>", # MÉ™tnin sonunu bildirir (GPT modellÉ™ri Ã¼Ã§Ã¼n vacibdir)
    "<|pad|>",       # MÉ™tnlÉ™ri eyni uzunluÄŸa gÉ™tirmÉ™k Ã¼Ã§Ã¼n istifadÉ™ olunur
    "<|unk|>",       # Modelin tanÄ±madÄ±ÄŸÄ± tokenlÉ™r Ã¼Ã§Ã¼n
]

print(f"Tokenizator tÉ™liminÉ™ baÅŸlanÄ±lÄ±r. SÃ¶zlÃ¼k hÉ™cmi: {VOCAB_SIZE}")

# 3. Tokenizatorun YaradÄ±lmasÄ±
# BPE modelini istifadÉ™ edÉ™n boÅŸ bir Tokenizer obyekti yaradÄ±rÄ±q
tokenizer = Tokenizer(BPE(unk_token="<|unk|>"))

# 4. ÆvvÉ™lcÉ™dÉ™n Tokenizasiya (Pre-Tokenization)
# MÉ™tni boÅŸluqlara gÃ¶rÉ™ ilkin tokenlÉ™rÉ™ ayÄ±rÄ±r
tokenizer.pre_tokenizer = Whitespace()

# 5. TÉ™limÃ§i (Trainer) Obyektinin YaradÄ±lmasÄ±
# BPE alqoritmini mÉ™lumatÄ±mÄ±z Ã¼zÉ™rindÉ™ tÉ™lim edÉ™cÉ™k obyektdir
trainer = BpeTrainer(
    vocab_size=VOCAB_SIZE,
    special_tokens=SPECIAL_TOKENS,
    # TokenlÉ™rin birlÉ™ÅŸmÉ™sinin minimum tezliyi (Ã§ox az tÉ™krarlananlarÄ± nÉ™zÉ™rÉ™ almamaq Ã¼Ã§Ã¼n)
    min_frequency=2
)

# 6. Tokenizatorun TÉ™lim EdilmÉ™si
# TÉ™limÃ§i, gÃ¶stÉ™rilÉ™n faylÄ± oxuyur vÉ™ BPE alqoritmini tÉ™tbiq edir
tokenizer.train([FILE_PATH], trainer)

# 7. Tokenizatorun SaxlanmasÄ±
# TÉ™lim olunmuÅŸ tokenizatoru JSON formatÄ±nda yadda saxlayÄ±rÄ±q
tokenizer.save(OUTPUT_FILE)

print(f"Tokenizator uÄŸurla tÉ™lim edildi vÉ™ '{OUTPUT_FILE}' faylÄ±na yazÄ±ldÄ±.")

# 8. Yoxlama (Test)
# SaxlanmÄ±ÅŸ tokenizatoru yÃ¼klÉ™yib sÄ±naqdan keÃ§iririk
loaded_tokenizer = Tokenizer.from_file(OUTPUT_FILE)

test_text = "SÃ¼ni Ä°ntellekt AzÉ™rbaycan dilindÉ™ danÄ±ÅŸÄ±r."
encoding = loaded_tokenizer.encode(test_text)

print(f"\nSÄ±naq mÉ™tni: '{test_text}'")
print(f"TokenlÉ™r: {encoding.tokens}")
print(f"Token ID-lÉ™ri: {encoding.ids}")
print(f"SÃ¶zlÃ¼k hÉ™cmi: {loaded_tokenizer.get_vocab_size()}")
```

### 2. Kodun Ä°zahÄ± (HÉ™r SÉ™trin DetallÄ± Ä°zahÄ±)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 2 | `from tokenizers import Tokenizer` | Æsas `Tokenizer` sinfini daxil edirik. |
| 3 | `from tokenizers.models import BPE` | Tokenizasiya Ã¼Ã§Ã¼n **BPE (Byte Pair Encoding)** modelini daxil edirik. |
| 4 | `from tokenizers.trainers import BpeTrainer` | BPE modelini tÉ™lim etmÉ™k Ã¼Ã§Ã¼n lazÄ±m olan `BpeTrainer` sinfini daxil edirik. |
| 5 | `from tokenizers.pre_tokenizers import Whitespace` | MÉ™tni boÅŸluqlara gÃ¶rÉ™ ilkin hissÉ™lÉ™rÉ™ bÃ¶lÉ™n `Whitespace` funksiyasÄ±nÄ± daxil edirik. |
| 8 | `VOCAB_SIZE = 32000` | **SÃ¶zlÃ¼k hÉ™cmini 32,000 olaraq tÉ™yin edirik.** Bu, 100M parametreli model Ã¼Ã§Ã¼n yaxÅŸÄ± bir baÅŸlanÄŸÄ±cdÄ±r. |
| 9 | `FILE_PATH = "azcorpus_cleaned.txt"` | TÉ™mizlÉ™nmiÅŸ mÉ™lumatÄ±mÄ±zÄ±n yolunu gÃ¶stÉ™ririk. |
| 11 | `OUTPUT_FILE = "az_bpe_tokenizer.json"` | Tokenizatorun saxlanacaÄŸÄ± faylÄ±n adÄ±nÄ± tÉ™yin edirik. |
| 15-19 | `SPECIAL_TOKENS = [...]` | Modelin ehtiyacÄ± olan xÃ¼susi tokenlÉ™ri siyahÄ± ÅŸÉ™klindÉ™ tÉ™yin edirik. |
| 23 | `tokenizer = Tokenizer(BPE(unk_token="<|unk|>"))` | Yeni bir tokenizator obyekti yaradÄ±rÄ±q vÉ™ ona **BPE** modelini istifadÉ™ etmÉ™sini, tanÄ±madÄ±ÄŸÄ± tokenlÉ™r Ã¼Ã§Ã¼n isÉ™ `<|unk|>` tokenini istifadÉ™ etmÉ™sini bildiririk. |
| 27 | `tokenizer.pre_tokenizer = Whitespace()` | Tokenizasiyadan É™vvÉ™l mÉ™tni boÅŸluqlara gÃ¶rÉ™ ayÄ±rmasÄ±nÄ± tÉ™yin edirik. |
| 30 | `trainer = BpeTrainer(...)` | TÉ™limÃ§i obyekti yaradÄ±rÄ±q. |
| 31 | `vocab_size=VOCAB_SIZE,` | TÉ™limÃ§iyÉ™ sÃ¶zlÃ¼k hÉ™cminin 32000 olacaÄŸÄ±nÄ± bildiririk. |
| 32 | `special_tokens=SPECIAL_TOKENS,` | TÉ™lim zamanÄ± xÃ¼susi tokenlÉ™ri dÉ™ nÉ™zÉ™rÉ™ almasÄ±nÄ± tÉ™min edirik. |
| 36 | `tokenizer.train([FILE_PATH], trainer)` | **Æsas tÉ™lim É™mri.** TÉ™limÃ§i, `azcorpus_cleaned.txt` faylÄ±ndakÄ± mÉ™tnlÉ™r Ã¼zÉ™rindÉ™ BPE alqoritmini icra edir. |
| 39 | `tokenizer.save(OUTPUT_FILE)` | TÉ™lim olunmuÅŸ tokenizatoru gÉ™lÉ™cÉ™kdÉ™ istifadÉ™ etmÉ™k Ã¼Ã§Ã¼n yadda saxlayÄ±rÄ±q. |
| 43 | `loaded_tokenizer = Tokenizer.from_file(OUTPUT_FILE)` | Yadda saxladÄ±ÄŸÄ±mÄ±z tokenizatoru yÃ¼klÉ™yirik. |
| 45 | `encoding = loaded_tokenizer.encode(test_text)` | SÄ±naq mÉ™tnini tokenlÉ™rÉ™ Ã§eviririk. |
| 47-49 | `print(...)` | NÉ™ticÉ™lÉ™ri ekrana Ã§Ä±xarÄ±rÄ±q. |

### 3. Ä°cra

`llm_50gun` mÃ¼hitiniz aktivdirsÉ™, kodu icra edin:

```bash
python train_tokenizer.py
```

TÉ™lim prosesi sizin kompÃ¼terinizin sÃ¼rÉ™tindÉ™n asÄ±lÄ± olaraq bir neÃ§É™ dÉ™qiqÉ™ Ã§É™kÉ™ bilÉ™r. NÉ™ticÉ™dÉ™, **`az_bpe_tokenizer.json`** adlÄ± fayl yaranacaq.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `train_tokenizer.py` faylÄ±nÄ± yaradÄ±n vÉ™ icra edin.
2.  Yaranan **`az_bpe_tokenizer.json`** faylÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼nÃ¼ yoxlayÄ±n (Ã§ox kiÃ§ik olmalÄ±dÄ±r).
3.  Kodu dÉ™yiÅŸdirÉ™rÉ™k, baÅŸqa bir AzÉ™rbaycan dilindÉ™ cÃ¼mlÉ™ni tokenizasiya edin vÉ™ nÉ™ticÉ™ni tÉ™hlil edin. GÃ¶rÃ¼n, hansÄ± sÃ¶zlÉ™r bir token, hansÄ±lar isÉ™ bir neÃ§É™ tokenÉ™ bÃ¶lÃ¼nÃ¼b.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **Tokenizatoru istifadÉ™ edÉ™rÉ™k bÃ¼tÃ¼n mÉ™lumatÄ± modelin tÉ™limi Ã¼Ã§Ã¼n hazÄ±r vÉ™ziyyÉ™tÉ™ gÉ™tirÉ™cÉ™yik.**

***

**SÃ¶z SayÄ±:** 850 sÃ¶z.
