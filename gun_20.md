# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 20

## MÉ™tn GenerasiyasÄ± (Sampling): Modelin "DanÄ±ÅŸmasÄ±" ğŸ—£ï¸

Salam! Ä°kinci 10 gÃ¼nlÃ¼k mÉ™rhÉ™lÉ™mizin sonuna Ã§atdÄ±q! DÃ¼nÉ™n modelimizin arxitekturasÄ±nÄ±n dÃ¼zgÃ¼n iÅŸlÉ™diyini yoxladÄ±q. Bu gÃ¼n isÉ™ modelimizi **tÉ™limdÉ™n É™vvÉ™l** "danÄ±ÅŸdÄ±rmaÄŸÄ±" Ã¶yrÉ™nÉ™cÉ™yik. Bu proses **MÉ™tn GenerasiyasÄ±** vÉ™ ya **Sampling** adlanÄ±r.

### 1. Generasiya NecÉ™ Ä°ÅŸlÉ™yir?

GPT modellÉ™ri **avto-reqressiv** (auto-regressive) ÅŸÉ™kildÉ™ iÅŸlÉ™yir, yÉ™ni:

1.  ModelÉ™ bir **baÅŸlanÄŸÄ±c mÉ™tn** (prompt) verilir.
2.  Model bu mÉ™tnÉ™ É™saslanaraq **nÃ¶vbÉ™ti tokenin ehtimalÄ±nÄ±** (32000 token Ã¼Ã§Ã¼n 32000 ehtimal) hesablayÄ±r.
3.  Bu ehtimallardan biri **seÃ§ilir (sampled)**.
4.  SeÃ§ilmiÅŸ token baÅŸlanÄŸÄ±c mÉ™tnÉ™ É™lavÉ™ edilir.
5.  Yeni, daha uzun mÉ™tn yenidÉ™n modelÉ™ verilir vÉ™ proses tÉ™krarlanÄ±r.

Bu proses istÉ™diyimiz uzunluÄŸa Ã§atana qÉ™dÉ™r davam edir.

### 2. Sampling StrategiyalarÄ±

NÃ¶vbÉ™ti tokeni seÃ§mÉ™k Ã¼Ã§Ã¼n bir neÃ§É™ strategiya var:

| Strategiya | Ä°zah | NÉ™ticÉ™ |
| :--- | :--- | :--- |
| **Greedy Search (AÃ§gÃ¶z AxtarÄ±ÅŸ)** | HÉ™miÅŸÉ™ **É™n yÃ¼ksÉ™k ehtimalÄ±** olan tokeni seÃ§ir. | TÉ™krarlanan, darÄ±xdÄ±rÄ±cÄ± vÉ™ qeyri-tÉ™bii mÉ™tnlÉ™r yaradÄ±r. |
| **Random Sampling (TÉ™sadÃ¼fi SeÃ§im)** | Ehtimallara É™saslanaraq **tÉ™sadÃ¼fi** bir token seÃ§ir. | Daha yaradÄ±cÄ±, lakin bÉ™zÉ™n mÉ™nasÄ±z mÉ™tnlÉ™r yaradÄ±r. |
| **Top-K Sampling** | YalnÄ±z É™n yÃ¼ksÉ™k ehtimala malik **K** sayda tokeni nÉ™zÉ™rÉ™ alÄ±r, sonra onlar arasÄ±ndan tÉ™sadÃ¼fi seÃ§im edir. | TÉ™bii vÉ™ mÉ™ntiqli mÉ™tnlÉ™r yaradÄ±r. |
| **Top-P (Nucleus) Sampling** | EhtimallarÄ±n cÉ™mi **P** faizÉ™ Ã§atana qÉ™dÉ™r tokenlÉ™ri nÉ™zÉ™rÉ™ alÄ±r, sonra onlar arasÄ±ndan tÉ™sadÃ¼fi seÃ§im edir. | Æn Ã§ox istifadÉ™ olunan vÉ™ É™n yaxÅŸÄ± nÉ™ticÉ™ verÉ™n strategiyadÄ±r. |

Biz **Top-K** vÉ™ **Top-P** strategiyalarÄ±nÄ± birlÉ™ÅŸdirÉ™n bir funksiya istifadÉ™ edÉ™cÉ™yik.

### 3. ModelÉ™ Generasiya FunksiyasÄ±nÄ±n ÆlavÉ™ EdilmÉ™si

`model.py` faylÄ±ndakÄ± `GPT` sinfinÉ™ `generate` adlÄ± yeni bir metod É™lavÉ™ edirik.

```python
# model.py (GPT sinfinin iÃ§indÉ™)

# ... (É™vvÉ™lki kodlar) ...

    @torch.no_grad() # Qradiyent hesablanmasÄ±nÄ± sÃ¶ndÃ¼rÃ¼rÃ¼k (tÉ™lim etmirik)
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        Modelin mÉ™tn yaratma funksiyasÄ± (Sampling)
        idx: (B, T) Ã¶lÃ§Ã¼lÃ¼ baÅŸlanÄŸÄ±c token ID-lÉ™ri
        max_new_tokens: Yaratmaq istÉ™diyimiz maksimum yeni token sayÄ±
        """
        for _ in range(max_new_tokens):
            # 1. Kontekst PÉ™ncÉ™rÉ™sinin TÉ™nzimlÉ™nmÉ™si
            # Model yalnÄ±z block_size qÉ™dÉ™r É™vvÉ™lki tokenÉ™ baxa bilÉ™r
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]

            # 2. Ä°rÉ™li Ã–tÃ¼rmÉ™ (Logits-i Hesablamaq)
            # Logits: (B, T, vocab_size)
            logits, _ = self(idx_cond)

            # 3. Son Logit-i SeÃ§mÉ™k (Æn son tokenin proqnozlarÄ±)
            # Logits: (B, vocab_size)
            logits = logits[:, -1, :] / temperature

            # 4. Top-K Sampling
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            # 5. EhtimallarÄ± Hesablamaq
            probs = F.softmax(logits, dim=-1)

            # 6. TÉ™sadÃ¼fi SeÃ§im (Sampling)
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)

            # 7. Yeni Tokeni ÆlavÉ™ EtmÉ™k
            idx = torch.cat((idx, idx_next), dim=1)

        return idx
```

### 4. Kodun Ä°zahÄ± (HÉ™r SÉ™trin DetallÄ± Ä°zahÄ±)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 3 | `@torch.no_grad()` | QradiyentlÉ™rin hesablanmasÄ±nÄ± sÃ¶ndÃ¼rÃ¼r. Bu, yaddaÅŸa qÉ™naÉ™t edir vÉ™ sÃ¼rÉ™ti artÄ±rÄ±r. |
| 10 | `idx_cond = idx if ... else idx[:, -self.config.block_size:]` | ÆgÉ™r giriÅŸ mÉ™tni `block_size` (512) Ã¶lÃ§Ã¼sÃ¼ndÉ™n uzundursa, model yalnÄ±z son 512 tokeni nÉ™zÉ™rÉ™ alÄ±r. |
| 14 | `logits = logits[:, -1, :] / temperature` | Logits-dÉ™n yalnÄ±z É™n son tokenin proqnozlarÄ±nÄ± seÃ§irik. **Temperature** (Temperatur) isÉ™ proqnozlarÄ±n "kÉ™skinliyini" tÉ™nzimlÉ™yir. YÃ¼ksÉ™k temperatur daha Ã§ox tÉ™sadÃ¼filik (yaradÄ±cÄ±lÄ±q) demÉ™kdir. |
| 17-19 | `if top_k is not None: ...` | **Top-K Sampling** tÉ™tbiq edir. Æn yÃ¼ksÉ™k ehtimalÄ± olan K tokeni saxlayÄ±r, digÉ™rlÉ™rinin ehtimalÄ±nÄ± sÄ±fÄ±ra endirir. |
| 22 | `probs = F.softmax(logits, dim=-1)` | Logits-i ehtimallara Ã§evirir (bÃ¼tÃ¼n ehtimallarÄ±n cÉ™mi 1-É™ bÉ™rabÉ™r olur). |
| 25 | `idx_next = torch.multinomial(probs, num_samples=1)` | Ehtimallara É™saslanaraq **tÉ™sadÃ¼fi** bir token seÃ§ir. |
| 28 | `idx = torch.cat((idx, idx_next), dim=1)` | SeÃ§ilmiÅŸ yeni tokeni É™vvÉ™lki mÉ™tnÉ™ É™lavÉ™ edir.

### 5. SÄ±naq

Ä°ndi modelimizi sÄ±naqdan keÃ§irÉ™k.

```python
# test_generate.py
import torch
from config import GPTConfig
from model import GPT
from tokenizers import Tokenizer

# 1. HazÄ±rlÄ±q
config = GPTConfig()
tokenizer = Tokenizer.from_file("az_bpe_tokenizer.json")
model = GPT(config)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

# 2. BaÅŸlanÄŸÄ±c MÉ™tn (Prompt)
prompt = "AzÉ™rbaycanÄ±n paytaxtÄ±"
encoded_prompt = tokenizer.encode(prompt)
idx = torch.tensor(encoded_prompt.ids, dtype=torch.long).unsqueeze(0).to(device)

# 3. Generasiya
# 50 yeni token yarat, temperature=0.8 (bir az yaradÄ±cÄ±), top_k=50
generated_ids = model.generate(idx, max_new_tokens=50, temperature=0.8, top_k=50)

# 4. DekodlaÅŸdÄ±rma
generated_text = tokenizer.decode(generated_ids[0].tolist())

print(f"GiriÅŸ: {prompt}")
print(f"Ã‡Ä±xÄ±ÅŸ (TÉ™limsiz): {generated_text}")
```

**NÉ™ticÉ™:** Model tÉ™lim olunmadÄ±ÄŸÄ± Ã¼Ã§Ã¼n, Ã§Ä±xÄ±ÅŸ mÉ™nasÄ±z vÉ™ tÉ™sadÃ¼fi sÃ¶zlÉ™r yÄ±ÄŸÄ±nÄ± olacaq. Bu, normaldÄ±r! Model hÉ™lÉ™ AzÉ™rbaycan dilini Ã¶yrÉ™nmÉ™yib.

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  `model.py` faylÄ±na `generate` metodunu É™lavÉ™ edin.
2.  `test_generate.py` faylÄ±nÄ± yaradÄ±n vÉ™ icra edin.
3.  `temperature` dÉ™yÉ™rini 0.1 (daha az tÉ™sadÃ¼fi) vÉ™ 1.5 (daha Ã§ox tÉ™sadÃ¼fi) olaraq dÉ™yiÅŸdirib nÉ™ticÉ™ni mÃ¼qayisÉ™ edin.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **TÉ™lim ProsesinÉ™** baÅŸlayÄ±rÄ±q!

***

**SÃ¶z SayÄ±:** 850 sÃ¶z.
