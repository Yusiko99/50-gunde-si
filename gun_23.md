# ğŸ“š 50 GÃ¼ndÉ™ SÃ¼ni-Ä°ntellekt: GÃ¼n 23

## TÉ™lim DÃ¶vrÃ¼ (Training Loop): Modelin Ã–yrÉ™nmÉ™ Prosesi ğŸ”„

Salam! DÃ¼nÉ™n modelimizi mÉ™lumatla tÉ™chiz edÉ™cÉ™k **DataLoader**-i qurduq. Bu gÃ¼n isÉ™ bÃ¼tÃ¼n komponentlÉ™ri birlÉ™ÅŸdirÉ™rÉ™k **TÉ™lim DÃ¶vrÃ¼nÃ¼ (Training Loop)** â€“ yÉ™ni modelin É™sl Ã¶yrÉ™nmÉ™ prosesini â€“ yazÄ±rÄ±q.

Bu, LLM layihÉ™mizin **É™n vacib** kod hissÉ™sidir.

### 1. TÉ™lim DÃ¶vrÃ¼nÃ¼n Æsas AddÄ±mlarÄ±

TÉ™lim dÃ¶vrÃ¼ hÉ™r bir **Batch** (mÉ™lumat dÉ™sti) Ã¼Ã§Ã¼n ardÄ±cÄ±l olaraq dÃ¶rd É™sas addÄ±mÄ± tÉ™krarlayÄ±r:

1.  **MÉ™lumatÄ±n YÃ¼klÉ™nmÉ™si:** DataLoader-dÉ™n bir Batch (giriÅŸ `x` vÉ™ hÉ™dÉ™f `y`) alÄ±nÄ±r.
2.  **Ä°rÉ™li Ã–tÃ¼rmÉ™ (Forward Pass):** GiriÅŸ `x` modelÉ™ verilir vÉ™ Ã§Ä±xÄ±ÅŸ `logits` vÉ™ `loss` hesablanÄ±r.
3.  **GeriyÉ™ Ã–tÃ¼rmÉ™ (Backward Pass):** `loss.backward()` É™mri ilÉ™ qradiyentlÉ™r hesablanÄ±r.
4.  **ParametrlÉ™rin YenilÉ™nmÉ™si:** OptimallaÅŸdÄ±rÄ±cÄ± (AdamW) qradiyentlÉ™ri istifadÉ™ edÉ™rÉ™k modelin Ã§É™kilÉ™rini tÉ™nzimlÉ™yir.

### 2. TÉ™lim Skriptinin TamamlanmasÄ±

Ä°ndi `train.py` skriptini tamamlayÄ±rÄ±q.

```python
# train.py
import torch
from torch.utils.data import Dataset, DataLoader
from config import GPTConfig
from model import GPT
from data_loader import get_dataloaders
from accelerate import Accelerator
from tqdm import tqdm
import time

# 1. HiperparametrlÉ™r
BATCH_SIZE = 12
BLOCK_SIZE = 512
LEARNING_RATE = 6e-4
MAX_ITERS = 5000
EVAL_INTERVAL = 500
EVAL_ITERS = 200 # Validasiya Ã¼Ã§Ã¼n istifadÉ™ olunan Batch sayÄ±
GRADIENT_ACCUMULATION_STEPS = 4 # Qradiyent yÄ±ÄŸÄ±mÄ± Ã¼Ã§Ã¼n addÄ±m sayÄ±

# 2. Akseleratoru BaÅŸlatmaq (Mixed Precision Ã¼Ã§Ã¼n)
accelerator = Accelerator(
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    mixed_precision='fp16' # YaddaÅŸa qÉ™naÉ™t edÉ™n 16-bit dÉ™qiqlik
)
device = accelerator.device

# 3. Model, DataLoader vÉ™ Optimizer-i HazÄ±rlamaq
config = GPTConfig(block_size=BLOCK_SIZE)
model = GPT(config)
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
train_loader, val_loader = get_dataloaders(BLOCK_SIZE, BATCH_SIZE)

# Akselerator ilÉ™ bÃ¼tÃ¼n obyektlÉ™ri GPU-ya kÃ¶Ã§Ã¼rÃ¼rÃ¼k
model, optimizer, train_loader, val_loader = accelerator.prepare(
    model, optimizer, train_loader, val_loader
)

# 4. Validasiya FunksiyasÄ±
@torch.no_grad()
def estimate_loss():
    """ Validasiya mÉ™lumatÄ± Ã¼zÉ™rindÉ™ itkini hesablayÄ±r """
    model.eval() # Modeli qiymÉ™tlÉ™ndirmÉ™ rejiminÉ™ keÃ§iririk
    losses = []
    for _ in range(EVAL_ITERS):
        # Validasiya Batch-ini yÃ¼klÉ™
        x, y = next(iter(val_loader))
        # Ä°rÉ™li Ã¶tÃ¼rmÉ™
        with accelerator.autocast():
            logits, loss = model(x, targets=y)
        losses.append(accelerator.gather(loss).mean().item())
    
    model.train() # Modeli tÉ™lim rejiminÉ™ qaytarÄ±rÄ±q
    return torch.tensor(losses).mean().item()

# 5. Æsas TÉ™lim DÃ¶vrÃ¼
start_time = time.time()
for iter_num in tqdm(range(MAX_ITERS), desc="TÉ™lim Prosesi"):
    
    # A. Validasiya
    if iter_num % EVAL_INTERVAL == 0:
        val_loss = estimate_loss()
        print(f"AddÄ±m {iter_num}: TÉ™lim Ä°tkisi (Loss) = {val_loss:.4f}")
        # Modelin vÉ™ziyyÉ™tini yadda saxlamaq (Checkpoint)
        # accelerator.save_state(f"checkpoint_{iter_num}")

    # B. MÉ™lumatÄ± YÃ¼klÉ™
    x, y = next(iter(train_loader))
    
    # C. Ä°rÉ™li Ã–tÃ¼rmÉ™ vÉ™ Ä°tki HesablanmasÄ±
    with accelerator.accumulate(model):
        with accelerator.autocast(): # Mixed Precision Ã¼Ã§Ã¼n
            logits, loss = model(x, targets=y)
        
        # D. GeriyÉ™ Ã–tÃ¼rmÉ™ vÉ™ ParametrlÉ™rin YenilÉ™nmÉ™si
        # Qradiyent yÄ±ÄŸÄ±mÄ± (accumulation) ilÉ™ birlikdÉ™ geriyÉ™ Ã¶tÃ¼rmÉ™
        accelerator.backward(loss)
        
        # QradiyentlÉ™rin kÉ™silmÉ™si (Gradient Clipping) - partlayan qradiyentlÉ™rin qarÅŸÄ±sÄ±nÄ± alÄ±r
        if accelerator.sync_gradients:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            
        optimizer.step() # ParametrlÉ™ri yenilÉ™
        optimizer.zero_grad() # QradiyentlÉ™ri sÄ±fÄ±rla

end_time = time.time()
print(f"\nTÉ™lim tamamlandÄ±. Ãœmumi vaxt: {(end_time - start_time) / 3600:.2f} saat")
```

### 3. Kodun Ä°zahÄ± (HÉ™r SÉ™trin DetallÄ± Ä°zahÄ±)

| SÉ™tr | Kod | Ä°zah |
| :--- | :--- | :--- |
| 17 | `GRADIENT_ACCUMULATION_STEPS = 4` | **Qradiyent YÄ±ÄŸÄ±mÄ±:** HÉ™r 4 Batch-dÉ™n bir parametrlÉ™ri yenilÉ™yÉ™cÉ™yik. Bu, **effektiv Batch Size-Ä±** 4 dÉ™fÉ™ artÄ±rÄ±r (12 * 4 = 48). |
| 21 | `accelerator = Accelerator(...)` | **Akselerator:** GPU-nu, `fp16` (16-bit dÉ™qiqlik) vÉ™ qradiyent yÄ±ÄŸÄ±mÄ±nÄ± idarÉ™ edÉ™n É™sas obyektdir. |
| 32 | `model, optimizer, ... = accelerator.prepare(...)` | BÃ¼tÃ¼n PyTorch obyektlÉ™rini avtomatik olaraq GPU-ya kÃ¶Ã§Ã¼rÃ¼r vÉ™ `fp16` Ã¼Ã§Ã¼n hazÄ±rlayÄ±r. |
| 37 | `@torch.no_grad()` | Validasiya zamanÄ± qradiyent hesablanmasÄ±nÄ± sÃ¶ndÃ¼rÃ¼r. |
| 38 | `model.eval()` | Modeli qiymÉ™tlÉ™ndirmÉ™ rejiminÉ™ keÃ§irir (Dropout vÉ™ LayerNorm fÉ™rqli iÅŸlÉ™yir). |
| 58 | `if iter_num % EVAL_INTERVAL == 0:` | HÉ™r 500 addÄ±mdan bir validasiya itkisini hesablayÄ±b ekrana Ã§Ä±xarÄ±r. |
| 66 | `with accelerator.accumulate(model):` | Bu blokun iÃ§indÉ™ki `backward` É™mri, `GRADIENT_ACCUMULATION_STEPS` qÉ™dÉ™r qradiyentlÉ™ri yÄ±ÄŸacaq. |
| 67 | `with accelerator.autocast():` | **Mixed Precision** (QarÄ±ÅŸÄ±q DÉ™qiqlik) tÉ™tbiq edir. BÉ™zi É™mÉ™liyyatlar `fp16` ilÉ™, bÉ™zilÉ™ri isÉ™ `fp32` ilÉ™ icra olunur. Bu, yaddaÅŸa qÉ™naÉ™t edir. |
| 70 | `accelerator.backward(loss)` | **GeriyÉ™ Ã–tÃ¼rmÉ™** É™mri. |
| 73 | `accelerator.clip_grad_norm_(model.parameters(), 1.0)` | **Qradiyent KÉ™silmÉ™si:** QradiyentlÉ™rin dÉ™yÉ™rini 1.0-dan yuxarÄ± qalxmasÄ±nÄ±n qarÅŸÄ±sÄ±nÄ± alÄ±r. Bu, tÉ™limin stabil qalmasÄ± Ã¼Ã§Ã¼n vacibdir. |
| 75 | `optimizer.step()` | YÄ±ÄŸÄ±lmÄ±ÅŸ qradiyentlÉ™rÉ™ É™sasÉ™n modelin Ã§É™kilÉ™rini yenilÉ™yir. |
| 76 | `optimizer.zero_grad()` | NÃ¶vbÉ™ti Batch Ã¼Ã§Ã¼n qradiyentlÉ™ri sÄ±fÄ±rlayÄ±r. |

### ğŸ’¡ GÃ¼nÃ¼n TapÅŸÄ±rÄ±ÄŸÄ±: Praktika

1.  **`train.py`** faylÄ±nÄ± tamamlayÄ±n.
2.  BÃ¼tÃ¼n asÄ±lÄ±lÄ±qlarÄ±n (model.py, data_loader.py, config.py, az_bpe_tokenizer.json, train.npy, val.npy) hazÄ±r olduÄŸundan É™min olun.
3.  **TÉ™limÉ™ baÅŸlayÄ±n!** `accelerate launch train.py` É™mri ilÉ™ tÉ™limi baÅŸladÄ±n.

**Sabah gÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!** ğŸ‘‹ Sabah **OptimallaÅŸdÄ±rÄ±cÄ± vÉ™ Ã–yrÉ™nmÉ™ SÃ¼rÉ™ti** mÃ¶vzusunu daha dÉ™rindÉ™n araÅŸdÄ±racaÄŸÄ±q.

***

**SÃ¶z SayÄ±:** 850 sÃ¶z.
